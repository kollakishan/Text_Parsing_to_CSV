,c2,c4,c5,c6
0,Following the notation in section 21  the ijth entry of the matrix W is defined as in   Wj  1  e Wz  where Cj is the total number of words in the jth sentence pair,Fundamental,Idea,1
1,For the simple bagofword bilingual LSA as described in Section 221  after SVD on the sparse matrix using the toolkit SVDPACK    all source and target words are projected into a lowdimensional R  88 LSAspace,Fundamental,Basis,1
2,Discriminative word alignment models  such as Ittycheriah and Roukos   Moore   Blunsom and Cohn    have received great amount of study recently,BackGround,GRelated,1
3,For instance  the 1 most relaxed IBM Model1  which assumes that any source word can be generated by any target word equally regardless of distance  can be improved by demanding a Markov process of alignments as in HMMbased models    or implementing a distribution of number of target words linked to a source word as in IBM fertilitybased models  ,BackGround,GRelated,1
4,It can be applied to complicated models such IBM Model4  ,BackGround,SRelated,1
5,The language model is a statistical trigram model estimated with Modified KneserNey smoothing   using all English sentences in the parallel training data,Fundamental,Basis,1
6,LSA has been successfully applied to information retrieval    statistical langauge modeling   and etc,BackGround,GRelated,2
7,Alternative constructions of the matrix are possible using raw counts or TFIDF  ,BackGround,SRelated,1
8,It has been shown that human knowledge  in the form of a small amount of manually annotated parallel data to be used to seed or guide model training  can significantly improve word alignment Fmeasure and translation performance  ,BackGround,GRelated,2
9,Our decoder is a phrasebased multistack implementation of the loglinear model similar to Pharaoh  ,Fundamental,Idea,1
10,Since Arabic is a morphologically rich language where affixes are attached to stem words to indicate gender  tense  case and etc  in order to reduce vocabulary size and address outofvocabulary words  we split Arabic words into affix and root according to a rulebased segmentation scheme   with the help from the Buckwalter analyzer   output,Fundamental,Basis,1
11,Basic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by   and  ,Fundamental,Idea,2
12,In    bilingual semantic maps are constructed to guide word alignment,BackGround,SRelated,1
13,As formulated in the competitive linking algorithm    the problem of word alignment can be regarded as a process of word linkage disambiguation  that is  choosing correct associations among all competing hypothesis,BackGround,GRelated,1
14,The example demos that due to reasonable constraints placed in word alignment training  the link to _tK is corrected and consequently we have accurate word translation for the Arabic singleton 7 Heuristics based on cooccurrence analysis  such as pointwise mutual information or Dice coefficients   have been shown to be indicative for word alignments  ,BackGround,GRelated,1
15,These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method  ,Fundamental,Basis,1
16,By combining word alignments in two directions using heuristics    a single set of static word alignments is then formed,Fundamental,Basis,1
17,We simply modify the GIZA toolkit   by always weighting lexicon probabilities with soft constraints during iterative model training  and obtain 07 TER reduction on both sets and 04 BLEU improvement on the test set,Fundamental,Basis,1
18,We measure translation performance by the BLEU score   and Translation Error Rate TER   with one reference for each hypothesis,Fundamental,Basis,1
19,Toutanova et al   augmented bilingual sentence pairs with partofspeech tags as linguistic constraints for HMMbased word alignments,BackGround,GRelated,1
20,While word alignments can help identifying semantic relations    we proceed in the reverse direction,BackGround,GRelated,3
21,We shall take HMMbased word alignment model   as an example and follow the notation of  ,Fundamental,Basis,1
22,Our baseline word alignment model is the wordtoword Hidden Markov Model  ,Fundamental,Basis,1
23,Many stateoftheart SMT systems do not use trees and base the ordering decisions on surface phrases  ,BackGround,GRelated,1
24,An important advantage of our model is that it is global  and does not decompose the task of ordering a target sentence into a series of local decisions  as in the recently proposed order models for Machine Transition  ,BackGround,GRelated,3
25,Alternatively  order is modelled in terms of movement of automatically induced hierarchical structure of sentences  ,BackGround,GRelated,3
26,These Nbest lists are generated using approximate search and simpler models  as in the reranking approach of  ,Fundamental,Basis,1
27,tings  even for a bigram language model  ,,,
28,9 The advantages of modeling how a target language syntax tree moves with respect to a source language syntax tree are that i we can capture the fact that constituents move as a whole and generally respect the phrasal cohesion constraints    and ii we can model broad syntactic reordering phenomena  such as subjectverbobject constructions translating into subjectobjectverb ones  as is generally the case for English and Japanese,BackGround,SRelated,2
29,Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees   or dependency trees    which are obtained using a parser trained to determine linguistic constituency,BackGround,GRelated,2
30,Our results show that combining features derived from the source and target dependency trees  distortion surface orderbased features like the distortion used in Pharaoh   and language modellike features results in a model which significantly outperforms models using only some of the information sources,Fundamental,Basis,2
31,Pharaoh DISP Displacement as used in Pharaoh  ,Fundamental,Basis,1
32,These models are combined as feature functions in a loglinear model for predicting a target sentence given a source sentence  in the framework proposed by  ,Fundamental,Basis,1
33,The target dependency trees are obtained through projection of the source dependency trees  using the word alignment we use GIZA    ensuring better parallelism of the source and target structures,Fundamental,Basis,1
34,The sentences were annotated with alignment using GIZA   and syntactic dependency structures of the source and target  obtained as described in Section 2,Fundamental,Basis,1
35,Our model is discriminatively trained to select the best order according to the BLEU measure   of an unordered target dependency tree from the space of possible orders,Fundamental,Basis,1
36,Our algorithm for obtaining target dependency trees by projection of the source trees via the word alignment is the one used in the MT system of  ,Fundamental,Basis,1
37,It follows the order model defined in  ,Fundamental,Idea,1
38,Our baseline SMT system is the system of Quirk et al  ,Fundamental,Basis,1
39,The projection algorithm of   defines heuristics for each of these problems,BackGround,SRelated,1
40,1 Previous studies have shown that if both the source and target dependency trees represent linguistic constituency  the alignment between subtrees in the two languages is very complex  ,BackGround,GRelated,1
41,A host of discriminative methods have been introduced  ,BackGround,GRelated,1
42,2 We also investigated extractionspecific metrics the frequency of interior nodes  a measure of how often the alignments violate the constituent structure of English parses  and a variant of the CPER metric of Ayan and Dorr  ,Fundamental,Basis,1
43, From Ayan and Dorr    growdiagfinal heuristic,,,
44,5 Similarly  we compared our Chinese results to the GIZA results in Ayan and Dorr  ,Compare,Compare,1
45,Additionally  we evaluated our model with the transducer analog to the consistent phrase error rate CPER metric of Ayan and Dorr  ,Fundamental,Basis,1
46,Like the classic IBM models    our model will introduce a latent alignment vector a  a 1   a J  that specifies the position of an aligned target word for each source word,Fundamental,Idea,1
47,However  few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them  ,BackGround,GRelated,3
48,Syntactic methods are an increasingly promising approach to statistical machine translation  being both algorithmically appealing   and empirically successful  ,BackGround,GRelated,2
49,Daume III and Marcu   employs a syntaxaware distortion model for aligning summaries to documents  but condition upon the roots of the constituents that are jumped over during a transition  instead ofthose that are visited during a walk through the tree,BackGround,GRelated,1
50,Our transductive learning algorithm  Algorithm 1  is inspired by the Yarowsky algorithm  ,Fundamental,Idea,1
51,Under certain precise conditions  as described in    we can analyze Algorithm 1 as minimizing the entropy of the distribution over translations of U,Fundamental,Idea,1
52,We used the following scoring functions in our experiments Lengthnormalized Score Each translated sentence pair t  s is scored according to the model probability p t  s normalized by the length t of the target sentence Scoret  s   p t  s 11 3 Confidence Estimation The confidence estimation which we implemented follows the approaches suggested in   The confidence score of a target sentence t is calculated as a loglinear combination of phrase posterior probabilities  Levenshteinbased word posterior probabilities  and a target language model score,Fundamental,Idea,1
53,One language pair creates data for another language pair and can be naturally used in a  style cotraining algorithm,BackGround,GRelated,1
54,These lists are rescored with the following models a the different models used in the decoder which are described above  b two different features based on IBM Model l    c posterior probabilities for words  phrases  ngrams  and sentence length    all calculated over the Nbest list and using the sentence probabilities which the baseline system assigns to the translation hypotheses,Fundamental,Basis,1
55,In    a generative model for word alignment is trained using unsupervised learning on parallel text,BackGround,GRelated,1
56,In   cotraining is applied to MT,BackGround,GRelated,1
57,Along similar lines    combine a generative model of word alignment with a loglinear discriminative model trained on a small set of hand aligned sentences,BackGround,GRelated,1
58,BLEU score using the algorithm described in  ,,,
59,The models or features which are employed by the decoder are a one or several phrase tables  which model the translation direction p s 1 1  b one or several ngram language models trained with the SRILM toolkit   in the experiments reported here  we used 4gram models on the NIST data  and a trigram model on EuroParl  c a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase  and d a word penalty,Fundamental,Basis,1
60,for a detailed description see  ,BackGround,SRelated,1
61,For details  see  ,BackGround,SRelated,1
62,It overlaps with the original phrase tables  but also contains many new phrase pairs  ,BackGround,SRelated,1
63,Selftraining for SMT was proposed in  ,BackGround,GRelated,1
64,Recently  Cabezas and Resnik   experimented with incorporating WSD translations into Pharaoh  a stateoftheart phrasebased MT system  ,BackGround,GRelated,2
65,The relatively small improvement reported by Cabezas and Resnik   without a statistical significance test appears to be inconclusive,BackGround,GRelated,3
66,Note that comparing with the MT systems used in   and    the Hiero system we are using represents a much stronger baseline MT system upon which the WSD system must improve,Compare,Compare,3
67,Carpuat and Wu   integrated the translation predictions from a Chinese WSD system   into a ChineseEnglish wordbased statistical MT system using the ISI ReWrite decoder  ,BackGround,GRelated,1
68,Note that the experiments in   did not use a stateoftheart MT system  while the experiments in   were not done using a fullfledged MT system and the evaluation was not on how well each source sentence was translated as a whole,BackGround,GRelated,3
69,We obtain accuracy that compares favorably to the best participating system in the task  ,Fundamental,Basis,1
70,For our experiments  we use the SVM implementation of   as it is able to work on multiclass problems to output the classification probability for each class,Fundamental,Basis,2
71,Capitalizing on the strength of the phrasebased approach  Chiang   introduced a hierarchical phrasebased statistical MT system  Hiero  which achieves significantly better translation performance than Pharaoh    which is a stateoftheart phrasebased statistical MT system,BackGround,GRelated,1
72,In this paper  we successfully integrate a stateoftheart WSD system into the stateoftheart hierarchical phrasebased MT system  Hiero  ,Fundamental,Basis,2
73,Hiero   is a hierarchical phrasebased model for statistical machine translation  based on weighted synchronous contextfree grammar CFG  ,BackGround,SRelated,1
74,Similar to    we trained the Hiero system on the FBIS corpus  used the NIST MT 2002 evaluation test set as our development set to tune the feature weights  and the NIST MT 2003 evaluation test set as our test data,Fundamental,Idea,1
75,Following    we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores   based on caseinsensitive ngram matching  where n is up to 4,Fundamental,Idea,1
76,A ngram language model adds a dependence on n1 neighboring targetside words    making decoding much more difficult but still polynomial in this paper  we add features that depend on the neighboring sourceside words  which does not affect decoding complexity at all because the source string is fixed,BackGround,SRelated,1
77,The improvement of 057 is statistically significant at p   005 using the signtest as described by Collins et al    with 374 1  318 1 and 227 0,Fundamental,Basis,2
78,To perform translation  stateoftheart MT systems use a statistical phrasebased approach   by treating phrases as the basic units of translation,BackGround,GRelated,1
79,The word alignments of both directions are then combined into a single set of alignments using the diagand method of Koehn et al  ,Fundamental,Basis,1
80,Prior research has shown that using Support Vector Machines SVM as the learning algorithm for WSD achieves good results  ,BackGround,GRelated,2
81,Our implemented WSD classifier uses the knowledge sources of local collocations  partsofspeech POS  and surrounding words  following the successful approach of  ,Fundamental,Idea,2
82,First  we performed word alignment on the FBIS parallel corpus using GIZA   in both directions,Fundamental,Basis,1
83,Hiero uses a general loglinear model   where the weight of a derivation  D for a particular source sentence and its translation is where    i is a feature function and  X i is the weight for feature    i,BackGround,SRelated,1
84,Using the MT 2002 test set  we ran the minimumerror rate training MERT   with the decoder to tune the weights for each feature,Fundamental,Basis,1
85,the English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus  we trained a trigram language model using the SRI Language Modelling Toolkit  ,Fundamental,Basis,1
86,WSD approaches can be classified as a knowledgebased approaches  which make use of linguistic knowledge  manually coded or extracted from lexical resources   b corpusbased approaches  which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models   and c hybrid approaches  which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge  ,BackGround,GRelated,1
87,Although it has been argued that WSD does not yield better translation quality than a machine translation system alone  it has been recently shown that a WSD module that is developed following specific multilingual requirements can significantly improve the performance of a machine translation system  ,BackGround,GRelated,2
88,Finally  MCWSD   is a multiclass averaged perceptron classifier using syntactic and narrow context features  with one component trained on the data provided by Senseval and other trained on WordNet glosses,BackGround,SRelated,1
89,It is an interesting approach to learning which has been considered promising for several applications in natural language processing and has been explored for a few of them  namely POStagging  grammar acquisition and semantic parsing  ,BackGround,GRelated,1
90,For example  Dang and Palmer   also use a rich set of features with a traditional learning algorithm maximum entropy,BackGround,GRelated,1
91,Linguistic knowledge is available in electronic resources suitable for practical use  such as WordNet    dictionaries and parsers,BackGround,GRelated,1
92,There is not always a direct relation between the possible senses for a word in a monolingual lexicon and its translations to a particular language  so this represents a different task to WSD against a monolingual lexicon  ,BackGround,GRelated,1
93,CLaC1   uses a Naive Bayes algorithm with a dynamically adjusted context window around the target word,BackGround,SRelated,1
94,The sense with the highest count of overlapping words in its dictionary definition and in the sentence containing the target verb excluding stop words        represented by has_overlappingsentence  translation  has_overlappingsnt 1  voltar,BackGround,SRelated,1
95,Verbs and possible senses in our corpus Both corpora were lemmatized and partofspeech POS tagged using Minipar   and Mxpost    respectivelly,Fundamental,Basis,1
96,These approaches have shown good results particularly those using supervised learning  ,BackGround,GRelated,2
97,WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories  ,BackGround,GRelated,2
98,Syntalex3   is based on an ensemble of bagged decision trees with narrow context partofspeech features and bigrams,BackGround,SRelated,1
99,This is achieved using Inductive Logic Programming ILP    which has not yet been applied to WSD,Fundamental,Basis,1
100,Inductive Logic Programming   employs techniques from Machine Learning and Logic Programming to build firstorder theories from examples and background knowledge  which are also represented by firstorder clauses,BackGround,SRelated,1
101,2 A more specific clause the bottom clause is built using inverse entailment    generally consisting of the representation of all the knowledge about that example,Fundamental,Basis,1
102,This corpus was automatically annotated with the translation of the verb using a tagging system based on parallel corpus  statistical information and translation dictionaries    followed by a manual revision,Fundamental,Basis,1
103,All the knowledge sources were made available to be used by the inference engine  since previous experiments showed that they are all relevant  ,BackGround,GRelated,1
104,We use the Aleph ILP system    which provides a complete inference engine and can be customized in various ways,Fundamental,Basis,1
105,In the hybrid approaches that have been explored so far  deep knowledge  like selectional preferences  is either preprocessed into a vector representation to accommodate machine learning algorithms  or used in previous steps to filter out possible senses eg  ,BackGround,GRelated,1
106,Roark and Bacchiani   showed that weighted countmerging is a special case of maximum a posteriori MAP estimation  and successfully used it for probabilistic contextfree grammar domain adaptation   and language model adaptation  ,BackGround,GRelated,2
107,We have recently shown that this algorithm is effective in estimating the sense priors of a set of nouns  ,BackGround,SRelated,2
108,However  in    we showed that in a supervised setting where one has access to some annotated training data  the EMbased method in section 5 estimates the sense priors more effectively than the method described in  ,Compare,Compare,1
109,A similar work is the recent research by Chen et al    where active learning was used successfully to reduce the annotation effort for WSD of 5 English verbs using coarsegrained evaluation,BackGround,SRelated,1
110,This is slightly higher than the 58 senses per verb in    where the experiments were conducted using coarsegrained evaluation,Compare,Compare,1
111,For WSD  Fujii et al   used selective sampling for a Japanese language WSD system  Chen et al   used active learning for 5 verbs using coarsegrained evaluation  and H,BackGround,GRelated,1
112,Dang   employed active learning for another set of 5 verbs,BackGround,GRelated,1
113,To investigate this  Escudero et al   and Martinez and Agirre   conducted experiments using the DSO corpus  which contains sentences from two different corpora  namely Brown Corpus BC and Wall Street Journal WSJ,BackGround,GRelated,1
114,Escudero et al   pointed out that one of the reasons for the drop in accuracy is the difference in sense priors ie  the proportions of the different senses of a word between BC and WSJ,BackGround,GRelated,1
115,Following the setup of    we similarly made use of the DSO corpus to perform our experiments on domain adaptation,Fundamental,Idea,1
116,As mentioned in section 1  research in   noted an improvement in accuracy when they adjusted the BC and WSJ datasets such that the proportions of the different senses of each word were the same between BC and WSJ,BackGround,SRelated,1
117,Escudero et al   used the DSO corpus to highlight the importance of the issue of domain dependence of WSD systems  but did not propose methods such as active learning or countmerging to address the specific problem of how to perform domain adaptation for WSD,BackGround,GRelated,1
118,This is similar to the approach taken in   where they focus on determining the predominant sense of words in corpora drawn from finance versus sports domains,Fundamental,Idea,1
119,Research by McCarthy et al   and Koeling et al   pointed out that a change of predominant sense is often indicative of a change in domain,BackGround,GRelated,1
120,These knowledge sources were effectively used to build a stateoftheart WSD program in one of our prior work  ,BackGround,SRelated,2
121,To reduce the effort required to adapt a WSD system to a new domain  we employ an active learning strategy   to select examples to annotate from the new domain of interest,Fundamental,Basis,1
122,With active learning    we use uncertainty sampling as shown r   WSD system trained on D T b   word sense prediction for d using r  p   confidence of prediction b if p  p  min then Figure 1 Active learning in Figure 1,Fundamental,Basis,1
123,The WordNet Domains resource   assigns domain labels to synsets in WordNet,BackGround,SRelated,1
124,Among the few currently available manually senseannotated corpora for WSD  the SEMCOR SC corpus   is the most widely used,BackGround,GRelated,1
125,The DSO corpus   contains 192 800 annotated examples for 121 nouns and 70 verbs  drawn from BC and WSJ,BackGround,SRelated,1
126,In this section  we describe an EMbased algorithm that was introduced by Saerens et al    which can be used to estimate the sense priors  or a priori probabilities of the different senses in a new dataset,Fundamental,Basis,1
127,Most of this section is based on  ,Fundamental,Basis,1
128,In applying active learning for domain adaptation  Zhang et al   presented work on sentence boundary detection using generalized Winnow  while Tur et al   performed language model adaptation of automatic speech recognition systems,BackGround,GRelated,1
129,In most contexts  the similarity between chocolate  say  and a narcotic like heroin will meagerly reflect the simple ontological fact that both are kinds of substances certainly  taxonomic measures of similarity as discussed in Budanitsky and Hirst   will capture little more than this commonality,BackGround,GRelated,1
130,The function sim arg 0 CAT reflects the perceived similarity between the putative member arg 0 and a synset CAT in WordNet  using one of the standard formulations described in Budanitsky and Hirst  ,BackGround,SRelated,1
131,Whissell   reduces the notion of affect to a single numeric dimension  to produce a dictionary of affect that associates a numeric value in the range 10 most unpleasant to 30  ,BackGround,SRelated,1
132,We have described an approach that can be seen as a functional equivalent to the CPA Corpus Pattern Analysis approach of Pustejovsky et al    in which our goal is not that of automated induction of word senses in context as it is in CPA but the automated induction of flexible  contextsensitive category structures,Fundamental,Basis,1
133,Since the line between literal and metaphoric uses of a category is often impossible to draw  the best one can do is to accept metaphor as a gradable phenomenon  ,BackGround,SRelated,1
134,The most revealing variations are syntagmatic in nature  which is to say  they look beyond individual word forms to larger patterns of contiguous usage  ,BackGround,GRelated,1
135,Dices coefficient   is used to implement this measure,Fundamental,Basis,1
136,As noted by De Leenheer and de Moor    ontologies are lexical representations of concepts  so we can expect the effects of context on language use to closely reflect the effects of context on ontologLinguistic variation across contexts is often symp ical structure,BackGround,SRelated,1
137,While simile is a mechanism for highlighting interconcept similarity  metaphor is at heart a mechanism of category inclusion  ,BackGround,SRelated,1
138,Glucksberg   notes that the same category  used figuratively  can exhibit different qualities in different metaphors,BackGround,SRelated,1
139,In this section  we describe how we use Markov chain Monte Carlo methods to perform inference in the statistical models described in the previous section Andrieu et al   provide an excellent introduction to MCMC techniques,BackGround,SRelated,1
140,These are short statements that restrict the space of languages in a concrete way for instance objectverb ordering implies adjectivenoun ordering Croft    Hawkins   and Song   provide excellent introductions to linguistic typology,BackGround,GRelated,2
141,This is a welldocumented issue see  eg    stemming from the fact that any set of languages is not sampled uniformly from the space of all probable languages,BackGround,SRelated,1
142,The closest work is represented by the books Possible and Probable Languages   and Language Classification by Numbers    but the focus of these books is on automatically discovering phylogenetic trees for languages based on IndoEuropean cognate sets  ,BackGround,GRelated,1
143,Those that reference Hawkins eg  11 are based on implications described by Hawkins   those that reference Lehmann are references to the principles decided by Lehmann   in Ch 4  8,Fundamental,Basis,1
144,They have also been used computationally to aid in the learning of unsupervised part of speech taggers  ,BackGround,GRelated,1
145,For instance our 7 is implication 18 from Greenberg  reproduced by Song  ,Fundamental,Basis,1
146,We examined sentences using a phrase structure parser   and an HPSG parser  ,Fundamental,Basis,1
147,Since the number of parameters in NLM is still large  several smoothing methods are used   to produce more accurate probabilities  and to assign nonzero probabilities to any word string,BackGround,GRelated,1
148,We would like to see more refined online learning methods with kernels   that we could apply in these areas,BackGround,MRelated,1
149,Therefore we make use of an online learning algorithm proposed by    which has a much smaller computational cost,Fundamental,Basis,1
150,Blei  2003 Wang et al  2005  our result may encourage the study ofthe combination offeatures forlanguage modeling,,,
151,We used a Viterbi decoding   for the partition,Fundamental,Basis,1
152,Discriminative language models DLMs have been proposed to classify sentences directly as correct or incorrect    and these models can handle both nonlocal and overlapping information,BackGround,GRelated,1
153,For fast kernel computation  the Polynomial Kernel Inverted method PKI is proposed    which is an extension of Inverted Index in Information Retrieval,BackGround,SRelated,1
154,The class model was originally proposed by  ,BackGround,SRelated,1
155,However  by considering only those counts that actually change  the algorithm can be made to scale somewhere between linearly and quadratically to the number of classes  ,BackGround,SRelated,1
156,Recently  Whole Sentence Maximum Entropy Models   WSMEs have been introduced,BackGround,GRelated,1
157,In our experiments  we did not examine the result of using other sampling methods  For example  it would be possible to sample sentences from a whole sentence maximum entropy model   and this is a topic for future research,BackGround,MRelated,1
158,A contrastive estimation method   is similar to ours with regard to constructing pseudonegative examples,Fundamental,Idea,1
159,If the kerneltrick   is applied to online marginbased learning  a subset of the observed examples  called the active set  needs to be stored,BackGround,SRelated,1
160,It should be noted that models based on finite state transducers have been shown to be adequate for describing fusion as well   and further work should evaluate these types of models in ASR of languages with higher indexes of fusion,BackGround,GRelated,1
161,The final approach applies a manually constructed rulebased morphological tagger ,Fundamental,Basis,1
162,For training the LMs  a subset of 43 million words from the Estonian Segakorpus was used   preprocessed with a morphological analyzer ,Fundamental,Basis,1
163,In   a WER of 445 was obtained with wordbased trigrams and a WER of 372 with items similar to ones from grammar using the same speech corpus as in this work,BackGround,SRelated,1
164,It should be noted that every OOV causes roughly two errors in recognition  and vocabulary decomposition approaches such as the ones evaluated here give some benefits to word error rate WER even in recognizing languages such as English ,BackGround,SRelated,1
165,This is similar to what was introduced as flat hybrid model   and it tries to model OOVwords as sequences of words and fragments,Fundamental,Idea,1
166,The results for hybrid are in in the range suggested by earlier work ,BackGround,SRelated,1
167,The morph approach was developed for the needs of Finnish speech recognition  which is a high synthesis  moderate fusion and very low orthographic irregularity language  whereas the hybrid approach in   was developed for English  which has low synthesis  moderate fusion  and very high orthographic irregularity,BackGround,GRelated,1
168,Varigrams  are used in this work  and to make LMs trained with each approach comparable  the varigrams have been grown to roughly sizes of 5 million counts,Fundamental,Basis,1
169,ing approach  growing varigram models  were used with no limits as to the order of ngrams  but limiting the number of counts to 48 and 5 million counts,Fundamental,Basis,1
170,For example  in English with language models LM of 60k words trained from the Gigaword Corpus V2   and testing on a very similar Voice of America portion of TDT4 speech corpora   this gives a OOV rate of 15,BackGround,SRelated,1
171,Models of this type have previously been shown to yield very good g2p conversion results  ,BackGround,GRelated,2
172,It has been argued that using morphological information is important for languages where morphology has an important influence on pronunciation  syllabiication and word stress such as German  Dutch  Swedish or  to a smaller extent  also English  ,BackGround,GRelated,1
173,Decision trees were one of the first databased approaches to g2p and are still widely used  ,BackGround,GRelated,1
174,Best results were obtained when using a variant of Modified KneserNey Smoothing 2  ,BackGround,SRelated,2
175,  also used a joint ngram model,BackGround,SRelated,1
176,We compared four different stateoftheart unsupervised systems for morphological decomposition cf  ,Compare,Compare,1
177,In very recent work    developed an unsupervised algorithm fmeas 68 an extension of RePortS whose segmentations improve g2p when using a the decision tree PER 345,BackGround,GRelated,2
178,The German corpus used in these experiments is CELEX  ,Fundamental,Basis,1
179,Among the unsupervised systems  best results 7 on the g2p task with morphological annotation were obtained with the RePortS system  ,Fundamental,Basis,2
180,The same algorithms have previously been shown to help a speech recognition task  ,BackGround,GRelated,2
181,The joint ngram model performs significantly better than the decision tree essentially based on    and achieves scores comparable to the Pronunciation by Analogy PbA algorithm  ,Compare,Compare,2
182,This is much faster than the times for Pronunciation by Analogy PbA   on the same corpus,Compare,Compare,2
183,Examples of such approaches using Hidden Markov Models are   who applied the HMM to the related task of phonemetographeme conversion    and  ,BackGround,GRelated,1
184,For German    show that information about stress assignment and the position of a syllable within a word improve g2p conversion,BackGround,GRelated,1
185,Vowel length and quality has been argued to also depend on morphological structure  ,BackGround,GRelated,1
186,The two rulebased systems we evaluated  the ETI 4 morphological system and SMOR 5    are both highquality systems with large lexica that have been developed over several years,Fundamental,Basis,2
187,We used the syllabifier described in    which works similar to the joint ngram model used for g2p conversion,Fundamental,Basis,1
188,A possible reason for the observed dichotomy in the behavior of the vowel and consonant inventories with respect to redundancy can be as follows while the organization of the vowel inventories is known to be governed by a single force  the maximal perceptual contrast    consonant inventories are shaped by a complex interplay of several forces  ,BackGround,GRelated,1
189,It has been postulated earlier by functional phonologists that such regularities are the consequences of certain general principles like maximal perceptual contrast    which is desirable between the phonemes of a language for proper perception of each individual phoneme in a noisy environment  ease of articulation    which requires that the sound systems of all languages are formed of certain universal and highly frequent sounds  and ease of learnability   which is necessary for a speaker to learn the sounds of a language with minimum effort,BackGround,GRelated,1
190,Such an observation is significant since whether or not these principles are similardifferent for the two inventories had been a question giving rise to perennial debate among the past researchers  ,BackGround,GRelated,1
191,On the other hand  in spite of several attempts   the organization of the consonant inventories lacks a satisfactory explanation,BackGround,GRelated,3
192,Various attempts have been made in the past to explain the aforementioned trends through linguistic insights   mainly establishing their statistical significance,BackGround,GRelated,1
193,For instance  in biological systems we find redundancy in the codons    in the genes   and as well in the proteins  ,BackGround,GRelated,1
194,In fact  the organization of the vowel inventories especially those with a smaller size across languages has been satisfactorily explained in terms of the single principle of maximal perceptual contrast  ,BackGround,GRelated,2
195,This redundancy is present mainly to reduce the risk of the complete loss of information that might occur due to accidental errors  ,BackGround,GRelated,1
196,Many typological studies   of segmental inventories have been carried out in past on the UCLA Phonological Segment Inventory Database UPSID  ,BackGround,GRelated,1
197,In order to explain these trends  feature economy was proposed as the organizing principle of the consonant inventories  ,BackGround,GRelated,1
198,Inspired by the aforementioned studies and the concepts of information theory   we try to quantitatively capture the amount of redundancy found across the consonant Table 1 The table shows four plosives,Fundamental,Idea,1
199,For this purpose  we present an information theoretic definition of redundancy  which is calculated based on the set of features 1   that are used to express the consonants,Fundamental,Basis,1
200,However  one of the earliest observations about the consonant inventories has been that consonants tend to occur in pairs that exhibit strong correlation in terms of their features  ,BackGround,GRelated,1
201,2 Previous Work Previous work  eg    has mostly assumed that one has a training lexicon of transliteration pairs  from which one can learn a model  often a sourcechannel or MaxEntbased model,BackGround,GRelated,1
202,A linear classifier is trained using the Winnow algorithm from the SNoW toolkit  ,Fundamental,Basis,1
203,Using comparable corpora  the namedentities for persons and locations were extracted from the English text in this paper  the English namedentities were extracted using the namedentity recognizer described in Li et al    based on the SNoW machine learning toolkit  ,Fundamental,Basis,1
204,This is quite small compared to previous approaches such as Knight and Graehl   or Gao et al  ,BackGround,SRelated,1
205,Gildea and Jurafsky   counted the number of features whose values are different  and used them as a substitution cost,BackGround,GRelated,1
206,Halle and Clements  s distinctive features are used in order to model the substitution insertiondeletion costs for the stringalignment algorithm and linear classifier,Fundamental,Basis,1
207,All pronunciations are based on the WorldBet transliteration system    an asciionly version of the IPA,Fundamental,Basis,1
208,a For all the training data  the pairs of pronunciations are aligned using standard string alignment algorithm based on Kruskal  ,Fundamental,Basis,1
209,For the set of features X and set of weights W  the linear classifier is defined as 1   X    X   X2  ,,,
210,In this paper  the phonetic transliteration is performed using the following steps 1  Generation of the pronunciation for English words and target words a Pronunciations for English words are obtained using the Festival texttospeech system  ,Fundamental,Basis,1
211,Based on the pronunciation error data of learners of English as a second language as reported in    we propose the use of what we will term pseudofeatures,Fundamental,Basis,1
212,Examples of the top3 candidates in the transliteration of EnglishKorean To evaluate the proposed transliteration methods quantitatively  the Mean Reciprocal Rank MRR  a measure commonly used in information retrieval when there is precisely one correct answer   was measured  following Tao and Zhai  ,Fundamental,Idea,1
213,In our work  we adopt the method proposed in   and apply it to the problem of transliteration,Fundamental,Basis,1
214,The substitutioninsertiondeletion cost for the string alignment algorithm is based on the baseline cost from  ,Fundamental,Basis,1
215,The pseudo features in this study are same as in Tao et al  ,Fundamental,Idea,1
216,MRRs of the phonetic transliteration The baseline was computed using the phonetic transliteration method proposed in Tao et al  ,Fundamental,Basis,1
217,By treating a lettercharacter as a word and a group of letterscharacters as a phrase or token unit in SMT  one can easily apply the traditional SMT models  such as the IBM generative model   or the phrasebased translation model   to transliteration,BackGround,GRelated,1
218,In G2P studies  Font Llitjos and Black   showed how knowledge of language of origin may improve conversion accuracy,BackGround,GRelated,1
219,Phonetic transliteration can be considered as an extension to the traditional graphemetophoneme G2P conversion    which has been a muchresearched topic in the field of speech processing,BackGround,GRelated,1
220,Many of the loanwords exist in todays Chinese through semantic transliteration  which has been well received   by the people because of many advantages,BackGround,GRelated,2
221,Unfortunately semantic transliteration  which is considered as a good tradition in translation practice    has not been adequately addressed computationally in the literature,BackGround,GRelated,3
222,5S Semantic Transliteration The performance was measured using the Mean Reciprocal Rank MRR metric    a measure that is commonly used in information retrieval  assuming there is precisely one correct answer,Fundamental,Basis,1
223,In computational linguistic literature  much effort has been devoted to phonetic transliteration  such as EnglishArabic  EnglishChinese    EnglishJapanese   and EnglishKorean,BackGround,GRelated,1
224,In the extraction of transliterations  datadriven methods are adopted to extract actual transliteration pairs from a corpus  in an effort to construct a large  uptodate transliteration lexicon  ,BackGround,GRelated,1
225,The Latinscripted personal names are always assumed to homogeneously follow the English phonic rules in automatic transliteration  ,BackGround,GRelated,1
226,This model is conceptually similar to the joint sourcechannel model   where the target token t i depends on not only its source token s i but also the history t i1 and s i 1,Fundamental,Idea,1
227,Some recent work   has attempted to introduce preference into a probabilistic framework for selection of Chinese characters in phonetic transliteration,BackGround,GRelated,1
228,In transliteration modeling  transliteration rules are trained from a large  bilingual transliteration lexicon    with the objective of translating unknown words on the fly in an open  general domain,BackGround,GRelated,1
229,As discussed elsewhere    out of several thousand common Chinese characters  a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese  egonly 731 Chinese characters are adopted in the EC corpus,BackGround,SRelated,1
230,As a Chinese transliteration can arouse to certain connotations  the choice of Chinese characters becomes a topic of interest  ,BackGround,GRelated,1
231,Table 4 Lexicon statistics For Arabic  as a fullsize Arabic lexicon was not available to us  we used the Buckwalter morphological analyzer   to derive a lexicon,Fundamental,Basis,1
232,For example    showed that factored language models  which consider morphological features and use an optimized backoff policy  yield lower perplexity,BackGround,GRelated,1
233,A recent work   experimented with EnglishtoTurkish translation with limited success  suggesting that inflection generation given morphological features may give positive results,BackGround,GRelated,1
234,More recently    achieved improvements in CzechEnglish MT  optimizing a Table 1 Morphological features used for Russian and Arabic set of possible source transformations  incorporating morphology,BackGround,GRelated,1
235,Ideally  the best word analysis should be provided as a result of contextual disambiguation eg    we leave this for future work,BackGround,MRelated,2
236,Another work   showed improvements by splitting compounds in German,BackGround,GRelated,2
237,Translating from a morphologypoor to a morphologyrich language is especially challenging since detailed morphological information needs to be decoded from a language that does not encode this information or does so only implicitly  ,BackGround,GRelated,1
238,Koehn   includes a survey of statistical MT systems in both directions for the Europarl corpus  and points out the challenges of this task,BackGround,GRelated,1
239,For example  it has been shown   that determiner segmentation and deletion in Arabic sentences in an ArabictoEnglish translation system improves sentence alignment  thus leading to improved overall translation quality,BackGround,GRelated,2
240,For Arabic  we apply the following heuristic use the most frequent analysis estimated from the gold standard labels in the Arabic Treebank   if a word does not appear in the treebank  we choose the first analysis returned by the Buckwalter analyzer,Fundamental,Basis,1
241,Our learning framework uses a Maximum Entropy Markov model  ,Fundamental,Basis,1
242,  demonstrated that a similar level of alignment quality can be achieved with smaller corpora applying morphosyntactic source restructuring  using hierarchical lexicon models  in translating from German into English,BackGround,GRelated,1
243,The sentence pairs were wordaligned using GIZA   and submitted to a treeletbased MT system    which uses the word dependency structure of the source language and projects word dependency structure to the target language  creating the structure shown in Figure 1 above,Fundamental,Basis,1
244,  experimented successfully with translating from inflectional languages into English making use of POS tags  word stems and suffixes in the source language,BackGround,GRelated,2
245,The framework suggested here is most closely related to    which uses a probabilistic model to generate Japanese case markers for EnglishtoJapanese MT,BackGround,SRelated,1
246,The algorithm is similar to the one described in  ,Fundamental,Idea,1
247,It also differs from now traditional uses of comparable corpora for detecting translation equivalents   or extracting terminology    which allows a onetoone correspondence irrespective of the context,Compare,Compare,1
248,In the spirit of    it is intended as a translators amenuensis under the tight control of a human translator ,Fundamental,Idea,1
249,It has been aligned on the sentence level by JAPA    and further on the word level by GIZA  ,Fundamental,Basis,1
250,Thus the present system is unlike SMT    where lexical selection is effected by a translation model based on aligned  parallel corpora  but the novel techniques it has developed are exploitable in the SMT paradigm,Compare,Compare,1
251,Similarity is measured as the cosine between collocation vectors  whose dimensionality is reduced by SVD using the implementation by Rapp  ,Fundamental,Basis,1
252,We have generalised the method used in our previous study   for extracting equivalents for continuous multiword expressions MWEs,Fundamental,Basis,1
253,Recent efforts in statistical machine translation MT have seen promising improvements in output quality  especially the phrasebased models   and syntaxbased models  ,BackGround,GRelated,1
254,By adapting the kbest parsing Algorithm 2 of Huang and Chiang    it achieves significant speedup over fullintegration on Chiangs Hiero system,BackGround,GRelated,2
255, We also devise a faster variant of cube pruning  called cube growing  which uses a lazy version of kbest parsing   that tries to reduce k to the minimum needed at each node to obtain the desired number of hypotheses at the root,Fundamental,Basis,1
256,In a nutshell  cube pruning works on the LM forest  keeping at most k LM items at each node  and uses the kbest parsing Algorithm 2 of Huang and Chiang   to speed up the computation,BackGround,SRelated,1
257,This situation is very similar to kbest parsing and we can adapt the Algorithm 2 of Huang and Chiang   here to explore this grid in a bestfirst order,Fundamental,Basis,1
258,This new method  called cube growing  is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang    is a lazy version of Algorithm 2 see Table 1,Fundamental,Idea,1
259,The different target sides then constitute a third dimension of the grid  forming a cube of possible combinations  ,BackGround,SRelated,1
260,The data set is same as in Section 51  except that we also parsed the Englishside using a variant of the Collins   parser  and then extracted 247M treetostring rules using the algorithm of  ,Fundamental,Basis,1
261,These forest rescoring algorithms have potential applications to other computationally intensive tasks involving combinations of different models  for example  headlexicalized parsing   joint parsing and semantic role labeling   or tagging and parsing with nonlocal features,BackGround,MRelated,1
262,In treetostring also called syntaxdirected decoding    the source string is first parsed into a tree  which is then recursively converted into a target string according to transfer rules in a synchronous grammar  ,BackGround,GRelated,1
263,We generalize cube pruning and adapt it to two systems very different from Hiero a phrasebased system similar to Pharaoh   and a treetostring system  ,Fundamental,Basis,1
264,We test our methods on two largescale EnglishtoChinese translation systems a phrasebased system and our treetostring system  ,Fundamental,Basis,1
265,Our data preparation follows Huang et al   the training data is a parallel corpus of283M words on the English side  and a trigram language model is trained on the Chinese side,Fundamental,Idea,1
266,For cube growing  we use a nonduplicate kbest method   to get 100best unique translations according to  LM to estimate the lowerbound heuristics,Fundamental,Basis,1
267,Since our treetostring rules may have many variables  we first binarize each hyperedge in the forest on the target projection  ,Fundamental,Basis,1
268,Thus we envision forest rescoring as being of general applicability for reducing complicated search spaces  as an alternative to simulated annealing methods  ,BackGround,MRelated,1
269,Part of the complexity arises from the expressive power of the translation model for example  a phrase or wordbased model with full reordering has exponential complexity  ,BackGround,GRelated,1
270,We will use the following example from Chinese to English for both systems described in this section yU  Shalong juxing le huitan with Sharon hold   past meeting held a meeting with Sharon A typical phrasebased decoder generates partial targetlanguage outputs in lefttoright order in the form of hypotheses  ,BackGround,GRelated,1
271,We implemented Cubit  a Python clone of the Pharaoh decoder    3 and adapted cube pruning to it as follows,Fundamental,Basis,1
272,We set the decoder phrasetable limit to 100 as suggested in   and the distortion limit to 4,Fundamental,Idea,1
273,An SCFG   is a contextfree rewriting system for generating string pairs,BackGround,SRelated,1
274,To integrate with a bigram language model  we can use the dynamicprogramming algorithms of Och and Ney   and Wu   for phrasebased and SCFGbased systems  respectively  which we may think of as doing a inergrained version of the deductions above,BackGround,SRelated,1
275,The language model also  if fully integrated into the decoder  introduces an expensive overhead for maintaining targetlanguage boundary words for dynamic programming  ,BackGround,GRelated,1
276,Similarly  the decoding problem with SCFGs can also be cast as a deductive parsing system  ,BackGround,SRelated,1
277,However  the hope is that by choosing the right value ofi  these estimates will be accurate enough to affect the search quality only slightly  which is analogous to almost admissible heuristics in A search  ,BackGround,SRelated,1
278,A few exceptions are the hierarchical possibly syntaxbased transduction models   and the string transduction models  ,BackGround,GRelated,1
279,The SFST approach described here is similar to the one described in   which has subsequently been adopted by  ,Fundamental,Idea,1
280,In preliminary experiments  we have associated the target lexical items with supertag information  ,Fundamental,Basis,1
281,We separate the most popular classification techniques into two broad categories also called Maxent as it finds the distribution with maximum entropy that properly estimates the average of each feature over the training data  ,,,
282,Most of the previous work on statistical machine translation  as exemplified in    employs wordalignment algorithm such as GIZA   that provides local associations between source and target words,BackGround,GRelated,1
283,The BOW approach is different from the parsing based approaches   where the translation model tightly couples the syntactic and lexical items of the two languages,Compare,Compare,1
284,The excellent results recently obtained with the SEARN algorithm   also suggest that binary classifiers  when properly trained and combined  seem to be capable ofmatching more complex structured output approaches,BackGround,GRelated,2
285,A new L1regularized Maxent algorithms was proposed for density estimation   and we adapted it to classification,Fundamental,Basis,1
286,From the bilanguage corpus B  we train an ngram language model using standard tools  ,Fundamental,Basis,1
287,The use of supertags in phrasebased SMT system has been shown to improve results  ,BackGround,GRelated,2
288,here all state hypotheses of a whole sentence are kept in memory  it is necessary to either use heuristic forward pruning or constrain permutations to be within a local window of adjustable size also see  ,BackGround,SRelated,1
289,Although Conditional Random Fields CRF   train an exponential model at the sequence level  in translation tasks such as ours the computational requirements of training such models are prohibitively expensive,BackGround,SRelated,1
290,We found this algorithm to converge faster than the current stateoftheart in Maxent training  which is L2regularized LBFGS   1,Compare,Compare,2
291,Discriminative training has been used mainly for translation model combination   and with the exception of    has not been used to directly train parameters of a translation model,BackGround,GRelated,1
292,For the work reported in this paper  we have used the GIZA tool   which implements a stringalignment algorithm,Fundamental,Basis,1
293,Each output label t is projected into a bit string with components b j t where probability of each component is estimated independently In practice  despite the approximation  the 1vsother scheme has been shown to perform as well as the multiclass scheme  ,BackGround,SRelated,1
294,For the Hansard corpus we used the same training and test split as in   14 million training sentence pairs and 5432 test sentences,Fundamental,Idea,1
295,In search of a balance between structural flexibility and computational complexity  several authors have proposed constraints to identify classes of nonprojective dependency structures that are computationally wellbehaved  ,BackGround,GRelated,2
296,This result generalizes previous work on the relation between ltag and dependency representations  ,BackGround,SRelated,1
297, The encoding of dependency structures as orderannotated trees allows us to reformulate two constraints on nonprojectivity originally defined on fully specified dependency structures   in terms of syntactic properties of the order annotations that they induce Gapdegree The gapdegree of a dependency structure is the maximum over the number of discontinuities in any yield of that structure,Fundamental,Basis,1
298,This enables us to generalize a previous result on the class of dependency structures generated by lexicalized tags   to the class of generated dependency languages  LTAL,Fundamental,Basis,1
299,Lately  they have also been used in many computational tasks  such as relation extraction    parsing    and machine translation  ,BackGround,GRelated,1
300,Unfortunately  most formal results on nonprojectivity are discouraging While grammardriven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized contextfree grammar    parsing is prohibitively expensive when unrestricted forms of nonprojectivity are permitted  ,BackGround,GRelated,3
301,We also show that adding the wellnestedness condition corresponds to the restriction of lcfrs to Coupled ContextFree Grammars    and that regular sets of wellnested structures with a gapdegree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoining Grammar ltag,Fundamental,Basis,1
302,This restriction is central to the formalism of CoupledContextFree Grammar ccfg  ,BackGround,SRelated,1
303,REGD wk  LCCFLk  1 As a special case  CoupledContextFree Grammars with fanout 2 are equivalent to Tree Adjoining Grammars tags  ,BackGround,SRelated,1
304,This gives rise to a notion of regular dependency languages  and allows us to establish a formal relation between the structural constraints and mildly contextsensitive grammar formalisms   We show that regular dependency languages correspond to the sets of derivations of lexicalized Linear ContextFree Rewriting Systems lcfrs    and that the gapdegree measure is the structural correspondent of the concept of fanout in this formalism  ,Fundamental,Basis,1
305,Such a comparison may be empirically more adequate than one based on traditional notions of generative capacity  ,BackGround,MRelated,2
306,Both constraints have been shown to be in very good fit with data from dependency treebanks  ,BackGround,GRelated,2
307,A dependency structure is projective  if each of its yields forms an interval with respect to the precedence order  ,BackGround,SRelated,1
308,Datadriven dependency parsing with nonprojective structures is quadratic when all attachment decisions are assumed to be independent of one another    but becomes intractable when this assumption is abandoned  ,BackGround,GRelated,1
309,The number of components in the orderannotation  and hence  the gapdegree of the resulting dependency language  corresponds to the fanout of the function the highest number of components among the arguments of the function  ,BackGround,SRelated,1
310,Linear ContextFree Rewriting Systems Gaprestricted dependency languages are closely related to Linear ContextFree Rewriting Systems lcfrs    a class of formal systems that generalizes several mildly contextsensitive grammar formalisms,BackGround,SRelated,1
311,The UnfoldFold transformation is a calculus for transforming functional and logic programs into equivalent but hopefully faster programs  ,BackGround,SRelated,1
312,Standard methods for converting weighted CFGs to equivalent PCFGs can be used if required  ,BackGround,SRelated,1
313,Second  EisnerSatta On 3 PBDG parsing algorithms are extremely fast  ,BackGround,GRelated,2
314,It is straightforward to extend the splithead CFG to encode the additional state information required by the head automata of Eisner and Satta   this corresponds to splitting the nonterminals L u and uR,BackGround,SRelated,2
315,The On 3 splithead grammar is closely related to the On 3 PBDG parsing algorithm given by Eisner and Satta  ,BackGround,SRelated,1
316,Goodman   observed that the Viterbi parse is in general not the optimal parse for evaluation metrics such as fscore that are based on the number of correct constituents in a parse,BackGround,SRelated,1
317,For example  incremental CFG parsing algorithms can be used with the CFGs produced by this transform  as can the InsideOutside estimation algorithm   and more exotic methods such as estimating adjoined hidden states  ,BackGround,GRelated,1
318,The closest related work we are aware of is McAllester    which also describes a reduction of PBDGs to efficientlyparsable CFGs and directly inspired this work,BackGround,SRelated,2
319,This paper investigates the relationship between ContextFree Grammar CFG parsing and the EisnerSatta PBDG parsing algorithms  including their extension to secondorder PBDG parsing  ,Fundamental,Basis,1
320,First  because they capture bilexical headtohead dependencies they are capable of producing extremely highquality parses stateoftheart discriminatively trained PBDG parsers rival the accuracy of the very best statistical parsers available today  ,BackGround,GRelated,2
321,The steps involved in CKY parsing with this grammar correspond closely to those of the McDonald   secondorder PBDG parsing algorithm,Fundamental,Basis,1
322,These weights are estimated by an online procedure as in McDonald    and are not intended to define a probability distribution,Fundamental,Idea,1
323,We provided one grammar which captures horizontal secondorder dependencies    and another which captures vertical secondorder headtoheadtohead dependencies,Fundamental,Basis,1
324,Since CFGs can be expressed as Hornclause logic programs   and the UnfoldFold transformation is provably correct for such programs    it follows that its application to CFGs is provably correct as well,BackGround,GRelated,1
325,Specifically  we show how to use an offline preprocessing step  the UnfoldFold transformation  to transform a PBDG into an equivalent CFG that can be parsed in On 3 time using a version of the CKY algorithm with suitable indexing    and extend this transformation so that it captures secondorder PBDG dependencies as well,Fundamental,Basis,1
326,By a slight generalization of a result by Aoto    this typing r h N  a must be negatively nonduplicated in the sense that each atomic type has at most one negative occurrence in it,Fundamental,Basis,1
327,By Aoto and Onos   generalization of the Coherence Theorem    it follows that every term P such that r h P  a for some r c r must be Srequal to N and consequently to N,BackGround,SRelated,1
328,The reduction to Datalog makes it possible to apply to parsing and generation sophisticated evaluation techniques for Datalog queries in particular  an application of generalized supplementary magicsets rewriting   automatically yields Earleystyle algorithms for both parsing and generation,BackGround,SRelated,1
329,In the case of an IO macro grammar  the result is an IO contextfree tree grammar   String copying becomes tree copying  and the resulting grammar can be represented by an almost linear CFLG and hence by a Datalog program,BackGround,SRelated,1
330,With regard to parsing and recognition of input strings  polynomialtime algorithms and the LOGCFL upper bound on the computational complexity are already known for the grammar formalisms covered by our results   nevertheless  we believe that our reduction to Datalog offers valuable insights,BackGround,SRelated,1
331,In this paper  we show that a similar reduction to Datalog is possible for more powerful grammar formalisms with contextfree derivations  such as multicomponent treeadjoining grammars    IO macro grammars    and parallel multiple contextfree grammars  ,BackGround,GRelated,2
332,By the main result of Gottlob et al    the related search problem of finding one derivation tree for the input term is in functional LOGCFL  ie  the class of functions that can be computed by a logspacebounded Turing machine with a LOGCFL oracle,BackGround,SRelated,1
333,Our method essentially relies on the encoding of different formalisms in terms of abstract categorial grammars  ,Fundamental,Basis,1
334,What we have called a contextfree  term grammar is nothing but an alternative notation for an abstract categorial grammar   whose abstract vocabulary is secondorder  with the restriction to linear terms removed,BackGround,SRelated,1
335,A stringgenerating grammar coupled with Montague semantics may be represented by a synchronous CFLG  a pair of CFLGs with matching rule sets  ,BackGround,SRelated,1
336,linear ACGs are known to be expressive enough to encode wellknown mildly contextsensitive grammar formalisms in a straightforward way  including TAGs and multiple contextfree grammars  ,BackGround,GRelated,2
337,For example  the linear CFLG in Figure 8 is an encoding of the TAG in Figure 3  where aS  oo and aA  o  o  o   o  ,BackGround,SRelated,1
338,We can eliminate erules from an almost linear CFLG by the same method that Kanazawa and Yoshinaka   used for linear grammars  noting that for any r and a  there are only finitely many almost linear terms M such that r h M  aIf a grammar has no erule  any derivation tree for the input term N that has a term P at its root node corresponds to a Datalog derivation tree whose number of leaves is equal to the number of occurrences of constants in P  which cannot exceed the number of occurrences of constants in N,Fundamental,Idea,1
339,For such P and D  it is known that D  q  D eD  P U D derives q  is in the complexity class LOGCFL  ,BackGround,SRelated,1
340,3 In the linear case  Salvati   has shown the recognitionparsing complexity to be PTIME  and exhibited an algorithm similar to Earley parsing for TAGs,BackGround,GRelated,1
341,The result of the generalized supplementary magicsets rewriting of Beeri and Ramakrishnan   applied to the Datalog program representing a CFG essentially coincides with the deduction system   or uninstantiated parsing system   for Earley parsing,BackGround,GRelated,1
342,By naive or seminaive bottomup evaluation    the answer to such a query can be computed in polynomial time in the size of the database for any Datalog program,BackGround,GRelated,1
343,We illustrate this approach with the program in Figure 4  following the presentation of Ullman  ,Fundamental,Idea,1
344,But there are also other factors involved  for example  the tendency to put given discourse elements before new ones  which has been shown to play a role independent of length  ,BackGround,GRelated,1
345,First  how close is dependency length in English to that of this optimal DLA Secondly  how similar is the optimal DLA to English in terms of the actual rules that arise Finding linear arrangements of graphs that minimize total edge length is a classic problem  NPcomplete for general graphs but with an On 16 algorithm for trees  ,BackGround,SRelated,1
346,Statistical parsers make use of features that capture dependency length egan adjacency feature in Collins    more explicit length features in McDonald et al   and Eisner and Smith   and thus learn to favor parses with shorter dependencies,BackGround,GRelated,1
347,We take sentences from the Wall Street Journal section of the Penn Treebank  extract the dependency trees using the headword rules of Collins    consider them to be unordered dependency trees  and linearize them to minimize dependency length,Fundamental,Basis,1
348,Exactly this pattern has been observed by Dryer   in natural languages,BackGround,SRelated,1
349,Frazier   suggests that this might serve the function of keeping heads and dependents close together,BackGround,SRelated,1
350,This has been offered as an explanation for numerous psycholinguistic phenomena  such as the greater processing difficulty of object relative clauses versus subject relative clauses  ,BackGround,GRelated,1
351,Hawkins   has shown that this principle is reflected in grammatical rules across many languages,BackGround,GRelated,1
352,One might suppose that such syntactic choices in English are guided at least partly by dependency length minimization  and indeed there is evidence for this for example  people tend to put the shorter of two PPs closer to the verb  ,BackGround,GRelated,1
353,The problem of finding the optimum weighted DLA for a set of input trees can be shown to be NPcomplete by reducing from the problem of finding a graphs minimum Feedback Arc Set  one of the 21 classic problems of Karp  ,BackGround,GRelated,1
354,This setting is reminiscent of the problem of optimizing feature weights for reranking of candidate machine translation outputs  and we employ an optimization technique similar to that used by Och   for machine translation,Fundamental,Idea,1
355,In particular  our approach would be applicable to corpora with framespecific role labels  egFrameNet  ,BackGround,MRelated,1
356,Our work suggests that feature generalization based on verbsimilarity may compliment approaches to generalization based on rolesimilarity  ,BackGround,MRelated,1
357,For this task we utilized the August 2005 release of the Charniak parser with the default speedaccuracy settings    which required roughly 360 hours of processor time on a 25 GHz PowerPC G5,Fundamental,Basis,1
358,To automatically identify all verb inflections  we utilized the English DELA electronic dictionary    which contained all but 21 of the PropBank verbs for which we provided the inflections ourselves  with oldEnglish verb inflections removed,Fundamental,Basis,1
359,Parse tree paths were used for semantic role labeling by Gildea and Jurafsky   as descriptive features of the syntactic relationship between predicates and their arguments in the parse tree of a sentence,BackGround,GRelated,1
360,In future work  it would be particularly interesting to compare empiricallyderived verb clusters to verb classes derived from theoretical considerations    and to the automated verb classification techniques that use these classes  ,BackGround,MRelated,1
361,Our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information  ,Fundamental,Idea,1
362,This observation further supports the distributional hypothesis of word similarity and corresponding technologies for identifying synonyms by similarity of lexicalsyntactic context  ,BackGround,SRelated,1
363,In our work  we utilized the GigaWord corpus of English newswire text    consisting of nearly 12 gigabytes of textual data,Fundamental,Basis,1
364,Annotations similar to these have been used to create automated semantic role labeling systems   for use in natural language processing applications that require only shallow semantic parsing,BackGround,GRelated,1
365,The overall performance of our semantic role labeling approach is not competitive with leading contemporary systems  which typically employ support vector machine learning algorithms with syntactic features   or syntactic tree kernels  ,Compare,Compare,2
366,A recent release of the PropBank   corpus of semantic role annotations of Treebank parses contained 112 917 labeled instances of 4 250 rolesets corresponding to 3 257 verbs  as illustrated by this example for the verb buy,BackGround,GRelated,1
367,An important area for future research will be to explore the correlation between our distance metric for syntactic similarity and various quantitative measures of semantic similarity  ,BackGround,MRelated,2
368,To prepare this corpus for analysis  we extracted the body text from each of the 41 million entries in the corpus and applied a maximumentropy algorithm to identify sentence boundaries  ,Fundamental,Basis,1
369,Featurebased Methods for SRL most features used in prior SRL research are generally extended from Gildea and Jurafsky    who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet  ,BackGround,GRelated,1
370,SVM   is selected as our classifier and the one vs,Fundamental,Basis,1
371,In the context of it  more and more kernels for restricted syntaxes or specific domains   are proposed and explored in the NLP domain,BackGround,GRelated,1
372,In this paper  we apply Alternating Structure Optimization ASO   to the semantic role labeling task on NomBank,Fundamental,Basis,1
373,ASO has been shown to be effective on the following natural language processing tasks text categorization  named entity recognition  partofspeech tagging  and word sense disambiguation  ,BackGround,GRelated,2
374,For a more complete description  see  ,BackGround,SRelated,1
375,In this work  we use a modification of Hubers robust loss function  similar to that used in   Lp  y 4py if py  1 1  py 2 if  1 py 1 2 0 if py  1 We fix the regularization parameter A to 10 4  similar to that used in  ,Fundamental,Idea,1
376,This relationship is modeled by  6 Tvi 3 The parameters w l  vi  6 may then be found by joint empirical risk minimization over all the m problems  ie  their values should minimize the combined empirical risk li  V   il J 4 An important observation in   is that the binary classification problems used to derive 6 are not necessarily those problems we are aiming to solve,BackGround,SRelated,2
377,Assuming there are k target problems and m auxiliary problems  it is shown in   that by performing one round of minimization  an approximate solution of 6 can be obtained from 4 by the following algorithm 1 For each of the m auxiliary problems  learn u l as described by 1,BackGround,SRelated,1
378,This is a simplified version of the definition in    made possible because the same A is used for all auxiliary problems,BackGround,SRelated,1
379,ASO has been demonstrated to be an effective semisupervised learning algorithm  ,BackGround,GRelated,2
380,A variety of auxiliary problems are tested in   in the semisupervised settings  ie  their auxiliary problems are generated from unlabeled data,BackGround,GRelated,1
381,More recently  for the word sense disambiguation WSD task    experimented with both supervised and semisupervised auxiliary problems  although the auxiliary problems she used are different from ours,BackGround,GRelated,1
382,In recent years  the availability of large humanlabeled corpora such as PropBank   and FrameNet   has made possible a statistical approach of identifying and classifying the arguments of verbs in natural language texts,BackGround,GRelated,1
383,This is known as multitask learning in the machine learning literature  ,BackGround,GRelated,1
384,A large number of SRL systems have been evaluated and compared on the standard data set in the CoNLL shared tasks    and many systems have performed reasonably well,BackGround,GRelated,1
385, In addition to the target outputs    discusses configurations where both used inputs and unused inputs due to excessive noise are utilized as additional outputs,BackGround,GRelated,1
386,First  we train the various classifiers on sections 2 to 21 using gold argument labels and automatic parse trees produced by Charniaks reranking parser    and test them on section 23 with automatic parse trees,Fundamental,Basis,1
387,Noun predicates also appear in FrameNet semantic role labeling    and many FrameNet SRL systems are evaluated in Senseval3  ,BackGround,GRelated,1
388,So far we are aware of only one English NomBankbased SRL system    which uses the maximum entropy classifier  although similar efforts are reported on the Chinese NomBank by   and on FrameNet by   using a small set of handselected nominalizations,BackGround,GRelated,1
389,Second  we achieve accuracy higher than that reported in   and advance the state of the art in SRL research,Compare,Compare,3
390,Eighteen baseline features and six additional features are proposed in   for NomBank argument identification,Fundamental,Basis,1
391,Unlike in    we do not prune arguments dominated by other arguments or those that overlap with the predicate in the training data,Compare,Compare,1
392,The JN column presents the result reported in   using both baseline and additional features,BackGround,SRelated,1
393,A diverse set of 28 features is used in   for argument classification,Fundamental,Basis,1
394,To find a smaller set of effective features  we start with all the features considered in    in    and various combinations of them  for a total of 52 features,Fundamental,Basis,1
395,The JN column presents the result reported in  ,BackGround,SRelated,1
396,This is the same configuration as reported in  ,Fundamental,Idea,1
397,Table 3 Fl scores of various classifiers on NomBank SRL Our maximum entropy classifier consistently outperforms    which also uses a maximum entropy classifier,Compare,Compare,3
398,Our results outperform those reported in  ,Compare,Compare,3
399,With the recent release of NomBank    it becomes possible to apply machine learning techniques to the task,BackGround,GRelated,1
400,Accordingly  we do not maximize the probability of the entire labeled parse tree as in  ,Fundamental,Idea,1
401,Some approaches have used WordNet for the generalization step    others EMbased clustering  ,BackGround,GRelated,1
402,The argument positions for which we compute selectional preferences will be semantic roles in the FrameNet   paradigm  and the predicates we consider will be semantic classes of words rather than individual words which means that different preferences will be learned for different senses of a predicate word,Fundamental,Basis,1
403,We use FrameNet    a semantic lexicon for English that groups words in semantic classes called frames and lists semantic roles for each frame,Fundamental,Basis,1
404,Brockmann and Lapata   perform a comparison of WordNetbased models,BackGround,GRelated,1
405,The sim function can equally well be incstantiated with a WordNetbased metric for an overview see Budanitsky and Hirst    but we restrict our experiments to corpusbased metrics a in the interest of greatest possible sim cosinew  w             p          sim Dicew w     ip  Miiip  a V      7 E r p  fw r p  2YE r p  f w r p   2   Dic  V      7    Rw       Rw   sim  Lj nw w    p  n  tt   i v  sim i accarHw w    p   i    a slm nindiew w        r  p sim Hindiew  w r p where sim Hindlew  w  r p        absmax1 w r p  1 w r p   if Iw r p   0 and I w rp   0 Table 1 Similarity measures used resourceindependence and b in order to be able to shape the similarity metric by the choice of generalization corpus,BackGround,SRelated,1
406,In SRL  the two most pressing issues today are 1 the development of strong semantic features to complement the current mostly syntacticallybased systems  and 2 the problem of the domain dependence  ,BackGround,GRelated,1
407,The preference that r p has for a given synset co  the selectional association between the two  is then defined as the contribution of c 0 to r ps selectional preference strength Arp C 0  P C 0r plog  gf S rp Further WordNetbased approaches to selectional preference induction include Clark and Weir    and Abe and Li  ,BackGround,GRelated,1
408,To determine headwords of the semantic roles  the corpus was parsed using the Collins   parser,Fundamental,Basis,1
409,5x2cv  ,,,
410,They have been used for example for syntactic disambiguation    word sense disambiguation WSD   and semantic role labeling SRL  ,BackGround,GRelated,1
411,While EMbased models have been shown to work better in SRL tasks    this has been attributed to the difference in coverage,BackGround,GRelated,1
412,We will be using the similarity metrics shown in Table 1 Cosine  the Dice and Jaccard coefficients  and Hindles   and Lins   mutual informationbased metrics,Fundamental,Basis,1
413,Selectional restrictions and selectional preferences that predicates impose on their arguments have long been used in semantic theories  see eg  ,BackGround,GRelated,1
414,It was parsed using Minipar    which is considerably faster than the Collins parser but failed to parse about a third of all sentences,Fundamental,Basis,2
415,In this paper we propose a new  simple model for selectional preference induction that uses corpusbased semantic similarity metrics  such as Cosine or Lins   mutual informationbased metric  for the generalization step,Fundamental,Basis,1
416,The corpusbased induction of selectional preferences was first proposed by Resnik  ,BackGround,GRelated,1
417,The induction of selectional preferences from corpus data was pioneered by Resnik  ,BackGround,GRelated,2
418,Rooth et al   generalize over seen headwords using EMbased clustering rather than WordNet,BackGround,GRelated,1
419,Experimental design Like Rooth et al   we evaluate selectional preference induction approaches in a pseudodisambiguation task,Fundamental,Idea,1
420,The intuition that hard to learn examples are suspect corpus errors is not new  and appears also in Abney et al    who consider the heaviest samples in the final distribution of the AdaBoost algorithm to be the hardest to classify and thus likely corpus errors,BackGround,SRelated,1
421,The HEB  Err version of the corpus is obtained by projecting the chunk boundaries on the sequence of PoS and morphology tags obtained by the automatic PoS tagger of Adler  Elhadad  ,Fundamental,Basis,1
422,We tested this hypothesis by training the ErrorDriven Pruning EDP method of   with an extended set of features,Fundamental,Basis,1
423,In    we established that the task is not trivially transferable to Hebrew  but reported that SVM based chunking   performs well,BackGround,SRelated,2
424,In   we argued that it is not applicable to Hebrew  mainly because of the prevalence of the Hebrews construct state smixut,BackGround,GRelated,1
425,For the Hebrew experiments  we use the corpora of  ,Fundamental,Basis,1
426,These are the same settings as in  ,Fundamental,Idea,1
427,Refining the SimpleNP Definition The hard cases analysis identified examples that challenge the SimpleNP definition proposed in Goldberg et al  ,BackGround,SRelated,1
428,Kudo and Matsumoto   used SVM as a classification engine and achieved an FScore of 9379 on the shared task NPs,BackGround,GRelated,1
429,Further details can be found in Kudo and Matsumoto  ,BackGround,SRelated,1
430,Following Ramshaw and Marcus    the current dominant approach is formulating chunking as a classification task  in which each word is classified as the Beginning  Inside or Ooutside of a chunk,Fundamental,Idea,1
431,NP chunks in the shared task data are BaseNPs  which are nonrecursive NPs  a definition first proposed by Ramshaw and Marcus  ,BackGround,GRelated,1
432,For the English experiments  we use the nowstandard training and test sets that were introduced in   2,Fundamental,Basis,1
433,This method is similar to the corpus error detection method presented by Nakagawa and Matsumoto  ,Fundamental,Idea,1
434,It is a well studied problem in English  and was the focus of CoNLL2000s Shared Task  ,BackGround,GRelated,1
435,We applied this definition to the Hebrew Tree Bank    and constructed a moderate size corpus about 5 000 sentences for Hebrew SimpleNP chunking,Fundamental,Basis,1
436,SVM   is a supervised binary classifier,BackGround,SRelated,1
437,However  each of these assumes that the relations themselves are known in advance implicitly or explicitly so that the method can be provided with seed patterns    patternbased rules    relation keywords    or word pairs exemplifying relation instances  ,BackGround,GRelated,1
438,Most related work deals with discovery of hypernymy    synonymy   and meronymy  ,BackGround,GRelated,1
439,In addition to these basic types  several studies deal with the discovery and labeling of more specific relation subtypes  including interverb relations   and nouncompound relationships  ,BackGround,GRelated,1
440,It should be noted that some of these papers utilize language and domaindependent preprocessing including syntactic parsing   and named entity tagging    while others take advantage of handcrafted databases such as WordNet   and Wikipedia  ,BackGround,GRelated,1
441,In several studies   it has been shown that relatively unsupervised and languageindependent methods could be used to generate many thousands of sets of words whose semantics is similar in some sense,BackGround,GRelated,1
442,We do this as follows  essentially implementing a simplified version of the method of Davidov and Rappoport  ,Fundamental,Basis,1
443,Note that our method differs from that of Davidov and Rappoport   in that here we provide an initial seed pair  representing our target concept  while there the goal is grouping of as many words as possible into concept classes,Compare,Compare,1
444,It was shown in   that pairs of words that often appear together in such symmetric patterns tend to belong to the same class that is  they share some notable aspect of their semantics,BackGround,SRelated,1
445,Studying relationships between tagged named entities    proposed unsupervised clustering methods that assign given or semiautomatically extracted sets of pairs into several clusters  where each cluster corresponds to one of a known relationship type,BackGround,GRelated,1
446,A lot of this research is based on the initial insight   that certain lexical patterns X is a country can be exploited to automatically generate hyponyms of a specified word,BackGround,GRelated,1
447,In some recent work    it has been shown that related pairs can be generated without prespecifying the nature of the relation sought,BackGround,GRelated,1
448,Finally    provided a pattern distance measure which allows a fully unsupervised measurement of relational similarity between two pairs of words however  relationship types were not discovered explicitly,BackGround,GRelated,1
449,The bracketing guidelines   also mention the considerable difficulty of identifying the correct scope for nominal modifiers,BackGround,GRelated,1
450,We use Bikels implementation   of Collins parser   in order to carry out these experiments  using the nondeficient Collins settings,Fundamental,Basis,1
451,We draw our counts from a corpus of ngram counts calculated over 1 trillion words from the web  ,Fundamental,Basis,1
452,We use the Briscoe and Carroll   version of DepBank  a 560 sentence subset used to evaluate the rasp parser,Fundamental,Basis,1
453,We map the brackets to dependencies by finding the head of the np  using the Collins   head finding rules  and then creating a dependency between each other childs head and this head,Fundamental,Basis,1
454,We discretised the nonbinary features using an implementation of Fayyad and Iranis   algorithm  and classify using MegaM 2,Fundamental,Basis,1
455,For instance  CCGbank   was created by semiautomatically converting the Treebank phrase structure to Combinatory Categorial Grammar ccg   derivations,BackGround,GRelated,1
456,An additional grammar rule is needed just to get a parse  but it is still not correct Hockenmaier  2003  p,BackGround,SRelated,3
457,We check the correctness of the corpus by measuring interannotator agreement  by reannotating the first section  and by comparing against the sub NP structure in DepBank  ,Compare,Compare,1
458,We used the PARC700 Dependency Bank   which consists of 700 Section 23 sentences annotated with labelled dependencies,Fundamental,Basis,1
459,Our annotation guidelines 1 are based on those developed for annotating full sub np structure in the biomedical domain  ,Fundamental,Basis,1
460,Lapata and Keller   derive estimates from web counts  and only compare at a lexical level  achieving 787 accuracy,BackGround,GRelated,1
461,Finally  we test the utility of the extended Treebank for training statistical models on two tasks NP bracketing   and full parsing  ,Fundamental,Basis,1
462,Lauer   has demonstrated superior performance of the dependency model using a test set of 244 216 unique noun compounds drawn from Groliers encyclopedia,BackGround,GRelated,1
463,We implement a similar system to Table 4 Comparison of NP bracketing corpora Table 5 Lexical overlap Lauer    described in Section 3  and report on results from our own data and Lauers original set,Fundamental,Idea,1
464,The np bracketing task has often been posed in terms of choosing between the left or right branching structure of three word noun compounds a  world oil prices  Rightbranching b  crude oil prices  Leftbranching Most approaches to the problem use unsupervised methods  based on competing association strength between two of the words in the compound Marcus  1980  p,BackGround,GRelated,1
465,The Penn Treebank   is perhaps the most influential resource in Natural Language Processing NLP,BackGround,GRelated,2
466,According to Marcus et al    asking annotators to markup base np structure significantly reduced annotation speed  and for this reason base nps were left flat,BackGround,GRelated,1
467,For the original bracketing of the Treebank  annotators performed at 375475 words per hour after a Table 1 Agreement between annotators few weeks  and increased to about 1000 words per hour after gaining more experience  ,BackGround,SRelated,1
468,Nakov and Hearst   also use web counts  but incorporate additional counts from several variations on simple bigram queries  including queries for the pairs of words concatenated or joined by a hyphen,BackGround,GRelated,1
469,With our new data set  we began running experiments similar to those carried out in the literature  ,Fundamental,Idea,1
470,Many approaches to identifying base noun phrases have been explored as part of chunking    but determining sub np structure is rarely addressed,BackGround,GRelated,1
471,The bracketing tool often suggests a bracketing using rules based mostly on named entity tags  which are drawn from the bbn corpus  ,Fundamental,Basis,1
472,The most common form of parser evaluation is to apply the parseval metrics to phrasestructure parsers based on the penn Treebank  and the highest reported scores are now over 90  ,BackGround,GRelated,2
473,In this paper we evaluate a ccg parser   on the Briscoe and Carroll version of DepBank  ,Fundamental,Basis,1
474,Briscoe and Carroll   reannotated this resource using their grs scheme  and used it to evaluate the rasp parser,BackGround,GRelated,1
475,Parsers have been developed for a variety of grammar formalisms  for example hpsg    lfg    tag    ccg    and variants of phrasestructure grammar    including the phrasestructure grammar implicit in the Penn Treebank  ,BackGround,GRelated,1
476,And third  we provide the first evaluation of a widecoverage ccg parser outside of CCGbank  obtaining impressive results on DepBank and outperforming the rasp parser   by over 5 overall and on the majority of dependency types,Compare,Compare,3
477,For the gold standard we chose the version of DepBank reannotated by Briscoe and Carroll    consisting of 700 sentences from Section 23 of the Penn Treebank,Fundamental,Basis,1
478,The results in Table 4 were obtained by parsing the sentences from CCGbank corresponding to those in the 560sentence test set used by Briscoe et al  ,Fundamental,Basis,1
479,the macroaveraged scores are the mean of the individual scores for each relation  ,BackGround,SRelated,1
480,Can the ccg parser be compared with parsers other than rasp Briscoe and Carroll   give a rough comparison of rasp with the Parc lfg parser on the different versions of DepBank  obtaining similar results overall  but they acknowledge that the results are not strictly comparable because of the different annotation schemes used,BackGround,GRelated,1
481,Briscoe et al   split the 700 sentences in DepBank into a test and development set  but the latter only consists of 140 sentences which was not enough to reliably create the transformation,BackGround,GRelated,3
482,All the results were obtained using the RASP evaluation scripts  with the results for the rasp parser taken from Briscoe et al  ,Fundamental,Basis,1
483,Preiss   compares the parsers of Collins   and Charniak    the gr finder of Buchholz et al    and the rasp parser  using the Carroll et al   goldstandard,BackGround,GRelated,1
484,It has been argued that the parseval metrics are too forgiving and that phrase structure is not the ideal representation for a gold standard  ,BackGround,GRelated,3
485,Carroll et al   describe such a suite  consisting of sentences taken from the Susanne corpus  annotated with Grammatical Relations grs which specify the syntactic relation between a head and dependent,BackGround,GRelated,1
486,We chose not to use the corpus based on the Susanne corpus   because the grs are less like the ccg dependencies the corpus is not based on the Penn Treebank  making comparison more difficult because of tokenisation differences  for example and the latest results for rasp are on DepBank,BackGround,SRelated,3
487,parser evaluation has improved on the original parseval measures    but the challenge remains to develop a representation and evaluation suite which can be easily applied to a wide variety of parsers and formalisms,BackGround,SRelated,1
488,Clark and Curran   describes the ccg parser used for the evaluation,BackGround,SRelated,1
489,Previous evaluations of ccg parsers have used the predicateargument dependencies from CCGbank as a test set    with impressive results of over 84 Fscore on labelled dependencies,BackGround,SRelated,2
490,Kaplan et al   compare the Collins   parser with the Parc lfg parser by mapping lfg Fstructures and Penn Treebank parses into DepBank dependencies  claiming that the lfg parser is considerably more accurate with only a slight reduction in speed,BackGround,GRelated,1
491,The ccg parser results are based on automatically assigned pos tags  using the Curran and Clark   tagger,Fundamental,Basis,1
492,An example of this is from CCGbank    where all modifiers in nounnoun compound constructions modify the final noun because the penn Treebank  from which CCGbank is derived  does not contain the necessary information to obtain the correct bracketing,BackGround,GRelated,1
493,The grammar used by the parser is extracted from CCGbank  a ccg version of the Penn Treebank  ,BackGround,SRelated,1
494,Such conversions have been performed for other parsers  including parsers producing phrase structure output  ,BackGround,GRelated,1
495,Kaplan et al   clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies  but they also note that This conversion was relatively straightforward for LFG structures ,BackGround,GRelated,1
496,In the case of Kaplan et al    the testing procedure would include running their conversion process on Section 23 of the Penn Treebank and evaluating the output against DepBank,BackGround,GRelated,1
497,A similar resource  the Parc Dependency Bank DepBank    has been created using sentences from the Penn Treebank,BackGround,GRelated,1
498,The bc scheme is similar to the original DepBank scheme    but overall contains less grammatical detail Briscoe and Carroll   describes the differences,BackGround,Basis,1
499,Different parsers produce different output  for example phrase structure trees    dependency trees    grammatical relations    and formalismspecific dependencies  ,BackGround,GRelated,1
500,The grammar consists of 425 lexical categories  expressing subcategorisation information  plus a small number of combinatory rules which combine the categories  ,BackGround,SRelated,1
501,A more interesting statement would be that it makes learning easier  along the lines of the result of    note  however  that their results are for the semisupervised domain adaptation problem and so do not apply directly,BackGround,SRelated,1
502,A partofspeech tagging problem on PubMed abstracts introduced by Blitzer et al  ,Fundamental,Basis,1
503,The first model  which we shall refer to as the Prior model  was first introduced by Chelba and Acero  ,Fundamental,Basis,1
504,This is a recapitalization task introduced by Chelba and Acero   and also used by Daume III and Marcu  ,BackGround,GRelated,1
505,For the CNNRecap task  we use identical feature to those used by both Chelba and Acero   and Daume III and Marcu   the current  previous and next word  and 13 letter prefixes and suffixes,Fundamental,Basis,1
506,Many of these are presented and evaluated by Daume III and Marcu  ,BackGround,GRelated,1
507,Daume III and Marcu   provide empirical evidence on four datasets that the Prior model outperforms the baseline approaches,BackGround,GRelated,1
508,More recently  Daume III and Marcu   presented an algorithm for domain adaptation for maximum entropy classifiers,BackGround,GRelated,1
509,We additionally ran the MegaM model   on these data though not in the multiconditional case for this  we considered the single source as the union of all sources,Fundamental,Basis,1
510,In all cases  we use the S earn algorithm for solving the sequence labeling problem   with an underlying averaged perceptron classifier implementation due to  ,Fundamental,Basis,1
511,Second  it is arguable that a measure like F 1 is inappropriate for chunking tasks  ,BackGround,SRelated,1
512,Following    we call the first the source domain  and the second the target domain,Fundamental,Idea,1
513,Recently there have been some studies addressing domain adaptation from different perspectives  ,BackGround,GRelated,1
514,The POS data set and the CTS data set have previously been used for testing other adaptation methods    though the setup there is different from ours,BackGround,GRelated,1
515,Blitzer et al   propose a domain adaptation method that uses the unlabeled target instances to infer a good feature representation  which can be regarded as weighting the features,BackGround,GRelated,1
516,Chelba and Acero   use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data,BackGround,GRelated,1
517,The setup is very similar to Daume III and Marcu  ,Fundamental,Idea,1
518,els the different distributions in the source and the target domains is by Daume III and Marcu  ,Fundamental,Basis,1
519,Florian et al   first train a NE tagger on the source domain  and then use the taggers predictions as features for training and testing on the target domain,BackGround,GRelated,1
520,This way of setting 7 corresponds to the entropy minimization semisupervised learning method  ,BackGround,SRelated,1
521,For generative syntactic parsing  Roark and Bacchiani   have used the source domain data to construct a Dirichlet prior for MAP estimation of the PCFG for the target domain,BackGround,GRelated,1
522,ber of hidden components is not fixed  but emerges We begin by presenting three finite tree models  each naturally from the training data  ,Fundamental,Basis,1
523,The closely related infinite hidden Markov model is an HMM in which the transitions are modeled using an HDP  enabling unsupervised learning of sequence models when the number of hidden states is unknown  ,BackGround,GRelated,1
524,The infinite hidden Markov model iHMM or HDPHMM   is a model of sequence data with transitions modeled by an HDP,BackGround,GRelated,1
525,This is useful  because coarsegrained syntactic categories  such as those used in the Penn Treebank PTB  make insufficient distinctions to be the basis of accurate syntactic parsing  ,BackGround,GRelated,3
526,Hence  stateoftheart parsers either supplement the partofspeech POS tags with the lexical forms themselves    manually split the tagset into a finergrained one    or learn finer grained tag distinctions using a heuristic learning procedure  ,BackGround,GRelated,2
527,But the introduction of nonparametric priors such as the Dirichletprocess   enabled development of infinite mixture models  in which the numTeh et al   proposed the hierarchical Dirichlet process HDP as a way of applying the Dirichlet process DP to more complex model forms  so as to allow multiple  groupspecific  infinite mixture models to share their mixture components,BackGround,GRelated,1
528,8 Additionally  we compute the mutual information of the learned clusters with the gold tags  and we compute the cluster Fscore  ,Fundamental,Basis,1
529,First  we use the standard approach of greedily assigning each of the learned classes to the POS tag with which it has the greatest overlap  and then computing tagging accuracy  ,Fundamental,Basis,1
530,For comparison  Haghighi and Klein   report an unsupervised baseline of 413  and a best result of 805 from using handlabeled prototypes and distributional similarity,Compare,Compare,1
531,Earlier  Johnson et al   presented adaptor grammars  which is a very similar model to the HDPPCFG,BackGround,GRelated,1
532,We use the generative dependency parser distributed with the Stanford factored parser   for the comparison  since it performs simultaneous tagging and parsing during testing,Fundamental,Basis,1
533,The HDPPCFG    developed at the same time as this work  aims to learn state splits for a binarybranching PCFG,BackGround,GRelated,1
534,In contrast  Liang et al   define a global DP over sequences  with the base measure defined over the global state probabilities   0 locally  each state has an HDP  with this global DP as the base measure,Compare,Compare,1
535,For both experiments  we used dependency trees extracted from the Penn Treebank   using the head rules and dependency extractor from Yamada and Matsumoto  ,Fundamental,Basis,1
536,To generate  n we first generate an infinite sequence of variables  n  n k each of which is distributed according to the Beta distribution Then  n  n k   1 is defined as 1 Following Pitman   we refer to this process as n  GEM a 0,Fundamental,Idea,1
537,Teh  2006  pc  to sample each m jk sampleM j  k 1  if n jk  0 2  then m jk  0 3  else m jk  1 4  for i  2 to n jk 5  doifrand T 6  then m jk  m jk  1 7  return m jk Sampling  ,,,
538,In many cases  improving semisupervised models was done by seeding these models with domain information taken from dictionaries or ontology  ,BackGround,GRelated,1
539,This follows a conceptually similar approach by   that uses a large namedentity dictionary  where the similarity between the candidate namedentity and its matching prototype in the dictionary is encoded as a feature in a supervised classifier,BackGround,GRelated,1
540,Therefore  an increasing attention has been recently given to semisupervised learning  where large amounts of unlabeled data are used to improve the models learned from a small training set  ,BackGround,GRelated,1
541,This was used  for example  by   in information extraction  and by   in POS tagging,BackGround,GRelated,1
542,This decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs  in which case the linear sum corresponds to log likelihood assigned to the inputoutput pair by the model for details see   for the classification case and   for the structured case,BackGround,SRelated,1
543,For example    proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labels,BackGround,GRelated,1
544,The second problem we consider is extracting fields from advertisements  ,Fundamental,Basis,1
545,  and   also report results for semisupervised learning for these domains,BackGround,GRelated,1
546,  extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity,BackGround,GRelated,1
547,We implement some global constraints and include unary constraints which were largely imported from the list of seed words used in  ,Fundamental,Basis,1
548,  also worked on one of our data sets,BackGround,SRelated,1
549,1 The first task is to identify fields from citations   ,Fundamental,Basis,1
550,Another way to look the algorithm is from the selftraining perspective  ,BackGround,SRelated,1
551,However  in the general case  semisupervised approaches give mixed results  and sometimes even degrade the model performance  ,BackGround,GRelated,3
552,  has suggested to balance the contribution of labeled and unlabeled data to the parameters,BackGround,SRelated,1
553, ,,,
554,This confirms results reported for the supervised learning case in  ,BackGround,SRelated,1
555,On the other hand  in the supervised setting  it has been shown that incorporating domain and problem specific structured information can result in substantial improvements  ,BackGround,GRelated,1
556,However   showed that reasoning with more expressive  nonsequential constraints can improve the performance for the supervised protocol,BackGround,GRelated,1
557,We note that in the presence of constraints  the inference procedure for finding the output y that maximizes the cost function is usually done with search techniques rather than Viterbi decoding  see   for a discussion  we chose beamsearch decoding,BackGround,SRelated,1
558,While   showed the significance of using hard constraints  our experiments show that using soft constraints is a superior option,BackGround,GRelated,3
559,Conceptually  although not technically  the most related work to ours is   that  in a somewhat adhoc manner uses soft constraints to guide an unsupervised model that was crafted for mention tracking,BackGround,SRelated,1
560,Crucially  the kind of lexical descriptions that we employ are those that are commonly devised within lexicondriven approaches to linguistic syntax  egLexicalized TreeAdjoining Grammar   and Combinary Categorial Grammar  ,Fundamental,Basis,1
561,There are currently two supertagging approaches available LTAGbased   and CCGbased  ,BackGround,GRelated,1
562,One important way of portraying such lexical descriptions is via the supertags devised in the LTAG and CCG frameworks  ,BackGround,SRelated,2
563,The term supertagging   refers to tagging the words of a sentence  each with a supertag,BackGround,SRelated,1
564,The LTAGbased supertagger of   is a standard HMM tagger and consists of a secondorder Markov language model over supertags and a lexical model conditioning the probability of every word on its own supertag just like standard HMMbased POS taggers,BackGround,SRelated,1
565,For the LTAG supertags experiments  we used the LTAG English supertagger 5   to tag the English part of the parallel data and the supertag language model data,Fundamental,Basis,1
566,Akin to POS tagging  the process of supertagging an input utterance proceeds with statistics that are based on the probability of a wordsupertag pair given their Markovian or local context  ,BackGround,GRelated,1
567,Besides the difference in probabilities and statistical estimates  these two supertaggers differ in the way the supertags are extracted from the Penn Treebank  cf  ,BackGround,GRelated,1
568,Only quite recently have   and   shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT system,BackGround,GRelated,2
569,Among the first to demonstrate improvement when adding recursive structure was    who allows for hierarchical phrase probabilities that handle a range of reordering phenomena in the correct fashion,BackGround,GRelated,1
570,The CCG supertagger   is based on loglinear probabilities that condition a supertag on features representing its context,BackGround,SRelated,1
571,For the CCG supertag experiments  we used the CCG supertagger of   and the Edinburgh CCG tools 6 to tag the English part of the parallel corpus as well as the CCG supertag language model data,Fundamental,Basis,1
572,Both the LTAG   and the CCG supertag sets   were acquired from the WSJ section of the PennII Treebank using handbuilt extraction rules,Fundamental,Basis,1
573,Decoder The decoder used in this work is Moses  a loglinear decoder similar to Pharaoh    modified to accommodate supertag phrase probabilities and supertag language models,Fundamental,Idea,1
574,Within the field of Machine Translation  by far the most dominant paradigm is Phrasebased Statistical Machine Translation PBSMT  ,BackGround,GRelated,1
575,For example    demonstrated that adding syntax actually harmed the quality of their SMT system,BackGround,GRelated,1
576,The bidirectional word alignment is used to obtain phrase translation pairs using heuristics presented in   and    and the Moses decoder was used for phrase extraction and decoding,Fundamental,Basis,1
577,The bidirectional word alignment is used to obtain lexical phrase translation pairs using heuristics presented in   and  ,,,
578,Coming right up to date    demonstrate that syntactified target language phrases can improve translation quality for ChineseEnglish,BackGround,GRelated,2
579,While the research of   has much in common with the approach proposed here such as the syntactified target phrases  there remain a number of significant differences,BackGround,SRelated,1
580,The NIST MT03 test set is used for development  particularly for optimizing the interpolation weights using Minimum Error Rate training  ,Fundamental,Basis,1
581,Firstly  rather than induce millions of xRS rules from parallel data  we extract phrase pairs in the standard way   and associate with each phrasepair a set of target language syntactic structures based on supertag sequences,Fundamental,Basis,1
582,Table 1 presents the BLEU scores   of both systems on the NIST 2005 MT Evaluation test set,Fundamental,Basis,1
583,For less commonly used languages  one might use open source research systems  ,BackGround,MRelated,1
584,Also relevant is previous work that applied machine learning approaches to MT evaluation  both with human references   and without  ,BackGround,GRelated,1
585,METEOR uses the Porter stemmer and synonymmatching via WordNet to calculate recall and precision more accurately  ,BackGround,GRelated,1
586,As its loss function  support vector regression uses an einsensitive error function  which allows for errors within a margin of a small positive value  e  to be considered as having zero error cfBishop    pp339344,BackGround,SRelated,1
587,This can be seen as a form of confidence estimation on MT outputs  ,BackGround,GRelated,1
588,To remove the bias in the distributions of scores between different judges  we follow the normalization procedure described by Blatz et al  ,Fundamental,Idea,1
589,We conducted experiments to determine the feasibility of the proposed approach and to address the following questions 1 How informative are pseudo references inandof themselves Does varying the number andor the quality of the references have an impact on the metrics 2 What are the contributions of the adequacy features versus the fluency features to the learningbased metric 3 How do the quality and distribution of the training examples  together with the quality of the pseudo references  impact the metric training 4 Do these factors impact the metrics ability in assessing sentences produced within a single MT system How does that systems quality affect metric performance The implementation of support vector regression used for these experiments is SVMLight  ,Fundamental,Basis,1
590,To compare the relative quality of different metrics  we apply bootstrapping resampling on the data  and then use paired ttest to determine the statistical significance of the correlation differences  ,Fundamental,Basis,1
591,ROUGE utilizes skip ngrams  which allow for matches of sequences of words that are not necessarily adjacent  ,BackGround,GRelated,1
592,BLEU is smoothed    and it considers only matching up to bigrams because this has higher correlations with human judgments than when higherordered ngrams are included,BackGround,SRelated,1
593,The HWC metrics compare dependency and constituency trees for both reference and machine translations  ,BackGround,GRelated,1
594,In addition to adapting the idea of Head Word Chains    we also compared the input sentences argument structures against the treebank for certain syntactic categories,Fundamental,Idea,1
595,Referencebased metrics such as BLEU   have rephrased this subjective task as a somewhat more objective question how closely does the translation resemble sentences that are known to be good translations for the same source This approach requires the participation of human translators  who provide the gold standard reference sentences,BackGround,GRelated,1
596,The relationship between word alignments and their impact on MT is also investigated in  ,BackGround,GRelated,1
597,Most current statistical models   treat the aligned sentences in the corpus as sequences of tokens that are meant to be words the goal of the alignment process is to find links between source and target words,BackGround,GRelated,1
598,To quickly and approximately evaluate this phenomenon  we trained the statistical IBM wordalignment model 4    1 using the GIZA software   for the first two language pairs  and the Europarl corpus   for the last one,Fundamental,Basis,1
599,They can be seen as extensions of the simpler IBM models 1 and 2  ,BackGround,GRelated,1
600,We use a standard loglinear phrasebased statistical machine translation system as a baseline GIZA implementation of IBM word alignment model 4    8 the refinement and phraseextraction heuristics described in    minimumerrorrate training Table 2 ChineseEnglish corpus statistics   using Phramer    a 3gram language model with KneserNey smoothing trained with SRILM   on the English side of the training data and Pharaoh   with default settings to decode,Fundamental,Basis,1
601,We also want to bootstrap on different word aligners in particular  one possibility is to use the flexible HMM wordtophrase model of Deng and Byrne   in place of IBM model 4,BackGround,MRelated,1
602,We evaluate the reliability of these candidates  using simple metrics based on cooccurence frequencies  similar to those used in associative approaches to word alignment  ,Fundamental,Idea,1
603,Second  an increase in AER does not necessarily imply an improvement in translation quality   and viceversa  ,BackGround,SRelated,1
604,This very simple measure is frequently used in associative approaches  ,BackGround,SRelated,1
605, there is want to need not IiS in front of  as soon as  look at Figure 2 Examples of entries from the manually developed dictionary The intrinsic quality of word alignment can be assessed using the Alignment Error Rate AER metric    that compares a systems alignment output to a set of goldstandard alignment,BackGround,SRelated,1
606,The quality of the translation output is evaluated using BLEU  ,Fundamental,Basis,1
607,The experiments were carried out using the ChineseEnglish datasets provided within the IWSLT 2006 evaluation campaign    extracted from the Basic Travel Expression Corpus BTEC  ,Fundamental,Basis,1
608,For Chinese  the data provided were tokenized according to the output format of ASR systems  and humancorrected  ,Fundamental,Basis,1
609,Note that the need to consider segmentation and alignment at the same time is also mentioned in    and related issues are reported in  ,BackGround,GRelated,1
610,More importantly  however  this segmentation is often performed in a monolingual context  which makes the word alignment task more difficult since different languages may realize the same concept using varying numbers of words see eg  ,BackGround,GRelated,1
611,The loglinear model is also based on standard features conditional probabilities and lexical smoothing ofphrases in both directions  and phrase penalty  ,Fundamental,Basis,1
612,To test the influence of the initial word segmentation on the process of word packing  we considered an additional segmentation configuration  based on an automatic segmenter combining rulebased and statistical techniques  ,Fundamental,Basis,1
613,These resources follow more or less the same format as the output of the word segmenter mentioned in Section 512    so the experiments are carried out using this segmentation,Fundamental,Idea,1
614,It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision  ,BackGround,SRelated,1
615,Recently  confusion network decoding for MT system combination has been proposed  ,BackGround,GRelated,1
616,Powells method   is used to tune the system and feature weights simultaneously so as to optimize various automatic evaluation metrics on a development set,BackGround,GRelated,1
617,In this work  modified Powells method as proposed by   is used,Fundamental,Basis,1
618,Six MT systems were combined three A C E were phrasebased similar to    two B D were hierarchical similar to   and one F was syntaxbased similar to  ,Fundamental,Idea,1
619,Combination of speech recognition outputs is an example of this approach  ,BackGround,GRelated,1
620,Also  a more heuristic alignment method has been proposed in a different system combination approach  ,BackGround,GRelated,1
621,In speech recognition  confusion network decoding   has become widely used in system combination,BackGround,GRelated,1
622,In    different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA  ,BackGround,GRelated,1
623,Tuning is fully automatic  as opposed to   where global system weights were set manually,BackGround,GRelated,1
624,Similar combination of multiple confusion networks was presented in  ,BackGround,SRelated,1
625,The same Powells method has been used to estimate feature weights of a standard featurebased phrasal MT decoder in  ,BackGround,SRelated,1
626,The optimization of the system and feature weights may be carried out using best lists as in  ,BackGround,SRelated,1
627,Currently  the most widely used automatic MT evaluation metric is the NIST BLEU4  ,BackGround,SRelated,1
628,This work was extended in   by introducing system weights for word confidences,BackGround,GRelated,1
629,In    simple score was assigned to the word coming from the thbest hypothesis,BackGround,GRelated,1
630,In    the total confidence of the nth best confusion network hypothesis   including NULL words  given the th source sentence  was given by where is the number of nodes in the confusion network for the source sentence   is the number of translation systems  is the th system weight   c wn is the accumulated confidence for word produced by system between nodes and   and is a weight for the number of NULL links along the hypothesis ,BackGround,SRelated,1
631,The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in   on the Arabic to English and Chinese to English NIST MT05 tasks,Compare,Compare,1
632,Compared to the baseline from    the new method improves the BLEU scores significantly,Compare,Compare,3
633,In ensemble learning  a collection of simple classifiers is used to yield better performance than any single classifier for example boosting  ,BackGround,GRelated,1
634,A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate TER   was used to align hypotheses in  ,BackGround,GRelated,1
635,Minimum Bayes risk MBR was used to choose the skeleton in  ,BackGround,GRelated,1
636,This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities  ,BackGround,SRelated,1
637,It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output  ,BackGround,SRelated,1
638,Translation edit rate TER   has been proposed as more intuitive evaluation metric since it is based on the rate of edits required to transform the hypothesis into the reference,BackGround,SRelated,1
639,However  this would require time consuming evaluations such as human mediated TER postediting  ,BackGround,MRelated,1
640,The TnT tagger   and the TreeTagger   are used for tagging and lemmatization,Fundamental,Basis,1
641,Motivated by the theoretical work by Chafe   and Jacobs    we view the VF as the place for elements which modify the situation described in the sentence  ie,Fundamental,Idea,1
642,Finally  the articles are parsed with the CDG dependency parser  ,Fundamental,Basis,1
643,The preferences summarized below have motivated our choice of features  constituents in the nominative case precede those in other cases  and dative constituents often precede those in the accusative case    the verb arguments order depends on the verbs  subcategorization properties    constituents with a definite article precede those with an indefinite one    pronominalized constituents precede nonpronominalized ones     animate referents precede inanimate ones    short constituents precede longer ones    the preferred topic position is right after the verb    the initial position is usually occupied by scenesetting elements and topics   ,BackGround,GRelated,1
644,The sentenceinitial position  which in German is the VF  has been shown to be cognitively more prominent than other positions  ,BackGround,SRelated,1
645,Inspired by the findings of the Prague School   and Systemic Functional Linguistics    they focus on the role that information structure plays in constituent ordering,BackGround,GRelated,1
646,Harbusch et al   present a generation workbench  which has the goal of producing not the most appropriate order  but all grammatical ones,BackGround,GRelated,1
647,We suppose that this dificulty comes from the double function of the initial position which can either introduce the addressation topic  or be the scene or framesetting position  ,BackGround,MRelated,1
648,We hypothesize that the reasons which bring a constituent to the VF are different from those which place it  say  to the beginning of the MF  for the order in the MF has been shown to be relatively rigid  ,BackGround,SRelated,1
649,Since our learner treats all values as nominal  we discretized the values of dep and len with a C45 classifier  ,Fundamental,Basis,1
650,Kruijff et al   describe an architecture which supports generating the appropriate word order for different languages,BackGround,GRelated,1
651,KruijffKorbayova et al   address the task of word order generation in the same vein,BackGround,GRelated,1
652,Similar to Langkilde  Knight   we utilize statistical methods,Fundamental,Idea,1
653,Kendalls t  which has been used for evaluating sentence ordering tasks    is the second metric we use,Fundamental,Basis,1
654,Eg  in texttotext generation    new sentences are fused from dependency structures of input sentences,BackGround,GRelated,1
655,Ringger et al   aim at regenerating the order of constituents as well as the order within them for German and French technical manuals,BackGround,GRelated,1
656,Similar to Ringger et al    we find the order with the highest probability conditioned on syntactic and semantic categories,Fundamental,Idea,1
657,Apart from acc and t  we also adopt the metrics used by Uchimoto et al   and Ringger et al  ,Fundamental,Basis,1
658,According to the inv metric  our results are considerably worse than those reported by Ringger et al  ,Compare,Compare,2
659,We retrained our system on a corpus of newspaper articles   which is manually annotated but encodes no semantic knowledge,Fundamental,Basis,1
660,The work of Uchimoto et al   is done on the free word order language Japanese,BackGround,GRelated,1
661,For the fourth baseline UCHIMOTO  we utilized a maximum entropy learner OpenNLP 8 and reimplemented the algorithm of Uchimoto et al  ,Fundamental,Basis,1
662,Uszkoreit   addresses the problem from a mostly grammarbased perspective and suggests weighted constraints  such as nom  dat  pro  pro  focus  focus  etc,BackGround,GRelated,1
663,Unlike overgeneration approaches   which select the best of all possible outputs ours is more efficient  because we do not need to generate every permutation,Compare,Compare,1
664,It also compares reasonably with other more recent evaluations   which derive their input data from the penn Treebank by transforming each sentence tree into a format suitable for the realiser  ,BackGround,SRelated,1
665,For instance    reports that the implementation of such a processor for Surge was the most time consuming part of the evaluation with the resulting component containing 4000 lines of code and 900 rules,BackGround,GRelated,1
666,The realiser presented here differs in mainly two ways from existing reversible realisers such as  s CCG system or the HPSG ERG based realiser  ,Compare,Compare,1
667,The reason for this is that the grammar is compiled from a higher level description where tree fragments are first encapsulated into socalled classes and then explicitly combined by inheritance  conjunction and disjunction to produce the grammar elementary trees cf  ,BackGround,SRelated,1
668,Thus for instance  both REALPRO   and Surge   assume that the input associates semantic literals with low level syntactic and lexical information mostly leaving the realiser to just handle inflection  word order  insertion of grammatical words and agreement,BackGround,GRelated,1
669,To associate semantic representations with natural language expressions  the FTAG is modified as proposed in  ,Fundamental,Idea,1
670,The proposal draws on ideas from   and aims to determine whether for a given input a set of TAG elementary trees whose semantics equate the input semantics  syntactic requirements and resources cancel out,Fundamental,Idea,1
671,It could be used for instance  in combination with the parser and the semantic construction module described in    to support textual entailment recognition or answer detection in question answering,BackGround,GRelated,1
672,We rely on these features to associate one and the same semantic to large sets of trees denoting semantically equivalent but syntactically distinct configurations cf  ,Fundamental,Basis,1
673,The basic surface realisation algorithm used is a bottom up  tabular realisation algorithm   optimised for TAGs,Fundamental,Basis,2
674,Similarly  KPML   assumes access to ideational  interpersonal and textual information which roughly corresponds to semantic  moodvoice  themerheme and focusground information,BackGround,SRelated,1
675,In order to ensure this determinism  NLG geared realisers generally rely on theories of grammar which systematically link form to function such as systemic functional grammar SFG    and  to a lesser extent  Meaning Text Theory MTT   ,BackGround,GRelated,1
676,First  the paraphrase figures might seem low wrt to eg  work by   which mentions several thousand outputs for one given input and an average number of realisations per input varying between 857 and 1022,BackGround,SRelated,1
677,This does not seem to be the case in  s approach where the count seems to include all sentences associated by the grammar with the input semantics,BackGround,SRelated,1
678,A Featurebased TAG FTAG    consists of a set of auxiliary or initial elementary trees and of two tree composition operations substitution and adjunction,BackGround,SRelated,1
679,A first possibility would be to draw on  s proposal and compute the enriched input based on the traversal of a systemic network,BackGround,SRelated,1
680,Thus for instance    resorts to ad hoc mapping tables to associate substitution nodes with semantic indices and frnodes to constrain adjunction to the correct nodes,BackGround,GRelated,1
681,While there have been previous systems that encode generation as planning    our approach is distinguished from these systems by its focus on the grammatically specified contributions of each individual word and the TAG tree it anchors to syntax  semantics  and local pragmatics  ,BackGround,GRelated,1
682,It also allows us to benefit from the past and ongoing advances in the performance of offtheshelf planners  ,BackGround,GRelated,1
683,Unlike some approaches    we do not have to distinguish between generating NPs and expressions of other syntactic categories,Compare,Compare,1
684,The context set of an intended referent is the set of all individuals that the hearer might possibly confuse it with  ,BackGround,SRelated,1
685,It is based on the wellknown STRIPS language  ,BackGround,SRelated,2
686,The grammar formalism we use here is that of lexicalized treeadjoining grammars LTAG Joshi and Schabes  ,Fundamental,Basis,1
687,In order to use the planner as a surface realization algorithm for TAG along the lines of Koller and Striegnitz    we attach semantic content to each elementary tree and require that the sentence achieves a certain communicative goal,Fundamental,Basis,1
688,However  this problem is NPcomplete  by reduction of Hamiltonian Cycle  unsurprisingly  given that it encompasses realization  and the very similar realization problem in Koller and Striegnitz   is NPhard,BackGround,SRelated,1
689,PDDL   is the standard input language for modern planning systems,BackGround,SRelated,1
690,In a scenario that involves multiple rabbits  multiple hats  and multiple individuals that are inside other individuals  but only one pair of a rabbit r inside a hat h  the expression X takes the rabbit from the hat is sufficient to refer uniquely to r and h  ,BackGround,SRelated,1
691,We share these advantages with systems such as SPUD  ,Fundamental,Basis,1
692,This makes our encoding more direct and transparent than those in work like Thomason and Hobbs   and Stone et al  ,Compare,Compare,1
693,We follow Stone et al   in formalizing the semantic content of a lexicalized elementary tree t as a finite set of atoms but unlike in earlier approaches  we use the semantic roles in t as the arguments of these atoms,Fundamental,Idea,1
694,The three pragmatic predicates that we will use here are hearernew  indicating that the hearer does not know about the existence of an individual and cant infer it    hearerold for the opposite  and contextset,BackGround,SRelated,1
695,In addition to the semantic content  we equip every elementary tree in the grammar with a semantic requirement and a pragmatic condition  ,Fundamental,Basis,1
696,Supertag This is a variant of the approach above  but using supertags   instead of PoS tags,BackGround,SRelated,1
697,For example  the metrics proposed in Bangalore et al    such as Simple Accuracy and Generation Accuracy  measure changes with respect to a reference string based on the idea of stringedit distance,BackGround,GRelated,1
698,The judges were then presented with the 50 sentences in random order  and asked to score the sentences according to their own scale  as in magnitude estimation   these scores were then normalised in the range 0 1,Fundamental,Idea,1
699,Regarding the interpretation of the absolute value of Pearsons correlation coefficients  both here and in the rest of the paper  we adopt Cohens scale   for use in human judgements  given in Table 1 we use this as most of this work is to do with human judgements of fluency,Fundamental,Basis,1
700,Those chosen were the Connexor parser  2 the Collins parser    and the Link Grammar parser  ,Fundamental,Basis,1
701,For example  in statistical MT the translation model and the language model are treated separately  characterised as faithfulness and fluency respectively as in the treatment in Jurafsky and Martin  ,BackGround,GRelated,1
702,A neat solution to poor sentencelevel evaluation proposed by Kulesza and Shieber   is to use a Support Vector Machine  using features such as word error rate  to estimate sentencelevel translation quality,BackGround,SRelated,1
703,Bleu   is a canonical example in matching ngrams in a candidate translation text with those in a reference text  the metric measures faithfulness by counting the matches  and fluency by implicitly using the reference ngrams as a language model,BackGround,GRelated,1
704,Quite a different idea was suggested in Wan et al    of using the grammatical judgement of a parser to assess fluency  giving a measure independent of the language model used to generate the text,BackGround,GRelated,1
705,In terms of automatic evaluation  we are not aware of any technique that measures only fluency or similar characteristics  ignoring content  apart from that of Wan et al  ,BackGround,SRelated,1
706,The consistency and magnitude of the first three parser metrics  however  lends support to the idea of Wan et al   to use something like these as indicators of generated sentence fluency,BackGround,SRelated,1
707,Similarly  the ultrasummarisation model of Witbrock and Mittal   consists of a content model  modelling the probability that a word in the source text will be in the summary  and a language model,BackGround,GRelated,1
708,In this model we violate the Markov assumption of independence in much the same way as Witbrock and Mittal   in their combination of content and language model probabilities  by backtracking at every state in order to discourage repeated words and avoid loops,Fundamental,Idea,1
709,Zajic et al   use similar scales for summarisation,BackGround,GRelated,1
710,Coreference resolution on text datasets is wellstudied eg   ,BackGround,GRelated,1
711,We employ a set of verbal features that is similar to the features used by stateoftheart coreference resolution systems that operate on text eg   ,Fundamental,Basis,2
712,Evaluation metric Coreference resolution is often performed in two phases a binary classification phase  in which the likelihood of coreference for each pair of noun phrases is assessed and a partitioning phase  in which the clusters of mutuallycoreferring NPs are formed  maximizing some global criterion  ,BackGround,SRelated,1
713,The verbal features that we have included are a representative sample from the literature eg   ,Fundamental,Basis,2
714,also consider training separate classifiers and combining their posteriors  either through weighted addition or multiplication this is sometimes called late fusion Late fusion is also employed for gesturespeech combination in  ,BackGround,GRelated,1
715,All features are computed from hand and body pixel coordinates  which are obtained via computer vision our vision system is similar to  ,Fundamental,Idea,1
716,The continuousvalued features were binned using a supervised technique  ,Fundamental,Basis,1
717,While people have little difficulty distinguishing between meaningful gestures and irrelevant hand motions eg  selftouching  adjusting glasses    NLP systems may be confused by such seemingly random movements,BackGround,GRelated,1
718,Markable noun phrases  those that are permitted to participate in coreference relations  were annotated by the first author  in accordance with the MUC task definition  ,Fundamental,Basis,1
719,To measure the similarity between gesture trajectories  we use dynamic time warping    which gives a similarity metric for temporal data that is invariant to speed,Fundamental,Basis,1
720,In addition  verbal language is different when used in combination with meaningful nonverbal communication than when it is used unimodally  ,BackGround,SRelated,1
721,Kehler finds that fullyspecified noun phrases are less likely to receive multimodal support  ,BackGround,GRelated,1
722,Last  we note that NPs with adjectival modifiers were assigned negative weights  supporting the finding of   that fullyspecified NPs are less likely to receive multimodal support,BackGround,SRelated,1
723,Experiments in both   and   find no conclusive winner among early fusion  additive late fusion  and multiplicative late fusion,BackGround,GRelated,1
724,JSdiv reports the JensenShannon divergence  a continuousvalued feature used to measure the similarity in cluster assignment probabilities between the two gestures  ,BackGround,GRelated,1
725,The objective function Equation 1 is optimized using a Java implementation of LBFGS  a quasiNewton numerical optimization technique  ,Fundamental,Basis,1
726,However  nonverbal modalities are often noisy  and their interactions with speech are complex  ,BackGround,GRelated,1
727,Our nonverbal features attempt to capture similarity between the speakers hand gestures similar gestures are thought to suggest semantic similarity  ,Fundamental,Idea,1
728,Euclidean distance captures cases in which the speaker is performing a gestural hold in roughly the same location  ,BackGround,SRelated,1
729,Nonverbal meta features Research on gesture has shown that semantically meaningful hand motions usually take place away from rest position  which is located at the speakers lap or sides  ,BackGround,GRelated,1
730,Indeed  the psychology literature describes a finitestate model of gesture  proceeding from preparation  to stroke  hold  and then retraction  ,BackGround,GRelated,1
731,Verbal meta features Meaningful gesture has been shown to be more frequent when the associated speech is ambiguous  ,BackGround,GRelated,1
732,The use of hidden variables in a conditionallytrained model follows  ,Fundamental,Idea,1
733,For example  Shriberg et al   explore the use of prosodic features for sentence and topic segmentation,BackGround,GRelated,1
734,While more flexible than the interpolation techniques described in    training modalityspecific classifiers separately is still suboptimal compared to training them jointly  because independent training of the modalityspecific classifiers forces them to account for data that they cannot possibly explain,BackGround,GRelated,1
735,Toyama and Horvitz   introduce a Bayesian network approach to modality combination for speaker identification,BackGround,GRelated,1
736,Introduction With recent advances in spoken dialogue system technologies  researchers have turned their attention to more complex domains egtutoring    technical support    medication assistance  ,BackGround,GRelated,1
737,Average standard deviation for objective metrics in the first problem Related work Discourse structure has been successfully used in noninteractive settings egunderstanding specific lexical and prosodic phenomena     natural language generation    essay scoring   as well as in interactive settings egpredictivegenerative models of postural shifts    generationinterpretation of anaphoric expressions    performance modeling  ,BackGround,GRelated,2
738,Other visual improvements for dialoguebased computer tutors have been explored in the past egtalking heads  ,BackGround,GRelated,1
739,This information is implicitly encoded in the intentional structure of a discourse as proposed in the Grosz  Sidner theory of discourse  ,Fundamental,Basis,1
740,3  The Navigation Map NM We use the Grosz  Sidner theory of discourse   to inform our NM design,Fundamental,Basis,1
741,2 ITSPOKE ITSPOKE   is a stateoftheart tutoring spoken dialogue system for conceptual physics,BackGround,SRelated,1
742,Thus  interacting with such systems can be characterized by an increased user cognitive load associated with listening to often lengthy system turns and the need to integrate the current information to the discussion overall  ,BackGround,GRelated,1
743,However  implementing the NM in a new domain requires little expertise as previous work has shown that nave users can reliably annotate the information needed for the NM  ,BackGround,GRelated,1
744,While a somewhat similar graphical representation of the discourse structure has been explored in one previous study    to our knowledge we are the first to test its benefits see Section 6,BackGround,SRelated,1
745,This theory has inspired several generic dialogue managers for spoken dialogue systems eg  ,BackGround,GRelated,1
746,One related study is that of  ,BackGround,SRelated,1
747,Results for Q16 Questions Q16 were inspired by previous work on spoken dialogue system evaluation eg   and measure users overall perception of the system,Fundamental,Idea,1
748,This situation is very similar to the training process of translation models in statistical machine translation    where parallel corpus is used to find the mappings between words from different languages by exploiting their cooccurrence patterns,Fundamental,Idea,1
749,   JPrWj o k  1  Vk j 1 This optimization problem can be solved by the EM algorithm  ,BackGround,SRelated,1
750,Studies have also shown that eye gaze has a potential to improve resolution of underspecified referring expressions in spoken dialog systems   and to disambiguate speech input  ,BackGround,GRelated,2
751,Given the recent advances in eye tracking technology    integrating nonintrusive and high performance eye trackers with conversational interfaces becomes feasible,BackGround,GRelated,1
752,Motivated by psycholinguistic studies   and recent investigations on computational models for language acquisition and grounding    we are particularly interested in two unique questions related to multimodal conversational systems 1 In a multimodal conversation that involves more complex tasks eg  both user initiated tasks and system initiated tasks  is there a reliable temporal alignment between eye gaze and spoken references so that the coupled inputs can be used for automated vocabulary acquisition and interpretation 2 If such an alignment exists  how can we model this alignment and automatically acquire and interpret the vocabularies To address the first question  we conducted an empirical study to examine the temporal relationships between eye fixations and their corresponding spoken references,Fundamental,Idea,1
753,Additionally  before speaking a word  the eyes usually move to the objects to be mentioned  ,BackGround,GRelated,1
754,Previous psycholinguistics studies have shown that the direction of gaze carries information about the focus of the users attention  ,BackGround,GRelated,1
755,In research on multimodal interactive systems  recent work indicates that the speech and gaze integration patterns can be modeled reliably for individual users and therefore be used to improve multimodal system performances  ,BackGround,GRelated,2
756,In addition  visual properties of the interface also affect user gaze behavior and thus influence the predication of attention   based on eye gaze,BackGround,SRelated,1
757,Recent work has shown that the effect of eye gaze in facilitating spoken language processing varies among different users  ,BackGround,GRelated,1
758,Recent studies have shown that multisensory information eg  through vision and language processing can be combined to effectively acquire words to their perceptually grounded objects in the environment  ,BackGround,GRelated,2
759,The perceived visual context influences spoken word recognition and mediates syntactic processing  ,BackGround,GRelated,1
760,Figure 1 Multimodal interface on tablet In this paper we explore the application of multimodal interface technologies See Andr   for an overview to the creation of more effective systems used to search and browse for entertainment content in the home,Fundamental,Basis,1
761,These interfaces are cumbersome and do not scale well as the range of content available increases  ,BackGround,GRelated,3
762,An important advantage of speech is that it makes it easy to combine multiple constraints over multiple dimensions within a single query  ,BackGround,SRelated,2
763,A number of previous systems have investigated the addition of unimodal spoken search queries to a graphical electronic program guide   Goto et al  2003 Wittenburg et al  2006,BackGround,GRelated,1
764,Others have gone beyond unimodal speech input and added multimodal commands combining speech with pointing  ,BackGround,GRelated,1
765,This develops and extends upon the multimodal architecture underlying the MATCH system  ,Fundamental,Basis,1
766,Speech recognition results  pointing gestures made on the display  and handwritten inputs  are all passed to a multimodal understanding server which uses finitestate multimodal language processing techniques   to interpret and integrate the speech and gesture,Fundamental,Basis,1
767,However  as also reported in previous work    recognition accuracy remains a serious problem,BackGround,GRelated,1
768,The past few years have seen considerable improvement in the performance of unsupervised parsers   and  for the first time  unsupervised parsers have been able to improve on the rightbranching heuristic for parsing English,BackGround,GRelated,1
769,Some of these subsets were used for scoring in  ,BackGround,SRelated,1
770,Table 1 gives two baselines and the parsing results for WSJ10  WSJ40  Negra10 and Negra40 for recent unsupervised parsing algorithms CCM and DMVCCM    UDOP   and UMLDOP  ,Fundamental,Basis,1
771,There are several algorithms for doing so    which cluster words into classes based on the most frequent neighbors of each word,BackGround,GRelated,1
772,This restriction is inspired by psycholinguistic research which suggests that humans process language incrementally  ,Fundamental,Idea,1
773,When Klein and Manning induce the partsofspeech  they do so from a much larger corpus containing the full WSJ treebank together with additional WSJ newswire  ,BackGround,SRelated,1
774,This can either be semisupervised parsing  using both annotated and unannotated data   or unsupervised parsing  training entirely on unannotated text,BackGround,GRelated,1
775,This problem is known in psycholinguistics as the problem of reanalysis  ,BackGround,GRelated,1
776,For large datasets  we use an ensemble technique inspired by Bagging  ,Fundamental,Idea,1
777,In particular  we consider an algorithm proposed by Camerini et al   which has a worstcase complexity of Okm logn  where k is the number of parses we want  n is the number of words in the input sentence  and m is the number of edges in the hypothesis graph,Fundamental,Basis,1
778,The kbest MST algorithm we introduce in this paper is the algorithm described in Camerini et al  ,Fundamental,Basis,1
779,Algorithm 1 is a version of the MST algorithm as presented by Camerini et al   subtleties of the algorithm have been omitted,Fundamental,Idea,1
780,We have introduced the Camerini et al   kbest MST algorithm and have shown how to efficiently train MaxEnt models for dependency parsing,Fundamental,Basis,2
781,Many of the model features have been inspired by the constituencybased features presented in Charniak and Johnson  ,Fundamental,Idea,1
782,Other DP solutions use constituencybased parsers to produce phrasestructure trees  from which dependency structures are extracted  ,BackGround,GRelated,1
783,An efficient algorithm for generating the kbest parse trees for a constituencybased parser was presented in Huang and Chiang   a variation of that algorithm was used for generating projective dependency trees for parsing in Dreyer et al   and for training in McDonald et al  ,BackGround,GRelated,1
784,The DP algorithms are generally variants of the CKY bottomup chart parsing algorithm such as that proposed by Eisner  ,BackGround,SRelated,1
785,2 In order to explore a rich set of syntactic features in the MST framework  we can either approximate the optimal nonprojective solution as in McDonald and Pereira    or we can use the constrained MST model to select a subset of the set of dependency parses to which we then apply lessconstrained models,BackGround,GRelated,1
786,Unlike the training procedure employed by McDonald et al   and McDonald and Pereira    we provide positive and negative examples in the training data,Compare,Compare,1
787,A second labeling stage can be applied to get labeled dependency structures as described in  ,BackGround,SRelated,1
788,The Maximum Spanning Tree algorithm 1 was recently introduced as a viable solution for nonprojective dependency parsing  ,BackGround,GRelated,1
789,McDonald et al   introduced a model for dependency parsing based on the EdmondsChuLiu algorithm,BackGround,GRelated,1
790,Many of the features above were introduced in McDonald et al   specifically  the nodetype  inside  and edge features,BackGround,SRelated,1
791,We have adopted the conditional Maximum Entropy MaxEnt modeling paradigm as outlined in Charniak and Johnson   and Riezler et al  ,Fundamental,Basis,1
792,Work on statistical dependency parsing has utilized either dynamicprogramming DP algorithms or variants of the EdmondsChuLiu MST algorithm see Tarjan  ,BackGround,SRelated,1
793,This can be reduced to Okn 2 in dense graphs 4 by choosing appropriate data structures  ,BackGround,SRelated,1
