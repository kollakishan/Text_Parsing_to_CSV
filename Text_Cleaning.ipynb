{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Cleaning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXoxxiYLQz3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-4FhSNOQ9iP",
        "colab_type": "code",
        "outputId": "d24ed8b7-e3f4-473f-83b7-b309c11f9a90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "#url = \n",
        "urlcolumn = [\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\"]\n",
        "file = pd.read_csv(\"/content/Dataset.txt\", names= urlcolumn,  sep=\"\\\\t\",header=None)\n",
        "file\n",
        "file.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c1</th>\n",
              "      <th>c2</th>\n",
              "      <th>c3</th>\n",
              "      <th>c4</th>\n",
              "      <th>c5</th>\n",
              "      <th>c6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>07-1001_0</td>\n",
              "      <td>Following the notation in section 2.1 , the ij...</td>\n",
              "      <td>VVG DT NN IN NN CD , DT NN NN IN DT NN NP VBZ ...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>07-1001_1</td>\n",
              "      <td>For the simple bag-of-word bilingual LSA as de...</td>\n",
              "      <td>IN DT JJ NN JJ NP RB VVD IN NP CD , IN NN IN D...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>07-1001_2</td>\n",
              "      <td>Discriminative word alignment models , such as...</td>\n",
              "      <td>JJ NN NN NNS , JJ IN NP CC NP ( NP NP ( NP NP ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>07-1001_3</td>\n",
              "      <td>For instance , the 1 most relaxed IBM Model-1 ...</td>\n",
              "      <td>IN NN , DT CD RBS VVN NP NP , WDT VVZ IN/that ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>07-1001_3</td>\n",
              "      <td>It can be applied to complicated models such I...</td>\n",
              "      <td>PP MD VB VVN TO JJ NNS JJ NP NP ( )</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          c1  ...       c6\n",
              "0  07-1001_0  ...  Neutral\n",
              "1  07-1001_1  ...  Neutral\n",
              "2  07-1001_2  ...  Neutral\n",
              "3  07-1001_3  ...  Neutral\n",
              "4  07-1001_3  ...  Neutral\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6ef9sOGXsVJ",
        "colab_type": "code",
        "outputId": "44239b47-141a-4b96-b9a5-7e19e4426683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "file.tail()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c1</th>\n",
              "      <th>c2</th>\n",
              "      <th>c3</th>\n",
              "      <th>c4</th>\n",
              "      <th>c5</th>\n",
              "      <th>c6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>789</th>\n",
              "      <td>07-1050_9</td>\n",
              "      <td>McDonald et al. ( ) introduced a model for dep...</td>\n",
              "      <td>NP NP NP ( ) VVD DT NN IN NN VVG VVN IN DT NP NN</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>790</th>\n",
              "      <td>07-1050_9</td>\n",
              "      <td>Many of the features above were introduced in ...</td>\n",
              "      <td>JJ IN DT NNS RB VBD VVN IN NP NP NP ( NN RB , ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>791</th>\n",
              "      <td>07-1050_12</td>\n",
              "      <td>We have adopted the conditional Maximum Entrop...</td>\n",
              "      <td>PP VHP VVN DT JJ NP NP JJ NN NN IN VVN IN NP C...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792</th>\n",
              "      <td>07-1050_13</td>\n",
              "      <td>Work on statistical dependency parsing has uti...</td>\n",
              "      <td>NN IN JJ NN VVG VHZ VVN DT JJ NN NNS CC NNS IN...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>793</th>\n",
              "      <td>07-1050_13</td>\n",
              "      <td>This can be reduced to O(kn 2) in dense graphs...</td>\n",
              "      <td>DT MD VB VVN TO NP JJ IN JJ NNS CD IN VVG JJ N...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             c1  ...       c6\n",
              "789   07-1050_9  ...  Neutral\n",
              "790   07-1050_9  ...  Neutral\n",
              "791  07-1050_12  ...  Neutral\n",
              "792  07-1050_13  ...  Neutral\n",
              "793  07-1050_13  ...  Neutral\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQPGRautX2FD",
        "colab_type": "code",
        "outputId": "d777866c-0496-4ff0-c268-04114ca1c4f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "file.sample(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c1</th>\n",
              "      <th>c2</th>\n",
              "      <th>c3</th>\n",
              "      <th>c4</th>\n",
              "      <th>c5</th>\n",
              "      <th>c6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>07-1031_3</td>\n",
              "      <td>We use the Briscoe and Carroll ( ) version of ...</td>\n",
              "      <td>PP VVP DT NP CC NP ( ) NN IN NP , DT CD NN NN ...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>07-1039_2</td>\n",
              "      <td>Most current statistical models ( ) treat the ...</td>\n",
              "      <td>RBS JJ JJ NNS ( ) VV DT VVN NNS IN DT NN IN NN...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>07-1012_1</td>\n",
              "      <td>It should be noted that models based on finite...</td>\n",
              "      <td>PP MD VB VVN IN/that NNS VVN IN JJ NN NNS VHP ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>07-1029_7</td>\n",
              "      <td>NP chunks in the shared task data are BaseNPs ...</td>\n",
              "      <td>NN NNS IN DT VVN NN NNS VBP NP , WDT VBP JJ NP...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>578</th>\n",
              "      <td>07-1037_8</td>\n",
              "      <td>Coming right up to date , ( ) demonstrate that...</td>\n",
              "      <td>VVG RB RB TO NN , ( ) VV DT NP NN NN NNS MD VV...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            c1  ...        c6\n",
              "452  07-1031_3  ...   Neutral\n",
              "597  07-1039_2  ...   Neutral\n",
              "160  07-1012_1  ...   Neutral\n",
              "431  07-1029_7  ...   Neutral\n",
              "578  07-1037_8  ...  Positive\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo2jYIlQYAvG",
        "colab_type": "code",
        "outputId": "9c9a45bf-c301-4ccc-c1b6-8cdcd6c46e9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "file.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c1</th>\n",
              "      <th>c2</th>\n",
              "      <th>c3</th>\n",
              "      <th>c4</th>\n",
              "      <th>c5</th>\n",
              "      <th>c6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>794</td>\n",
              "      <td>794</td>\n",
              "      <td>794</td>\n",
              "      <td>784</td>\n",
              "      <td>784</td>\n",
              "      <td>784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>552</td>\n",
              "      <td>794</td>\n",
              "      <td>794</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>07-1027_11</td>\n",
              "      <td>Such conversions have been performed for other...</td>\n",
              "      <td>PP VHP VVN DT NN VVN IN PP$ JJ NN ( ) IN VVG N...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>493</td>\n",
              "      <td>314</td>\n",
              "      <td>688</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                c1  ...       c6\n",
              "count          794  ...      784\n",
              "unique         552  ...        3\n",
              "top     07-1027_11  ...  Neutral\n",
              "freq            11  ...      688\n",
              "\n",
              "[4 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVj05_fVYDhW",
        "colab_type": "code",
        "outputId": "d4fa4ae0-885b-4c7c-a8c7-d68d4afdbc33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "file.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 794 entries, 0 to 793\n",
            "Data columns (total 6 columns):\n",
            "c1    794 non-null object\n",
            "c2    794 non-null object\n",
            "c3    794 non-null object\n",
            "c4    784 non-null object\n",
            "c5    784 non-null object\n",
            "c6    784 non-null object\n",
            "dtypes: object(6)\n",
            "memory usage: 37.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHWXv7PgpFJ_",
        "colab_type": "code",
        "outputId": "6a901205-003c-4042-c544-a4d639f3dc60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "file2 =file.drop([\"c1\",\"c3\"], axis = 1)\n",
        "file2.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c2</th>\n",
              "      <th>c4</th>\n",
              "      <th>c5</th>\n",
              "      <th>c6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Following the notation in section 2.1 , the ij...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>For the simple bag-of-word bilingual LSA as de...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Discriminative word alignment models , such as...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>For instance , the 1 most relaxed IBM Model-1 ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It can be applied to complicated models such I...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  c2  ...       c6\n",
              "0  Following the notation in section 2.1 , the ij...  ...  Neutral\n",
              "1  For the simple bag-of-word bilingual LSA as de...  ...  Neutral\n",
              "2  Discriminative word alignment models , such as...  ...  Neutral\n",
              "3  For instance , the 1 most relaxed IBM Model-1 ...  ...  Neutral\n",
              "4  It can be applied to complicated models such I...  ...  Neutral\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RkwOQuXpWFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file2[\"c6\"] = pd.Categorical(file2.c6)\n",
        "file2[\"c2\"] = pd.Categorical(file2.c6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLV7dG6Brwh0",
        "colab_type": "code",
        "outputId": "8a3fc703-fc44-4815-fc14-fa22e7779a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "file2.info()"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 794 entries, 0 to 793\n",
            "Data columns (total 4 columns):\n",
            "c2    784 non-null category\n",
            "c4    784 non-null object\n",
            "c5    784 non-null object\n",
            "c6    784 non-null category\n",
            "dtypes: category(2), object(2)\n",
            "memory usage: 14.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzoOhBfXr0We",
        "colab_type": "code",
        "outputId": "81c1dae9-b9d1-4d6b-a7f6-681d791044c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "aa= file2.replace(\"Neutral\", \"1\")\n",
        "bb = aa.replace(\"Positive\", \"2\")\n",
        "cc = bb.replace(\"Negative\", \"3\")\n",
        "cc.head()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c2</th>\n",
              "      <th>c4</th>\n",
              "      <th>c5</th>\n",
              "      <th>c6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  c2           c4        c5 c6\n",
              "0  1  Fundamental      Idea  1\n",
              "1  1  Fundamental     Basis  1\n",
              "2  1   BackGround  GRelated  1\n",
              "3  1   BackGround  GRelated  1\n",
              "4  1   BackGround  SRelated  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMMq5dSdxdNQ",
        "colab_type": "code",
        "outputId": "9d129408-064e-4299-c4a5-7c4ab6e9be13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "xx = dd.apply(lambda x: x.strip('('))\n",
        "xx.head()"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-af6526b83234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'('\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6485\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6486\u001b[0m                          kwds=kwds)\n\u001b[0;32m-> 6487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;31m# compute the result using the series generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-174-af6526b83234>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'('\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: (\"'Series' object has no attribute 'strip'\", 'occurred at index c2')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwntOhvNzHpI",
        "colab_type": "code",
        "outputId": "04909c21-4099-428d-ef5c-7d4c16c25271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "dd[\"c2\"].head()"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Following the notation in section 2.1 , the ij...\n",
              "1    For the simple bag-of-word bilingual LSA as de...\n",
              "2    Discriminative word alignment models , such as...\n",
              "3    For instance , the 1 most relaxed IBM Model-1 ...\n",
              "4    It can be applied to complicated models such I...\n",
              "Name: c2, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1b8sVt-3McL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "cf46db1f-d71d-4485-abac-1ba92e1124ab"
      },
      "source": [
        "newString = dd[\"c2\"].translate(string.maketrans(\"\",\"\"), string.punctuation)"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-176-6150e2b0ce39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnewString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"c2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'translate'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhuHVUZt81Ne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "895838db-171f-4ae1-881e-703f4e449b8c"
      },
      "source": [
        "import string\n",
        "s = dd[\"c2\"]\n",
        "\n",
        "exclude = str.maketrans(\",\" ,\"*\")\n",
        "\n",
        "s = ''.join(ch for ch in s if ch not in exclude)\n",
        "print (s)\n",
        "s\n",
        "\n",
        "#for string in dd[\"c2\"]:\n",
        "  "
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Following the notation in section 2.1 , the ij-th entry of the matrix W is defined as in ( ) Wj = (1 - e Wz ) where Cj is the total number of words in the j-th sentence pairFor the simple bag-of-word bilingual LSA as described in Section 2.2.1 , after SVD on the sparse matrix using the toolkit SVDPACK ( ) , all source and target words are projected into a low-dimensional (R = 88) LSA-spaceDiscriminative word alignment models , such as Ittycheriah and Roukos ( ); Moore ( ); Blunsom and Cohn ( ) , have received great amount of study recentlyFor instance , the 1 most relaxed IBM Model-1 , which assumes that any source word can be generated by any target word equally regardless of distance , can be improved by demanding a Markov process of alignments as in HMM-based models ( ) , or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models ( )It can be applied to complicated models such IBM Model-4 ( )The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing ( ) using all English sentences in the parallel training dataLSA has been successfully applied to information retrieval ( ) , statistical langauge modeling ( ) and etcAlternative constructions of the matrix are possible using raw counts or TF-IDF ( )It has been shown that human knowledge , in the form of a small amount of manually annotated parallel data to be used to seed or guide model training , can significantly improve word alignment F-measure and translation performance ( )Our decoder is a phrase-based multi-stack imple-mentation of the log-linear model similar to Pharaoh ( )Since Arabic is a morphologically rich language where affixes are attached to stem words to indicate gender , tense , case and etc , in order to reduce vocabulary size and address out-of-vocabulary words , we split Arabic words into affix and root according to a rule-based segmentation scheme ( ) with the help from the Buckwalter analyzer ( ) outputBasic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by ( ) and ( )In ( ) , bilingual semantic maps are constructed to guide word alignmentAs formulated in the competitive linking algorithm ( ) , the problem of word alignment can be regarded as a process of word linkage disambiguation , that is , choosing correct associations among all competing hypothesisThe example demos that due to reasonable constraints placed in word alignment training , the link to \"_tK\" is corrected and consequently we have accurate word translation for the Arabic singleton 7 Heuristics based on co-occurrence analysis , such as point-wise mutual information or Dice coefficients  , have been shown to be indicative for word alignments ( )These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method ( )By combining word alignments in two directions using heuristics ( ) , a single set of static word alignments is then formedWe simply modify the GIZA++ toolkit ( ) by always weighting lexicon probabilities with soft constraints during iterative model training , and obtain 0.7% TER reduction on both sets and 0.4% BLEU improvement on the test setWe measure translation performance by the BLEU score ( ) and Translation Error Rate (TER) ( ) with one reference for each hypothesisToutanova et al. ( ) augmented bilingual sentence pairs with part-of-speech tags as linguistic constraints for HMM-based word alignmentsWhile word alignments can help identifying semantic relations ( ) , we proceed in the reverse directionWe shall take HMM-based word alignment model ( ) as an example and follow the notation of ( )Our baseline word alignment model is the word-to-word Hidden Markov Model ( )Many state-of-the-art SMT systems do not use trees and base the ordering decisions on surface phrases ( )An important advantage of our model is that it is global , and does not decompose the task of ordering a target sentence into a series of local decisions , as in the recently proposed order models for Machine Transition ( )Alternatively , order is modelled in terms of movement of automatically induced hierarchical structure of sentences ( )These N-best lists are generated using approximate search and simpler models , as in the re-ranking approach of ( )tings , even for a bi-gram language model ( )9 The advantages of modeling how a target language syntax tree moves with respect to a source language syntax tree are that (i) we can capture the fact that constituents move as a whole and generally respect the phrasal cohesion constraints ( ) , and (ii) we can model broad syntactic reordering phenomena , such as subject-verb-object constructions translating into subject-object-verb ones , as is generally the case for English and JapanesePrevious work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees ( ) or dependency trees ( ) , which are obtained using a parser trained to determine linguistic constituencyOur results show that combining features derived from the source and target dependency trees , distortion surface order-based features (like the distortion used in Pharaoh ( )) and language model-like features results in a model which significantly outperforms models using only some of the information sourcesPharaoh DISP: Displacement as used in Pharaoh ( )These models are combined as feature functions in a (log)linear model for predicting a target sentence given a source sentence , in the framework proposed by ( )The target dependency trees are obtained through projection of the source dependency trees , using the word alignment (we use GIZA++ ( )) , ensuring better parallelism of the source and target structuresThe sentences were annotated with alignment (using GIZA++ ( )) and syntactic dependency structures of the source and target , obtained as described in Section 2Our model is discriminatively trained to select the best order (according to the BLEU measure) ( ) of an unordered target dependency tree from the space of possible ordersOur algorithm for obtaining target dependency trees by projection of the source trees via the word alignment is the one used in the MT system of ( )It follows the order model defined in ( )Our baseline SMT system is the system of Quirk et al. ( )The projection algorithm of ( ) defines heuristics for each of these problems1 Previous studies have shown that if both the source and target dependency trees represent linguistic constituency , the alignment between subtrees in the two languages is very complex ( )A host of discriminative methods have been introduced ( )2 We also investigated extraction-specific metrics: the frequency of interior nodes - a measure of how often the alignments violate the constituent structure of English parses - and a variant of the CPER metric of Ayan and Dorr ( )* From Ayan and Dorr ( ) , grow-diag-final heuristic5 Similarly , we compared our Chinese results to the GIZA++ results in Ayan and Dorr ( )Additionally , we evaluated our model with the transducer analog to the consistent phrase error rate (CPER) metric of Ayan and Dorr ( )Like the classic IBM models ( ) , our model will introduce a latent alignment vector a = {a 1  ,... ,a J } that specifies the position of an aligned target word for each source wordHowever , few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them ( )Syntactic methods are an increasingly promising approach to statistical machine translation , being both algorithmically appealing ( ) and empirically successful ( )Daume III and Marcu ( ) employs a syntax-aware distortion model for aligning summaries to documents , but condition upon the roots of the constituents that are jumped over during a transition , instead ofthose that are visited during a walk through the treeOur transductive learning algorithm , Algorithm 1 , is inspired by the Yarowsky algorithm ( )Under certain precise conditions , as described in ( ) , we can analyze Algorithm 1 as minimizing the entropy of the distribution over translations of UWe used the following scoring functions in our experiments: Length-normalized Score: Each translated sentence pair (t , s) is scored according to the model probability p (t | s) normalized by the length |t| of the target sentence: Score(t , s)  = p (t | s) 1*1 (3) Confidence Estimation: The confidence estimation which we implemented follows the approaches suggested in ( ): The confidence score of a target sentence t is calculated as a log-linear combination of phrase posterior probabilities , Levenshtein-based word posterior probabilities , and a target language model scoreOne language pair creates data for another language pair and can be naturally used in a ( )-style co-training algorithmThese lists are rescored with the following models: (a) the different models used in the decoder which are described above , (b) two different features based on IBM Model l ( ) , (c) posterior probabilities for words , phrases , n-grams , and sentence length ( ) , all calculated over the N-best list and using the sentence probabilities which the baseline system assigns to the translation hypothesesIn ( ) , a generative model for word alignment is trained using unsupervised learning on parallel textIn ( ) co-training is applied to MTAlong similar lines , ( ) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentencesBLEU score using the algorithm described in ( )The models (or features) which are employed by the decoder are: (a) one or several phrase table(s) , which model the translation direction p (s 1 1) , (b) one or several n-gram language model(s) trained with the SRILM toolkit ( ); in the experiments reported here , we used 4-gram models on the NIST data , and a trigram model on EuroParl , (c) a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase , and (d) a word penaltyfor a detailed description see ( )For details , see ( )It overlaps with the original phrase tables , but also contains many new phrase pairs ( )Self-training for SMT was proposed in ( )Recently , Cabezas and Resnik ( ) experimented with incorporating WSD translations into Pharaoh , a state-of-the-art phrase-based MT system ( )The relatively small improvement reported by Cabezas and Resnik ( ) without a statistical significance test appears to be inconclusiveNote that comparing with the MT systems used in ( ) and ( ) , the Hiero system we are using represents a much stronger baseline MT system upon which the WSD system must improveCarpuat and Wu ( ) integrated the translation predictions from a Chinese WSD system ( ) into a Chinese-English word-based statistical MT system using the ISI ReWrite decoder ( )Note that the experiments in ( ) did not use a state-of-the-art MT system , while the experiments in ( ) were not done using a full-fledged MT system and the evaluation was not on how well each source sentence was translated as a wholeWe obtain accuracy that compares favorably to the best participating system in the task ( )For our experiments , we use the SVM implementation of ( ) as it is able to work on multi-class problems to output the classification probability for each classCapitalizing on the strength of the phrase-based approach , Chiang ( ) introduced a hierarchical phrase-based statistical MT system , Hiero , which achieves significantly better translation performance than Pharaoh ( ) , which is a state-of-the-art phrase-based statistical MT systemIn this paper , we successfully integrate a state-of-the-art WSD system into the state-of-the-art hierarchical phrase-based MT system , Hiero ( )Hiero ( ) is a hierarchical phrase-based model for statistical machine translation , based on weighted synchronous context-free grammar (CFG) ( )Similar to ( ) , we trained the Hiero system on the FBIS corpus , used the NIST MT 2002 evaluation test set as our development set to tune the feature weights , and the NIST MT 2003 evaluation test set as our test dataFollowing ( ) , we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores ( ) based on case-insensitive n-gram matching , where n is up to 4A n-gram language model adds a dependence on (n�1) neighboring target-side words ( ) , making decoding much more difficult but still polynomial; in this paper , we add features that depend on the neighboring source-side words , which does not affect decoding complexity at all because the source string is fixedThe improvement of 0.57 is statistically significant at p  < 0.05 using the sign-test as described by Collins et al. ( ) , with 374 (+1) , 318 (�1) and 227 (0)To perform translation , state-of-the-art MT systems use a statistical phrase-based approach ( ) by treating phrases as the basic units of translationThe word alignments of both directions are then combined into a single set of alignments using the \"diag-and\" method of Koehn et al. ( )Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results ( )Our implemented WSD classifier uses the knowledge sources of local collocations , parts-of-speech (POS) , and surrounding words , following the successful approach of ( )First , we performed word alignment on the FBIS parallel corpus using GIZA++ ( ) in both directionsHiero uses a general log-linear model ( ) where the weight of a derivation  D for a particular source sentence and its translation is where  ^  i is a feature function and  X i is the weight for feature  ^  iUsing the MT 2002 test set , we ran the minimum-error rate training (MERT) ( ) with the decoder to tune the weights for each featurethe English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus , we trained a tri-gram language model using the SRI Language Modelling Toolkit ( )WSD approaches can be classified as (a) knowledge-based approaches , which make use of linguistic knowledge , manually coded or extracted from lexical resources ( ); (b) corpus-based approaches , which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models ( ); and (c) hybrid approaches , which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge ( )Although it has been argued that WSD does not yield better translation quality than a machine translation system alone , it has been recently shown that a WSD module that is developed following specific multilingual requirements can significantly improve the performance of a machine translation system ( )Finally , MC-WSD ( ) is a multi-class averaged perceptron classifier using syntactic and narrow context features , with one component trained on the data provided by Senseval and other trained on WordNet glossesIt is an interesting approach to learning which has been considered promising for several applications in natural language processing and has been explored for a few of them , namely POS-tagging , grammar acquisition and semantic parsing ( )For example , Dang and Palmer ( ) also use a rich set of features with a traditional learning algorithm (maximum entropy)Linguistic knowledge is available in electronic resources suitable for practical use , such as WordNet ( ) , dictionaries and parsersThere is not always a direct relation between the possible senses for a word in a (monolingual) lexicon and its translations to a particular language , so this represents a different task to WSD against a (monolingual) lexicon ( )CLaC1 ( ) uses a Naive Bayes algorithm with a dynamically adjusted context window around the target wordThe sense with the highest count of overlapping words in its dictionary definition and in the sentence containing the target verb (excluding stop words)   ( ) ,   represented by has_overlapping(sentence , translation) : has_overlapping(snt 1 , voltar)Verbs and possible senses in our corpus Both corpora were lemmatized and part-of-speech (POS) tagged using Minipar ( ) and Mxpost ( ) , respectivellyThese approaches have shown good results; particularly those using supervised learning ( )WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories ( )Syntalex-3 ( ) is based on an ensemble of bagged decision trees with narrow context part-of-speech features and bigramsThis is achieved using Inductive Logic Programming (ILP) ( ) , which has not yet been applied to WSDInductive Logic Programming ( ) employs techniques from Machine Learning and Logic Programming to build first-order theories from examples and background knowledge , which are also represented by first-order clauses2. A more specific clause (the bottom clause) is built using inverse entailment ( ) , generally consisting of the representation of all the knowledge about that exampleThis corpus was automatically annotated with the translation of the verb using a tagging system based on parallel corpus , statistical information and translation dictionaries ( ) , followed by a manual revisionAll the knowledge sources were made available to be used by the inference engine , since previous experiments showed that they are all relevant ( )We use the Aleph ILP system ( ) , which provides a complete inference engine and can be customized in various waysIn the hybrid approaches that have been explored so far , deep knowledge , like selectional preferences , is either pre-processed into a vector representation to accommodate machine learning algorithms , or used in previous steps to filter out possible senses e.g. ( )Roark and Bacchiani ( ) showed that weighted count-merging is a special case of maximum a posteriori (MAP) estimation , and successfully used it for probabilistic context-free grammar domain adaptation ( ) and language model adaptation ( )We have recently shown that this algorithm is effective in estimating the sense priors of a set of nouns ( )However , in ( ) , we showed that in a supervised setting where one has access to some annotated training data , the EM-based method in section 5 estimates the sense priors more effectively than the method described in ( )A similar work is the recent research by Chen et al. ( ) , where active learning was used successfully to reduce the annotation effort for WSD of 5 English verbs using coarse-grained evaluationThis is slightly higher than the 5.8 senses per verb in ( ) , where the experiments were conducted using coarse-grained evaluationFor WSD , Fujii et al. ( ) used selective sampling for a Japanese language WSD system , Chen et al. ( ) used active learning for 5 verbs using coarse-grained evaluation , and HDang ( ) employed active learning for another set of 5 verbsTo investigate this , Escudero et al. ( ) and Martinez and Agirre ( ) conducted experiments using the DSO corpus , which contains sentences from two different corpora , namely Brown Corpus (BC) and Wall Street Journal (WSJ)Escudero et al. ( ) pointed out that one of the reasons for the drop in accuracy is the difference in sense priors (i.e. , the proportions of the different senses of a word) between BC and WSJFollowing the setup of ( ) , we similarly made use of the DSO corpus to perform our experiments on domain adaptationAs mentioned in section 1 , research in ( ) noted an improvement in accuracy when they adjusted the BC and WSJ datasets such that the proportions of the different senses of each word were the same between BC and WSJEscudero et al. ( ) used the DSO corpus to highlight the importance of the issue of domain dependence of WSD systems , but did not propose methods such as active learning or count-merging to address the specific problem of how to perform domain adaptation for WSDThis is similar to the approach taken in ( ) where they focus on determining the predominant sense of words in corpora drawn from finance versus sports domainsResearch by McCarthy et al. ( ) and Koeling et al. ( ) pointed out that a change of predominant sense is often indicative of a change in domainThese knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( )To reduce the effort required to adapt a WSD system to a new domain , we employ an active learning strategy ( ) to select examples to annotate from the new domain of interestWith active learning ( ) , we use uncertainty sampling as shown r  � WSD system trained on D T b  � word sense prediction for d using r  p  � confidence of prediction b if p < p  min then Figure 1: Active learning in Figure 1The WordNet Domains resource ( ) assigns domain labels to synsets in WordNetAmong the few currently available manually sense-annotated corpora for WSD , the SEMCOR (SC) corpus ( ) is the most widely usedThe DSO corpus ( ) contains 192 ,800 annotated examples for 121 nouns and 70 verbs , drawn from BC and WSJIn this section , we describe an EM-based algorithm that was introduced by Saerens et al. ( ) , which can be used to estimate the sense priors , or a priori probabilities of the different senses in a new datasetMost of this section is based on ( )In applying active learning for domain adaptation , Zhang et al. ( ) presented work on sentence boundary detection using generalized Winnow , while Tur et al. ( ) performed language model adaptation of automatic speech recognition systemsIn most contexts , the similarity between chocolate , say , and a narcotic like heroin will mea-gerly reflect the simple ontological fact that both are kinds of substances; certainly , taxonomic measures of similarity as discussed in Budanitsky and Hirst ( ) will capture little more than this commonalityThe function (%sim arg 0 CAT) reflects the perceived similarity between the putative member arg 0 and a synset CAT in WordNet , using one of the standard formulations described in Budanitsky and Hirst ( )Whissell ( ) reduces the notion of affect to a single numeric dimension , to produce a dictionary of affect that associates a numeric value in the range 1.0 (most unpleasant) to 3.0 ( )We have described an approach that can be seen as a functional equivalent to the CPA (Corpus Pattern Analysis) approach of Pustejovsky et al. ( ) , in which our goal is not that of automated induction of word senses in context (as it is in CPA) but the automated induction of flexible , context-sensitive category structuresSince the line between literal and metaphoric uses of a category is often impossible to draw , the best one can do is to accept metaphor as a gradable phenomenon ( )The most revealing variations are syntagmatic in nature , which is to say , they look beyond individual word forms to larger patterns of contiguous usage ( )Dice's coefficient ( ) is used to implement this measureAs noted by De Leenheer and de Moor ( ) , ontologies are lexical representations of concepts , so we can expect the effects of context on language use to closely reflect the effects of context on ontolog-Linguistic variation across contexts is often symp- ical structureWhile simile is a mechanism for highlighting inter-concept similarity , metaphor is at heart a mechanism of category inclusion ( )Glucksberg ( ) notes that the same category , used figuratively , can exhibit different qualities in different metaphorsIn this section , we describe how we use Markov chain Monte Carlo methods to perform inference in the statistical models described in the previous section; Andrieu et al. ( ) provide an excellent introduction to MCMC techniquesThese are short statements that restrict the space of languages in a concrete way (for instance \"object-verb ordering implies adjective-noun ordering\"); Croft ( ) , Hawkins ( ) and Song ( ) provide excellent introductions to linguistic typologyThis is a well-documented issue (see , eg. , ( )) stemming from the fact that any set of languages is not sampled uniformly from the space of all probable languagesThe closest work is represented by the books Possible and Probable Languages ( ) and Language Classification by Numbers ( ) , but the focus of these books is on automatically discovering phylogenetic trees for languages based on Indo-European cognate sets ( )Those that reference Hawkins (eg. , #11) are based on implications described by Hawkins ( ); those that reference Lehmann are references to the principles decided by Lehmann ( ) in Ch 4 & 8They have also been used computationally to aid in the learning of unsupervised part of speech taggers ( )For instance our #7 is implication #18 from Greenberg , reproduced by Song ( )We examined sentences using a phrase structure parser ( ) and an HPSG parser ( )Since the number of parameters in NLM is still large , several smoothing methods are used ( ) to produce more accurate probabilities , and to assign nonzero probabilities to any word stringWe would like to see more refined online learning methods with kernels ( ) that we could apply in these areasTherefore we make use of an online learning algorithm proposed by ( ) , which has a much smaller computational costBlei , 2003; Wang et al. , 2005) , our result may encourage the study ofthe combination offeatures forlanguage modelingWe used a Viterbi decoding ( ) for the partitionDiscriminative language models (DLMs) have been proposed to classify sentences directly as correct or incorrect ( ) , and these models can handle both non-local and overlapping informationFor fast kernel computation , the Polynomial Kernel Inverted method (PKI)) is proposed ( ) , which is an extension of Inverted Index in Information RetrievalThe class model was originally proposed by ( )However , by considering only those counts that actually change , the algorithm can be made to scale somewhere between linearly and quadratically to the number of classes ( )Recently , Whole Sentence Maximum Entropy Models ( ) (WSMEs) have been introducedIn our experiments , we did not examine the result of using other sampling methods , For example , it would be possible to sample sentences from a whole sentence maximum entropy model ( ) and this is a topic for future researchA contrastive estimation method ( ) is similar to ours with regard to constructing pseudo-negative examplesIf the kernel-trick ( ) is applied to online margin-based learning , a subset of the observed examples , called the active set , needs to be storedIt should be noted that models based on finite state transducers have been shown to be adequate for describing fusion as well( ) , and further work should evaluate these types of models in ASR of languages with higher indexes of fusionThe final approach applies a manually constructed rule-based morphological tagger( )For training the LMs , a subset of 43 million words from the Estonian Segakorpus was used( ) , preprocessed with a morphological analyzer( )In ( ) a WER of 44.5% was obtained with word-based trigrams and a WER of 37.2% with items similar to ones from \"grammar\" using the same speech corpus as in this workIt should be noted that every OOV causes roughly two errors in recognition , and vocabulary decomposition approaches such as the ones evaluated here give some benefits to word error rate (WER) even in recognizing languages such as English( )This is similar to what was introduced as \"flat hybrid model\"( ) , and it tries to model OOV-words as sequences of words and fragmentsThe results for \"hybrid\" are in in the range suggested by earlier work( )The morph approach was developed for the needs of Finnish speech recognition , which is a high synthesis , moderate fusion and very low orthographic irregularity language , whereas the hybrid approach in ( ) was developed for English , which has low synthesis , moderate fusion , and very high orthographic irregularityVarigrams( ) are used in this work , and to make LMs trained with each approach comparable , the varigrams have been grown to roughly sizes of 5 million countsing approach , growing varigram models( ) were used with no limits as to the order of n-grams , but limiting the number of counts to 4.8 and 5 million countsFor example , in English with language models (LM) of 60k words trained from the Gigaword Corpus V.2( ) , and testing on a very similar Voice of America -portion of TDT4 speech corpora( ) , this gives a OOV rate of 1.5%Models of this type have previously been shown to yield very good g2p conversion results ( )It has been argued that using morphological information is important for languages where morphology has an important influence on pronunciation , syllabiication and word stress such as German , Dutch , Swedish or , to a smaller extent , also English ( )Decision trees were one of the first data-based approaches to g2p and are still widely used ( )Best results were obtained when using a variant of Modified Kneser-Ney Smoothing 2 ( )( ) also used a joint n-gram modelWe compared four different state-of-the-art unsu-pervised systems for morphological decomposition (cf. ( ))In very recent work , ( ) developed an unsupervised algorithm (f-meas: 68%; an extension of RePortS) whose segmentations improve g2p when using a the decision tree (PER: 3.45%)The German corpus used in these experiments is CELEX ( )Among the unsupervised systems , best results 7 on the g2p task with morphological annotation were obtained with the RePortS system ( )The same algorithms have previously been shown to help a speech recognition task ( )The joint n-gram model performs significantly better than the decision tree (essentially based on ( )) , and achieves scores comparable to the Pronunciation by Analogy (PbA) algorithm ( )This is much faster than the times for Pronunciation by Analogy (PbA) ( ) on the same corpusExamples of such approaches using Hidden Markov Models are ( ) (who applied the HMM to the related task of phoneme-to-grapheme conversion) , ( ) and ( )For German , ( ) show that information about stress assignment and the position of a syllable within a word improve g2p conversionVowel length and quality has been argued to also depend on morphological structure ( )The two rule-based systems we evaluated , the ETI 4 morphological system and SMOR 5 ( ) , are both high-quality systems with large lexica that have been developed over several yearsWe used the syllabifier described in ( ) , which works similar to the joint n-gram model used for g2p conversionA possible reason for the observed dichotomy in the behavior of the vowel and consonant inventories with respect to redundancy can be as follows: while the organization of the vowel inventories is known to be governed by a single force - the maximal perceptual contrast ( )) , consonant inventories are shaped by a complex interplay of several forces ( )It has been postulated earlier by functional phonologists that such regularities are the consequences of certain general principles like maximal perceptual contrast ( ) , which is desirable between the phonemes of a language for proper perception of each individ-ual phoneme in a noisy environment , ease of articulation ( ) , which requires that the sound systems of all languages are formed of certain universal (and highly frequent) sounds , and ease of learnability( ) , which is necessary for a speaker to learn the sounds of a language with minimum effortSuch an observation is significant since whether or not these principles are similar/different for the two inventories had been a question giving rise to perennial debate among the past researchers ( )On the other hand , in spite of several attempts ( ) the organization of the consonant inventories lacks a satisfactory explanationVarious attempts have been made in the past to explain the aforementioned trends through linguistic insights ( ) mainly establishing their statistical significanceFor instance , in biological systems we find redundancy in the codons ( ) , in the genes ( ) and as well in the proteins ( )In fact , the organization of the vowel inventories (especially those with a smaller size) across languages has been satisfactorily explained in terms of the single principle of maximal perceptual contrast ( )This redundancy is present mainly to reduce the risk of the complete loss of information that might occur due to accidental errors ( )Many typological studies ( ) of segmental inventories have been carried out in past on the UCLA Phonological Segment Inventory Database (UPSID) ( )In order to explain these trends , feature economy was proposed as the organizing principle of the consonant inventories ( )Inspired by the aforementioned studies and the concepts of information theory ( ) we try to quantitatively capture the amount of redundancy found across the consonant Table 1: The table shows four plosivesFor this purpose , we present an information theoretic definition of redundancy , which is calculated based on the set of features 1 ( ) that are used to express the consonantsHowever , one of the earliest observations about the consonant inventories has been that consonants tend to occur in pairs that exhibit strong correlation in terms of their features ( )2 Previous Work Previous work � e.g. ( ) � has mostly assumed that one has a training lexicon of transliteration pairs , from which one can learn a model , often a source-channel or MaxEnt-based modelA linear classifier is trained using the Winnow algorithm from the SNoW toolkit ( )Using comparable corpora , the named-entities for persons and locations were extracted from the English text; in this paper , the English named-entities were extracted using the named-entity recognizer described in Li et al. ( ) , based on the SNoW machine learning toolkit ( )This is quite small compared to previous approaches such as Knight and Graehl ( ) or Gao et al. ( )Gildea and Jurafsky ( ) counted the number of features whose values are different , and used them as a substitution costHalle and Clements ( )'s distinctive features are used in order to model the substitution/ insertion/deletion costs for the string-alignment algorithm and linear classifierAll pronunciations are based on the WorldBet transliteration system ( ) , an ascii-only version of the IPAa. For all the training data , the pairs of pronunciations are aligned using standard string alignment algorithm based on Kruskal ( )For the set of features X and set of weights W , the linear classifier is defined as [1] ( ) X  = { X [ , X2 , ..In this paper , the phonetic transliteration is performed using the following steps: 1)  Generation of the pronunciation for English words and target words: a. Pronunciations for English words are obtained using the Festival text-to-speech system ( )Based on the pronunciation error data of learners of English as a second language as reported in ( ) , we propose the use of what we will term pseudofeaturesExamples of the top-3 candidates in the transliteration of English-Korean To evaluate the proposed transliteration methods quantitatively , the Mean Reciprocal Rank (MRR) , a measure commonly used in information retrieval when there is precisely one correct answer ( ) was measured , following Tao and Zhai ( )In our work , we adopt the method proposed in ( ) and apply it to the problem of transliterationThe substitution/insertion/deletion cost for the string alignment algorithm is based on the baseline cost from ( )The pseudo features in this study are same as in Tao et al. ( )MRRs of the phonetic transliteration The baseline was computed using the phonetic transliteration method proposed in Tao et al. ( )By treating a letter/character as a word and a group of letters/characters as a phrase or token unit in SMT , one can easily apply the traditional SMT models , such as the IBM generative model ( ) or the phrase-based translation model ( ) to transliterationIn G2P studies , Font Llitjos and Black ( ) showed how knowledge of language of origin may improve conversion accuracyPhonetic transliteration can be considered as an extension to the traditional grapheme-to-phoneme (G2P) conversion ( ) , which has been a much-researched topic in the field of speech processingMany of the loanwords exist in today's Chinese through semantic transliteration , which has been well received ( ) by the people because of many advantagesUnfortunately semantic transliteration , which is considered as a good tradition in translation practice ( ) , has not been adequately addressed computationally in the literature5.S Semantic Transliteration The performance was measured using the Mean Reciprocal Rank (MRR) metric ( ) , a measure that is commonly used in information retrieval , assuming there is precisely one correct answerIn computational linguistic literature , much effort has been devoted to phonetic transliteration , such as English-Arabic , English-Chinese ( ) , English-Japanese ( ) and English-KoreanIn the extraction of transliterations , data-driven methods are adopted to extract actual transliteration pairs from a corpus , in an effort to construct a large , up-to-date transliteration lexicon ( )The Latin-scripted personal names are always assumed to homogeneously follow the English phonic rules in automatic transliteration ( )This model is conceptually similar to the joint source-channel model ( ) where the target token t i depends on not only its source token s i but also the history t i-1 and s i -1Some recent work ( ) has attempted to introduce preference into a probabilistic framework for selection of Chinese characters in phonetic transliterationIn transliteration modeling , transliteration rules are trained from a large , bilingual transliteration lexicon ( ) , with the objective of translating unknown words on the fly in an open , general domainAs discussed elsewhere ( ) , out of several thousand common Chinese characters , a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese , e.g.only 731 Chinese characters are adopted in the E-C corpusAs a Chinese transliteration can arouse to certain connotations , the choice of Chinese characters becomes a topic of interest ( )Table 4: Lexicon statistics For Arabic , as a full-size Arabic lexicon was not available to us , we used the Buckwalter morphological analyzer ( ) to derive a lexiconFor example , ( ) showed that factored language models , which consider morphological features and use an optimized backoff policy , yield lower perplexityA recent work ( ) experimented with English-to-Turkish translation with limited success , suggesting that inflection generation given morphological features may give positive resultsMore recently , ( ) achieved improvements in Czech-English MT , optimizing a Table 1: Morphological features used for Russian and Arabic set of possible source transformations , incorporating morphologyIdeally , the best word analysis should be provided as a result of contextual disambiguation (e.g. , ( )); we leave this for future workAnother work ( ) showed improvements by splitting compounds in GermanTranslating from a morphology-poor to a morphology-rich language is especially challenging since detailed morphological information needs to be decoded from a language that does not encode this information or does so only implicitly ( )Koehn ( ) includes a survey of statistical MT systems in both directions for the Europarl corpus , and points out the challenges of this taskFor example , it has been shown ( ) that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment , thus leading to improved overall translation qualityFor Arabic , we apply the following heuristic: use the most frequent analysis estimated from the gold standard labels in the Arabic Treebank ( ); if a word does not appear in the treebank , we choose the first analysis returned by the Buckwalter analyzerOur learning framework uses a Maximum Entropy Markov model ( )( ) demonstrated that a similar level of alignment quality can be achieved with smaller corpora applying morpho-syntactic source restructuring , using hierarchical lexicon models , in translating from German into EnglishThe sentence pairs were word-aligned using GIZA++ ( ) and submitted to a treelet-based MT system ( ) , which uses the word dependency structure of the source language and projects word dependency structure to the target language , creating the structure shown in Figure 1 above( ) experimented successfully with translating from inflectional languages into English making use of POS tags , word stems and suffixes in the source languageThe framework suggested here is most closely related to ( ) , which uses a probabilistic model to generate Japanese case markers for English-to-Japanese MTThe algorithm is similar to the one described in ( )It also differs from now traditional uses of comparable corpora for detecting translation equivalents ( ) or extracting terminology ( ) , which allows a one-to-one correspondence irrespective of the contextIn the spirit of ( ) , it is intended as a translator's amenuensis \"under the tight control of a human translator ..It has been aligned on the sentence level by JAPA ( ) , and further on the word level by GIZA++ ( )Thus the present system is unlike SMT ( ) , where lexical selection is effected by a translation model based on aligned , parallel corpora , but the novel techniques it has developed are exploitable in the SMT paradigmSimilarity is measured as the cosine between collocation vectors , whose dimensionality is reduced by SVD using the implementation by Rapp ( )We have generalised the method used in our previous study ( ) for extracting equivalents for continuous multiword expressions (MWEs)Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality , especially the phrase-based models ( ) and syntax-based models ( )By adapting the k-best parsing Algorithm 2 of Huang and Chiang ( ) , it achieves significant speed-up over full-integration on Chiang's Hiero system� We also devise a faster variant of cube pruning , called cube growing , which uses a lazy version of k-best parsing ( ) that tries to reduce k to the minimum needed at each node to obtain the desired number of hypotheses at the rootIn a nutshell , cube pruning works on the �LM forest , keeping at most k +LM items at each node , and uses the k-best parsing Algorithm 2 of Huang and Chiang ( ) to speed up the computationThis situation is very similar to k-best parsing and we can adapt the Algorithm 2 of Huang and Chiang ( ) here to explore this grid in a best-first orderThis new method , called cube growing , is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang ( ) , is a lazy version of Algorithm 2 (see Table 1)The different target sides then constitute a third dimension of the grid , forming a cube of possible combinations ( )The data set is same as in Section 5.1 , except that we also parsed the English-side using a variant of the Collins ( ) parser , and then extracted 24.7M tree-to-string rules using the algorithm of ( )These forest rescoring algorithms have potential applications to other computationally intensive tasks involving combinations of different models , for example , head-lexicalized parsing ( ); joint parsing and semantic role labeling ( ); or tagging and parsing with nonlocal featuresIn tree-to-string (also called syntax-directed) decoding ( ) , the source string is first parsed into a tree , which is then recursively converted into a target string according to transfer rules in a synchronous grammar ( )We generalize cube pruning and adapt it to two systems very different from Hiero: a phrase-based system similar to Pharaoh ( ) and a tree-to-string system ( )We test our methods on two large-scale English-to-Chinese translation systems: a phrase-based system and our tree-to-string system ( )Our data preparation follows Huang et al. ( ): the training data is a parallel corpus of28.3M words on the English side , and a trigram language model is trained on the Chinese sideFor cube growing , we use a non-duplicate k-best method ( ) to get 100-best unique translations according to � LM to estimate the lower-bound heuristicsSince our tree-to-string rules may have many variables , we first binarize each hyperedge in the forest on the target projection ( )Thus we envision forest rescoring as being of general applicability for reducing complicated search spaces , as an alternative to simulated annealing methods ( )Part of the complexity arises from the expressive power of the translation model: for example , a phrase- or word-based model with full reordering has exponential complexity ( )We will use the following example from Chinese to English for both systems described in this section: yU  Shalong juxing le huitan with Sharon hold   [past] meeting 'held a meeting with Sharon' A typical phrase-based decoder generates partial target-language outputs in left-to-right order in the form of hypotheses ( )We implemented Cubit , a Python clone of the Pharaoh decoder ( ) , 3 and adapted cube pruning to it as followsWe set the decoder phrase-table limit to 100 as suggested in ( ) and the distortion limit to 4An SCFG ( ) is a context-free rewriting system for generating string pairsTo integrate with a bigram language model , we can use the dynamic-programming algorithms of Och and Ney ( ) and Wu ( ) for phrase-based and SCFG-based systems , respectively , which we may think of as doing a iner-grained version of the deductions aboveThe language model also , if fully integrated into the decoder , introduces an expensive overhead for maintaining target-language boundary words for dynamic programming ( )Similarly , the decoding problem with SCFGs can also be cast as a deductive (parsing) system ( )However , the hope is that by choosing the right value ofi , these estimates will be accurate enough to affect the search quality only slightly , which is analogous to \"almost admissible\" heuristics in A* search ( )A few exceptions are the hierarchical (possibly syntax-based) trans-duction models ( ) and the string transduction models ( )The SFST approach described here is similar to the one described in ( ) which has subsequently been adopted by ( )In preliminary experiments , we have associated the target lexical items with supertag information ( )We separate the most popular classification techniques into two broad categories: also called Maxent as it finds the distribution with maximum entropy that properly estimates the average of each feature over the training data ( )Most of the previous work on statistical machine translation , as exemplified in ( ) , employs word-alignment algorithm (such as GIZA++ ( )) that provides local associations between source and target wordsThe BOW approach is different from the parsing based approaches ( ) where the translation model tightly couples the syntactic and lexical items of the two languagesThe excellent results recently obtained with the SEARN algorithm ( ) also suggest that binary classifiers , when properly trained and combined , seem to be capable ofmatching more complex structured output approachesA new L1-regularized Maxent algorithms was proposed for density estimation ( ) and we adapted it to classificationFrom the bilanguage corpus B , we train an n-gram language model using standard tools ( )The use of supertags in phrase-based SMT system has been shown to improve results ( )here: all state hypotheses of a whole sentence are kept in memory) , it is necessary to either use heuristic forward pruning or constrain permutations to be within a local window of adjustable size (also see ( ))Although Conditional Random Fields (CRF) ( ) train an exponential model at the sequence level , in translation tasks such as ours the computational requirements of training such models are prohibitively expensiveWe found this algorithm to converge faster than the current state-of-the-art in Maxent training , which is L2-regularized L-BFGS ( ) 1Discriminative training has been used mainly for translation model combination ( ) and with the exception of ( ) , has not been used to directly train parameters of a translation modelFor the work reported in this paper , we have used the GIZA++ tool ( ) which implements a string-alignment algorithmEach output label t is projected into a bit string with components b j (t) where probability of each component is estimated independently: In practice , despite the approximation , the 1-vs-other scheme has been shown to perform as well as the multiclass scheme ( )For the Hansard corpus we used the same training and test split as in ( ): 1.4 million training sentence pairs and 5432 test sentencesIn search of a balance between structural flexibility and computational complexity , several authors have proposed constraints to identify classes of non-projec-tive dependency structures that are computationally well-behaved ( )This result generalizes previous work on the relation between ltag and dependency representations ( )? The encoding of dependency structures as order-annotated trees allows us to reformulate two constraints on non-projectivity originally defined on fully specified dependency structures ( ) in terms of syntactic properties of the order annotations that they induce: Gap-degree The gap-degree of a dependency structure is the maximum over the number of discontinuities in any yield of that structureThis enables us to generalize a previous result on the class of dependency structures generated by lexicalized tags ( ) to the class of generated dependency languages , LTALLately , they have also been used in many computational tasks , such as relation extraction ( ) , parsing ( ) , and machine translation ( )Unfortunately , most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar ( ) , parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted ( )We also show that adding the well-nestedness condition corresponds to the restriction of lcfrs to Coupled Context-Free Grammars ( ) , and that regular sets of well-nested structures with a gap-degree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoining Grammar (ltag)This restriction is central to the formalism of Coupled-Context-Free Grammar (ccfg) ( )REGD w�(k) = LCCFL(k + 1) As a special case , Coupled-Context-Free Grammars with fan-out 2 are equivalent to Tree Adjoining Grammars (tags) ( )This gives rise to a notion of regular dependency languages , and allows us to establish a formal relation between the structural constraints and mildly context-sensitive grammar formalisms ( ): We show that regular dependency languages correspond to the sets of derivations of lexicalized Linear Context-Free Rewriting Systems (lcfrs) ( ) , and that the gap-degree measure is the structural correspondent of the concept of 'fan-out' in this formalism ( )Such a comparison may be empirically more adequate than one based on traditional notions of generative capacity ( )Both constraints have been shown to be in very good fit with data from dependency treebanks ( )A dependency structure is projective , if each of its yields forms an interval with respect to the precedence order ( )Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another ( ) , but becomes intractable when this assumption is abandoned ( )The number of components in the order-annotation , and hence , the gap-degree of the resulting dependency language , corresponds to the fan-out of the function: the highest number of components among the arguments of the function ( )Linear Context-Free Rewriting Systems Gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (lcfrs) ( ) , a class of formal systems that generalizes several mildly context-sensitive grammar formalismsThe Unfold-Fold transformation is a calculus for transforming functional and logic programs into equivalent but (hopefully) faster programs ( )Standard methods for converting weighted CFGs to equivalent PCFGs can be used if required ( )Second , Eisner-Satta O(n 3) PBDG parsing algorithms are extremely fast ( )It is straight-forward to extend the split-head CFG to encode the additional state information required by the head automata of Eisner and Satta ( ); this corresponds to splitting the non-terminals L u and uRThe O(n 3) split-head grammar is closely related to the O(n 3) PBDG parsing algorithm given by Eisner and Satta ( )Goodman ( ) observed that the Viterbi parse is in general not the optimal parse for evaluation metrics such as f-score that are based on the number of correct constituents in a parseFor example , incremental CFG parsing algorithms can be used with the CFGs produced by this transform , as can the Inside-Outside estimation algorithm ( ) and more exotic methods such as estimating adjoined hidden states ( )The closest related work we are aware of is McAllester ( ) , which also describes a reduction of PBDGs to efficiently-parsable CFGs and directly inspired this workThis paper investigates the relationship between Context-Free Grammar (CFG) parsing and the Eis-ner/Satta PBDG parsing algorithms , including their extension to second-order PBDG parsing ( )First , because they capture bilexical head-to-head dependencies they are capable of producing extremely high-quality parses: state-of-the-art dis-criminatively trained PBDG parsers rival the accuracy of the very best statistical parsers available today ( )The steps involved in CKY parsing with this grammar correspond closely to those of the McDonald ( ) second-order PBDG parsing algorithmThese weights are estimated by an online procedure as in McDonald ( ) , and are not intended to define a probability distributionWe provided one grammar which captures horizontal second-order dependencies ( ) , and another which captures vertical second-order head-to-head-to-head dependenciesSince CFGs can be expressed as Horn-clause logic programs ( ) and the Unfold-Fold transformation is provably correct for such programs ( ) , it follows that its application to CFGs is provably correct as wellSpecifically , we show how to use an off-line preprocessing step , the Unfold-Fold transformation , to transform a PBDG into an equivalent CFG that can be parsed in O(n 3) time using a version of the CKY algorithm with suitable indexing ( ) , and extend this transformation so that it captures second-order PBDG dependencies as wellBy a slight generalization of a result by Aoto ( ) , this typing r h N' : a must be negatively non-duplicated in the sense that each atomic type has at most one negative occurrence in itBy Aoto and Ono's ( ) generalization of the Coherence Theorem ( ) , it follows that every /-term P such that r' h P : a for some r' c r must be Sr-equal to N' (and consequently to N)The reduction to Datalog makes it possible to apply to parsing and generation sophisticated evaluation techniques for Datalog queries; in particular , an application of generalized supplementary magic-sets rewriting ( ) automatically yields Earley-style algorithms for both parsing and generation(In the case of an IO macro grammar , the result is an IO context-free tree grammar ( ).) String copying becomes tree copying , and the resulting grammar can be represented by an almost linear CFLG and hence by a Datalog programWith regard to parsing and recognition of input strings , polynomial-time algorithms and the LOGCFL upper bound on the computational complexity are already known for the grammar formalisms covered by our results ( ); nevertheless , we believe that our reduction to Datalog offers valuable insightsIn this paper , we show that a similar reduction to Datalog is possible for more powerful grammar formalisms with \"context-free\" derivations , such as (multi-component) tree-adjoining grammars ( ) , IO macro grammars ( ) , and (parallel) multiple context-free grammars ( )By the main result of Gottlob et al. ( ) , the related search problem of finding one derivation tree for the input /-term is in functional LOGCFL , i.e. , the class of functions that can be computed by a logspace-bounded Turing machine with a LOGCFL oracleOur method essentially relies on the encoding of different formalisms in terms of abstract categorial grammars ( )What we have called a context-free / -term grammar is nothing but an alternative notation for an abstract categorial grammar ( ) whose abstract vocabulary is second-order , with the restriction to linear /-terms removedA string-generating grammar coupled with Montague semantics may be represented by a synchronous CFLG , a pair of CFLGs with matching rule sets ( )linear ACGs are known to be expressive enough to encode well-known mildly context-sensitive grammar formalisms in a straightforward way , including TAGs and multiple context-free grammars ( )For example , the linear CFLG in Figure 8 is an encoding of the TAG in Figure 3 , where a(S) = o�>o and a(A) = (o �> o) �> o � > o ( )We can eliminate e-rules from an almost linear CFLG by the same method that Kanazawa and Yoshinaka ( ) used for linear grammars , noting that for any r and a , there are only finitely many almost linear /-terms M such that r h M : a.If a grammar has no e-rule , any derivation tree for the input /-term N that has a /-term P at its root node corresponds to a Datalog derivation tree whose number of leaves is equal to the number of occurrences of constants in P , which cannot exceed the number of occurrences of constants in NFor such P and D , it is known that {(D , q) | D eD , P U D derives q } is in the complexity class LOGCFL ( )3 In the linear case , Salvati ( ) has shown the recognition/parsing complexity to be PTIME , and exhibited an algorithm similar to Earley parsing for TAGsThe result of the generalized supplementary magic-sets rewriting of Beeri and Ramakrish-nan ( ) applied to the Datalog program representing a CFG essentially coincides with the deduction system ( ) or uninstantiated parsing system ( ) for Earley parsingBy naive (or seminaive) bottom-up evaluation ( ) , the answer to such a query can be computed in polynomial time in the size of the database for any Datalog programWe illustrate this approach with the program in Figure 4 , following the presentation of Ullman ( )But there are also other factors involved - for example , the tendency to put \"given\" discourse elements before \"new\" ones , which has been shown to play a role independent of length ( )First , how close is dependency length in English to that of this optimal DLA? Secondly , how similar is the optimal DLA to English in terms of the actual rules that arise? Finding linear arrangements of graphs that minimize total edge length is a classic problem , NP-complete for general graphs but with an O(n 16) algorithm for trees ( )Statistical parsers make use of features that capture dependency length (e.g.an adjacency feature in Collins ( ) , more explicit length features in McDonald et al. ( ) and Eisner and Smith ( )) and thus learn to favor parses with shorter dependenciesWe take sentences from the Wall Street Journal section of the Penn Treebank , extract the dependency trees using the head-word rules of Collins ( ) , consider them to be unordered dependency trees , and linearize them to minimize dependency lengthExactly this pattern has been observed by Dryer ( ) in natural languagesFrazier ( ) suggests that this might serve the function of keeping heads and dependents close togetherThis has been offered as an explanation for numerous psycholinguistic phenomena , such as the greater processing difficulty of object relative clauses versus subject relative clauses ( )Hawkins ( ) has shown that this principle is reflected in grammatical rules across many languagesOne might suppose that such syntactic choices in English are guided at least partly by dependency length minimization , and indeed there is evidence for this; for example , people tend to put the shorter of two PPs closer to the verb ( )The problem of finding the optimum weighted DLA for a set of input trees can be shown to be NP-complete by reducing from the problem of finding a graph's minimum Feedback Arc Set , one of the 21 classic problems of Karp ( )This setting is reminiscent of the problem of optimizing feature weights for reranking of candidate machine translation outputs , and we employ an optimization technique similar to that used by Och ( ) for machine translationIn particular , our approach would be applicable to corpora with frame-specific role labels , e.g.FrameNet ( )Our work suggests that feature generalization based on verb-similarity may compliment approaches to generalization based on role-similarity ( )For this task we utilized the August 2005 release of the Charniak parser with the default speed/accuracy settings ( ) , which required roughly 360 hours of processor time on a 2.5 GHz PowerPC G5To automatically identify all verb inflections , we utilized the English DELA electronic dictionary ( ) , which contained all but 21 of the PropBank verbs (for which we provided the inflections ourselves) , with old-English verb inflections removedParse tree paths were used for semantic role labeling by Gildea and Jurafsky ( ) as descriptive features of the syntactic relationship between predicates and their arguments in the parse tree of a sentenceIn future work , it would be particularly interesting to compare empirically-derived verb clusters to verb classes derived from theoretical considerations ( ) , and to the automated verb classification techniques that use these classes ( )Our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information ( )This observation further supports the distributional hypothesis of word similarity and corresponding technologies for identifying synonyms by similarity of lexical-syntactic context ( )In our work , we utilized the GigaWord corpus of English newswire text ( ) , consisting of nearly 12 gigabytes of textual dataAnnotations similar to these have been used to create automated semantic role labeling systems ( ) for use in natural language processing applications that require only shallow semantic parsingThe overall performance of our semantic role labeling approach is not competitive with leading contemporary systems , which typically employ support vector machine learning algorithms with syntactic features ( ) or syntactic tree kernels ( )A recent release of the PropBank ( ) corpus of semantic role annotations of Tree-bank parses contained 112 ,917 labeled instances of 4 ,250 rolesets corresponding to 3 ,257 verbs , as illustrated by this example for the verb buyAn important area for future research will be to explore the correlation between our distance metric for syntactic similarity and various quantitative measures of semantic similarity ( )To prepare this corpus for analysis , we extracted the body text from each of the 4.1 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries ( )Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky ( ) , who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet ( )SVM ( ) is selected as our classifier and the one vsIn the context of it , more and more kernels for restricted syntaxes or specific domains ( ) are proposed and explored in the NLP domainIn this paper , we apply Alternating Structure Optimization (ASO) ( ) to the semantic role labeling task on NomBankASO has been shown to be effective on the following natural language processing tasks: text categorization , named entity recognition , part-of-speech tagging , and word sense disambiguation ( )For a more complete description , see ( )In this work , we use a modification of Huber's robust loss function , similar to that used in ( ): L(p , y) �4py if py < �1 (1 � py) 2 if � 1 py< 1 (2) 0 if py > 1 We fix the regularization parameter A to 10 -4 , similar to that used in ( )This relationship is modeled by + 6 Tvi (3) The parameters [{w l , vi} , 6] may then be found by joint empirical risk minimization over all the m problems , i.e. , their values should minimize the combined empirical risk: l=i  V   i=l J (4) An important observation in ( ) is that the binary classification problems used to derive 6 are not necessarily those problems we are aiming to solveAssuming there are k target problems and m auxiliary problems , it is shown in ( ) that by performing one round of minimization , an approximate solution of 6 can be obtained from (4) by the following algorithm: 1. For each of the m auxiliary problems , learn u l as described by (1)This is a simplified version of the definition in ( ) , made possible because the same A is used for all auxiliary problemsASO has been demonstrated to be an effective semi-supervised learning algorithm ( )A variety of auxiliary problems are tested in ( ) in the semi-supervised settings , i.e. , their auxiliary problems are generated from unlabeled dataMore recently , for the word sense disambiguation (WSD) task , ( ) experimented with both supervised and semi-supervised auxiliary problems , although the auxiliary problems she used are different from oursIn recent years , the availability of large human-labeled corpora such as PropBank ( ) and FrameNet ( ) has made possible a statistical approach of identifying and classifying the arguments of verbs in natural language textsThis is known as multi-task learning in the machine learning literature ( )A large number of SRL systems have been evaluated and compared on the standard data set in the CoNLL shared tasks ( ) , and many systems have performed reasonably well In addition to the target outputs , ( ) discusses configurations where both used inputs and unused inputs (due to excessive noise) are utilized as additional outputsFirst , we train the various classifiers on sections 2 to 21 using gold argument labels and automatic parse trees produced by Charniak's re-ranking parser ( ) , and test them on section 23 with automatic parse treesNoun predicates also appear in FrameNet semantic role labeling ( ) , and many FrameNet SRL systems are evaluated in Senseval-3 ( )So far we are aware of only one English NomBank-based SRL system ( ) , which uses the maximum entropy classifier , although similar efforts are reported on the Chinese NomBank by ( ) and on FrameNet by ( ) using a small set of hand-selected nominalizationsSecond , we achieve accuracy higher than that reported in ( ) and advance the state of the art in SRL researchEighteen baseline features and six additional features are proposed in ( ) for NomBank argument identificationUnlike in ( ) , we do not prune arguments dominated by other arguments or those that overlap with the predicate in the training dataThe J&N column presents the result reported in ( ) using both baseline and additional featuresA diverse set of 28 features is used in ( ) for argument classificationTo find a smaller set of effective features , we start with all the features considered in ( ) , in ( ) , and various combinations of them , for a total of 52 featuresThe J&N column presents the result reported in ( )This is the same configuration as reported in ( )Table 3: Fl scores of various classifiers on NomBank SRL Our maximum entropy classifier consistently outperforms ( ) , which also uses a maximum entropy classifierOur results outperform those reported in ( )With the recent release of NomBank ( ) , it becomes possible to apply machine learning techniques to the taskAccordingly , we do not maximize the probability of the entire labeled parse tree as in ( )Some approaches have used WordNet for the generalization step ( ) , others EM-based clustering ( )The argument positions for which we compute selec-tional preferences will be semantic roles in the FrameNet ( ) paradigm , and the predicates we consider will be semantic classes of words rather than individual words (which means that different preferences will be learned for different senses of a predicate word)We use FrameNet ( ) , a semantic lexicon for English that groups words in semantic classes called frames and lists semantic roles for each frameBrockmann and Lapata ( ) perform a comparison of WordNet-based modelsThe sim function can equally well be in-c)stantiated with a WordNet-based metric (for an overview see Budanitsky and Hirst ( )) , but we restrict our experiments to corpus-based metrics (a) in the interest of greatest possible sim cosine(w , w')   =    ,-      p   �  , =    sim Dice(w ,w')   =  ip/  Miiip)  a V  '    7 ^/E r p  f(w ,r p ) 2YE r p  f (w' ,r p )  2   Dic ^ V  '    7  \\  R(w) \\  +  \\  R(w' ) \\ sim  Lj n(w ,w') = ^�  p�  �n� - tt  � i �v  sim i accarH(w ,w') =   p  ) \\i  ,�  a slm nindie(w ,w /)   =    r  p sim Hindie(w , w' ,r p) where sim Hindle(w , w' , r p)  =   ^   abs(max(1 (w ,r p ) ,1 (w' ,r p)))   if I(w ,r p) <  0 and I (w' ,rp) <  0 Table 1: Similarity measures used resource-independence and (b) in order to be able to shape the similarity metric by the choice of generalization corpusIn SRL , the two most pressing issues today are (1) the development of strong semantic features to complement the current mostly syntactically-based systems , and (2) the problem of the domain dependence ( )The preference that r p has for a given synset co , the selectional association between the two , is then defined as the contribution of c 0 to r p's selectional preference strength: A(rp ,C 0) = P (C 0|r p)log  ^gf* S (rp Further WordNet-based approaches to selec-tional preference induction include Clark and Weir ( ) , and Abe and Li ( )To determine headwords of the semantic roles , the corpus was parsed using the Collins ( ) parser5x2cv ( )They have been used for example for syntactic disambiguation ( ) , word sense disambiguation (WSD) ( ) and semantic role labeling (SRL) ( )While EM-based models have been shown to work better in SRL tasks ( ) , this has been attributed to the difference in coverageWe will be using the similarity metrics shown in Table 1: Cosine , the Dice and Jaccard coefficients , and Hindle's ( ) and Lin's ( ) mutual information-based metricsSelectional restrictions and selectional preferences that predicates impose on their arguments have long been used in semantic theories , (see e.g. ( ))It was parsed using Minipar ( ) , which is considerably faster than the Collins parser but failed to parse about a third of all sentencesIn this paper we propose a new , simple model for selectional preference induction that uses corpus-based semantic similarity metrics , such as Cosine or Lin's ( ) mutual information-based metric , for the generalization stepThe corpus-based induction of selectional preferences was first proposed by Resnik ( )The induction of selectional preferences from corpus data was pioneered by Resnik ( )Rooth et al. ( ) generalize over seen headwords using EM-based clustering rather than WordNetExperimental design Like Rooth et al. ( ) we evaluate selectional preference induction approaches in a pseudo-disambiguation taskThe intuition that \"hard to learn\" examples are suspect corpus errors is not new , and appears also in Abney et al. ( ) , who consider the \"heaviest\" samples in the final distribution of the AdaBoost algorithm to be the hardest to classify and thus likely corpus errorsThe HEB  Err version of the corpus is obtained by projecting the chunk boundaries on the sequence of PoS and morphology tags obtained by the automatic PoS tagger of Adler & Elhadad ( )We tested this hypothesis by training the Error-Driven Pruning (EDP) method of ( ) with an extended set of featuresIn ( ) , we established that the task is not trivially transferable to Hebrew , but reported that SVM based chunking ( ) performs wellIn ( ) we argued that it is not applicable to Hebrew , mainly because of the prevalence of the Hebrew's construct state (smixut)For the Hebrew experiments , we use the corpora of ( )These are the same settings as in ( )Refining the SimpleNP Definition: The hard cases analysis identified examples that challenge the SimpleNP definition proposed in Goldberg et al. ( )Kudo and Matsumoto ( ) used SVM as a classification engine and achieved an F-Score of 93.79 on the shared task NPsFurther details can be found in Kudo and Matsumoto ( )Following Ramshaw and Marcus ( ) , the current dominant approach is formulating chunking as a classification task , in which each word is classified as the (B)eginning , (I)nside or (O)outside of a chunkNP chunks in the shared task data are BaseNPs , which are non-recursive NPs , a definition first proposed by Ramshaw and Marcus ( )For the English experiments , we use the now-standard training and test sets that were introduced in ( ) 2This method is similar to the corpus error detection method presented by Nakagawa and Matsumoto ( )It is a well studied problem in English , and was the focus of CoNLL2000's Shared Task ( )We applied this definition to the Hebrew Tree Bank ( ) , and constructed a moderate size corpus (about 5 ,000 sentences) for Hebrew SimpleNP chunkingSVM ( ) is a supervised binary classifierHowever , each of these assumes that the relations themselves are known in advance (implicitly or explicitly) so that the method can be provided with seed patterns ( ) , pattern-based rules ( ) , relation keywords ( ) , or word pairs exemplifying relation instances ( )Most related work deals with discovery of hypernymy ( ) , synonymy ( ) and meronymy ( )In addition to these basic types , several studies deal with the discovery and labeling of more specific relation sub-types , including inter-verb relations ( ) and noun-compound relationships ( )It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing ( ) and named entity tagging ( ) , while others take advantage of handcrafted databases such as WordNet ( ) and Wikipedia ( )In several studies ( ) it has been shown that relatively unsupervised and language-independent methods could be used to generate many thousands of sets of words whose semantics is similar in some senseWe do this as follows , essentially implementing a simplified version of the method of Davidov and Rappoport ( )Note that our method differs from that of Davidov and Rappoport ( ) in that here we provide an initial seed pair , representing our target concept , while there the goal is grouping of as many words as possible into concept classesIt was shown in ( ) that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is , they share some notable aspect of their semantics)Studying relationships between tagged named entities , ( ) proposed unsupervised clustering methods that assign given (or semi-automatically extracted) sets of pairs into several clusters , where each cluster corresponds to one of a known relationship typeA lot of this research is based on the initial insight ( ) that certain lexical patterns ('X is a country') can be exploited to automatically generate hyponyms of a specified wordIn some recent work ( ) , it has been shown that related pairs can be generated without pre-specifying the nature of the relation soughtFinally , ( ) provided a pattern distance measure which allows a fully unsupervised measurement of relational similarity between two pairs of words; however , relationship types were not discovered explicitlyThe bracketing guidelines ( ) also mention the considerable difficulty of identifying the correct scope for nominal modifiersWe use Bikel's implementation ( ) of Collins' parser ( ) in order to carry out these experiments , using the non-deficient Collins settingsWe draw our counts from a corpus of n-gram counts calculated over 1 trillion words from the web ( )We use the Briscoe and Carroll ( ) version of DepBank , a 560 sentence subset used to evaluate the rasp parserWe map the brackets to dependencies by finding the head of the np , using the Collins ( ) head finding rules , and then creating a dependency between each other child's head and this headWe dis-cretised the non-binary features using an implementation of Fayyad and Irani's ( ) algorithm , and classify using MegaM 2For instance , CCGbank ( ) was created by semi-automatically converting the Treebank phrase structure to Combinatory Categorial Grammar (ccg) ( ) derivationsAn additional grammar rule is needed just to get a parse , but it is still not correct (Hockenmaier , 2003 , pWe check the correctness of the corpus by measuring inter-annotator agreement , by reannotating the first section , and by comparing against the sub -NP structure in DepBank ( )We used the PARC700 Dependency Bank ( ) which consists of 700 Section 23 sentences annotated with labelled dependenciesOur annotation guidelines 1 are based on those developed for annotating full sub -np structure in the biomedical domain ( )Lapata and Keller ( ) derive estimates from web counts , and only compare at a lexical level , achieving 78.7% accuracyFinally , we test the utility of the extended Treebank for training statistical models on two tasks: NP bracketing ( ) and full parsing ( )Lauer ( ) has demonstrated superior performance of the dependency model using a test set of 244 (216 unique) noun compounds drawn from Grolier's encyclopediaWe implement a similar system to Table 4: Comparison of NP bracketing corpora Table 5: Lexical overlap Lauer ( ) , described in Section 3 , and report on results from our own data and Lauer's original setThe np bracketing task has often been posed in terms of choosing between the left or right branching structure of three word noun compounds: (a)  (world (oil prices)) - Right-branching (b)  ((crude oil) prices) - Left-branching Most approaches to the problem use unsupervised methods , based on competing association strength between two of the words in the compound (Marcus , 1980 , pThe Penn Treebank ( ) is perhaps the most influential resource in Natural Language Processing (NLP)According to Marcus et al. ( ) , asking annota-tors to markup base -np structure significantly reduced annotation speed , and for this reason base- nps were left flatFor the original bracketing of the Treebank , anno-tators performed at 375-475 words per hour after a Table 1: Agreement between annotators few weeks , and increased to about 1000 words per hour after gaining more experience ( )Nakov and Hearst ( ) also use web counts , but incorporate additional counts from several variations on simple bigram queries , including queries for the pairs of words concatenated or joined by a hyphenWith our new data set , we began running experiments similar to those carried out in the literature ( )Many approaches to identifying base noun phrases have been explored as part of chunking ( ) , but determining sub -np structure is rarely addressedThe bracketing tool often suggests a bracketing using rules based mostly on named entity tags , which are drawn from the bbn corpus ( )The most common form of parser evaluation is to apply the parseval metrics to phrase-structure parsers based on the penn Treebank , and the highest reported scores are now over 90% ( )In this paper we evaluate a ccg parser ( ) on the Briscoe and Carroll version of DepBank ( )Briscoe and Carroll ( ) reannotated this resource using their grs scheme , and used it to evaluate the rasp parserParsers have been developed for a variety of grammar formalisms , for example hpsg ( ) , lfg ( ) , tag ( ) , ccg ( ) , and variants of phrase-structure grammar ( ) , including the phrase-structure grammar implicit in the Penn Treebank ( )And third , we provide the first evaluation of a wide-coverage ccg parser outside of CCGbank , obtaining impressive results on DepBank and outperforming the rasp parser ( ) by over 5% overall and on the majority of dependency typesFor the gold standard we chose the version of Dep-Bank reannotated by Briscoe and Carroll ( ) , consisting of 700 sentences from Section 23 of the Penn TreebankThe results in Table 4 were obtained by parsing the sentences from CCGbank corresponding to those in the 560-sentence test set used by Briscoe et al. ( )the macro-averaged scores are the mean of the individual scores for each relation ( )Can the ccg parser be compared with parsers other than rasp? Briscoe and Carroll ( ) give a rough comparison of rasp with the Parc lfg parser on the different versions of DepBank , obtaining similar results overall , but they acknowledge that the results are not strictly comparable because of the different annotation schemes usedBriscoe et al. ( ) split the 700 sentences in DepBank into a test and development set , but the latter only consists of 140 sentences which was not enough to reliably create the transformationAll the results were obtained using the RASP evaluation scripts , with the results for the rasp parser taken from Briscoe et al. ( )Preiss ( ) compares the parsers of Collins ( ) and Charniak ( ) , the gr finder of Buchholz et al. ( ) , and the rasp parser , using the Carroll et al. ( ) gold-standardIt has been argued that the parseval metrics are too forgiving and that phrase structure is not the ideal representation for a gold standard ( )Carroll et al. ( ) describe such a suite , consisting of sentences taken from the Susanne corpus , annotated with Grammatical Relations (grs) which specify the syntactic relation between a head and dependentWe chose not to use the corpus based on the Susanne corpus ( ) because the grs are less like the ccg dependencies; the corpus is not based on the Penn Treebank , making comparison more difficult because of tokenisation differences , for example; and the latest results for rasp are on DepBankparser evaluation has improved on the original parseval measures ( ) , but the challenge remains to develop a representation and evaluation suite which can be easily applied to a wide variety of parsers and formalismsClark and Curran ( ) describes the ccg parser used for the evaluationPrevious evaluations of ccg parsers have used the predicate-argument dependencies from CCGbank as a test set ( ) , with impressive results of over 84% F-score on labelled dependenciesKaplan et al. ( ) compare the Collins ( ) parser with the Parc lfg parser by mapping lfg F-structures and Penn Treebank parses into DepBank dependencies , claiming that the lfg parser is considerably more accurate with only a slight reduction in speedThe ccg parser results are based on automatically assigned pos tags , using the Curran and Clark ( ) taggerAn example of this is from CCGbank ( ) , where all modifiers in noun-noun compound constructions modify the final noun (because the penn Treebank , from which CCGbank is derived , does not contain the necessary information to obtain the correct bracketing)The grammar used by the parser is extracted from CCGbank , a ccg version of the Penn Treebank ( )Such conversions have been performed for other parsers , including parsers producing phrase structure output ( )Kaplan et al. ( ) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies , but they also note that \"This conversion was relatively straightforward for LFG structures ..In the case of Kaplan et al. ( ) , the testing procedure would include running their conversion process on Section 23 of the Penn Treebank and evaluating the output against DepBankA similar resource � the Parc Dependency Bank (DepBank) ( ) � has been created using sentences from the Penn TreebankThe b&c scheme is similar to the original DepBank scheme ( ) , but overall contains less grammatical detail; Briscoe and Carroll ( ) describes the differencesDifferent parsers produce different output , for ex-ample phrase structure trees ( ) , dependency trees ( ) , grammatical relations ( ) , and formalism-specific dependencies ( )The grammar consists of 425 lexical categories � expressing subcategorisation information � plus a small number of combinatory rules which combine the categories ( )A more interesting statement would be that it makes learning easier , along the lines of the result of ( ) � note , however , that their results are for the \"semi-supervised\" domain adaptation problem and so do not apply directlyA part-of-speech tagging problem on PubMed abstracts introduced by Blitzer et al. ( )The first model , which we shall refer to as the Prior model , was first introduced by Chelba and Acero ( )This is a recapitalization task introduced by Chelba and Acero ( ) and also used by Daume III and Marcu ( )For the CNN-Recap task , we use identical feature to those used by both Chelba and Acero ( ) and Daume III and Marcu ( ): the current , previous and next word , and 1-3 letter prefixes and suffixesMany of these are presented and evaluated by Daume III and Marcu ( )Daume III and Marcu ( ) provide empirical evidence on four datasets that the Prior model outperforms the baseline approachesMore recently , Daume III and Marcu ( ) presented an algorithm for domain adaptation for maximum entropy classifiersWe additionally ran the MegaM model ( ) on these data (though not in the multi-conditional case; for this , we considered the single source as the union of all sources)In all cases , we use the S earn algorithm for solving the sequence labeling problem ( ) with an underlying averaged perceptron classifier; implementation due to ( )Second , it is arguable that a measure like F 1 is inappropriate for chunking tasks ( )Following ( ) , we call the first the source domain , and the second the target domainRecently there have been some studies addressing domain adaptation from different perspectives ( )The POS data set and the CTS data set have previously been used for testing other adaptation methods ( ) , though the setup there is different from oursBlitzer et al. ( ) propose a domain adaptation method that uses the unlabeled target instances to infer a good feature representation , which can be regarded as weighting the featuresChelba and Acero ( ) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target dataThe setup is very similar to Daume III and Marcu ( )els the different distributions in the source and the target domains is by Daume III and Marcu ( )Florian et al. ( ) first train a NE tagger on the source domain , and then use the tagger's predictions as features for training and testing on the target domainThis way of setting 7 corresponds to the entropy minimization semi-supervised learning method ( )For generative syntactic parsing , Roark and Bac-chiani ( ) have used the source domain data to construct a Dirichlet prior for MAP estimation of the PCFG for the target domainber of hidden components is not fixed , but emerges We begin by presenting three finite tree models , each naturally from the training data ( )The closely related infinite hidden Markov model is an HMM in which the transitions are modeled using an HDP , enabling unsupervised learning of sequence models when the number of hidden states is unknown ( )The infinite hidden Markov model (iHMM) or HDP-HMM ( ) is a model of sequence data with transitions modeled by an HDPThis is useful , because coarse-grained syntactic categories , such as those used in the Penn Treebank (PTB) , make insufficient distinctions to be the basis of accurate syntactic parsing ( )Hence , state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves ( ) , manually split the tagset into a finer-grained one ( ) , or learn finer grained tag distinctions using a heuristic learning procedure ( )But the introduction of nonparametric priors such as the Dirichletprocess ( ) enabled development of infinite mixture models , in which the num-Teh et al. ( ) proposed the hierarchical Dirichlet process (HDP) as a way of applying the Dirichlet process (DP) to more complex model forms , so as to allow multiple , group-specific , infinite mixture models to share their mixture components8 Additionally , we compute the mutual information of the learned clusters with the gold tags , and we compute the cluster F-score ( )First , we use the standard approach of greedily assigning each of the learned classes to the POS tag with which it has the greatest overlap , and then computing tagging accuracy ( )For comparison , Haghighi and Klein ( ) report an unsupervised baseline of 41.3% , and a best result of 80.5% from using hand-labeled prototypes and distributional similarityEarlier , Johnson et al. ( ) presented adaptor grammars , which is a very similar model to the HDP-PCFGWe use the generative dependency parser distributed with the Stanford factored parser ( ) for the comparison , since it performs simultaneous tagging and parsing during testingThe HDP-PCFG ( ) , developed at the same time as this work , aims to learn state splits for a binary-branching PCFGIn contrast , Liang et al. ( ) define a global DP over sequences , with the base measure defined over the global state probabilities ,  0; locally , each state has an HDP , with this global DP as the base measureFor both experiments , we used dependency trees extracted from the Penn Treebank ( ) using the head rules and dependency extractor from Yamada and Matsumoto ( )To generate  n we first generate an infinite sequence of variables  n' = (n k each of which is distributed according to the Beta distribution: Then  n = (n k)� = = 1 is defined as: (1 Following Pitman ( ) we refer to this process as n � GEM (a 0)Teh , 2006 , p.c.) , to sample each m jk: sampleM (j , k) 1  if n jk = 0 2  then m jk = 0 3  else m jk = 1 4  for i ^ 2 to n jk 5  doifrand ()<^^T 6  then m jk = m jk + 1 7  return m jk Sampling /? In many cases , improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology ( )This follows a conceptually similar approach by ( ) that uses a large named-entity dictionary , where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classifierTherefore , an increasing attention has been recently given to semi-supervised learning , where large amounts of unlabeled data are used to improve the models learned from a small training set ( )This was used , for example , by ( ) in information extraction , and by ( ) in POS taggingThis decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs , in which case the linear sum corresponds to log likelihood assigned to the input/output pair by the model (for details see ( ) for the classification case and ( ) for the structured case)For example , ( ) proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labelsThe second problem we consider is extracting fields from advertisements ( )( ) and ( ) also report results for semi-supervised learning for these domains( ) extends the dictionary-based approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarityWe implement some global constraints and include unary constraints which were largely imported from the list of seed words used in ( )( ) also worked on one of our data sets1 The first task is to identify fields from citations ( ) Another way to look the algorithm is from the self-training perspective ( )However , in the general case , semi-supervised approaches give mixed results , and sometimes even degrade the model performance ( )( ) has suggested to balance the contribution of labeled and unlabeled data to the parameters( )This confirms results reported for the supervised learning case in ( )On the other hand , in the supervised setting , it has been shown that incorporating domain and problem specific structured information can result in substantial improvements ( )However ( ) showed that reasoning with more expressive , non-sequential constraints can improve the performance for the supervised protocolWe note that in the presence of constraints , the inference procedure (for finding the output y that maximizes the cost function) is usually done with search techniques (rather than Viterbi decoding , see ( ) for a discussion) , we chose beamsearch decodingWhile ( ) showed the significance of using hard constraints , our experiments show that using soft constraints is a superior optionConceptually , although not technically , the most related work to ours is ( ) that , in a somewhat ad-hoc manner uses soft constraints to guide an unsupervised model that was crafted for mention trackingCrucially , the kind of lexical descriptions that we employ are those that are commonly devised within lexicon-driven approaches to linguistic syntax , e.g.Lexicalized Tree-Adjoining Grammar ( ) and Combinary Categorial Grammar ( )There are currently two supertagging approaches available: LTAG-based ( ) and CCG-based ( )One important way of portraying such lexical descriptions is via the supertags devised in the LTAG and CCG frameworks ( )The term \"supertagging\" ( ) refers to tagging the words of a sentence , each with a supertagThe LTAG-based supertagger of ( ) is a standard HMM tagger and consists of a (second-order) Markov language model over supertags and a lexical model conditioning the probability of every word on its own supertag (just like standard HMM-based POS taggers)For the LTAG supertags experiments , we used the LTAG English supertagger 5 ( ) to tag the English part of the parallel data and the supertag language model dataAkin to POS tagging , the process of supertagging an input utterance proceeds with statistics that are based on the probability of a word-supertag pair given their Markovian or local context ( )Besides the difference in probabilities and statistical estimates , these two supertaggers differ in the way the supertags are extracted from the Penn Treebank , cf. ( )Only quite recently have ( ) and ( ) shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT systemAmong the first to demonstrate improvement when adding recursive structure was ( ) , who allows for hierarchical phrase probabilities that handle a range of reordering phenomena in the correct fashionThe CCG supertagger ( ) is based on log-linear probabilities that condition a supertag on features representing its contextFor the CCG supertag experiments , we used the CCG supertagger of ( ) and the Edinburgh CCG tools 6 to tag the English part of the parallel corpus as well as the CCG supertag language model dataBoth the LTAG ( ) and the CCG supertag sets ( ) were acquired from the WSJ section of the Penn-II Treebank using hand-built extraction rulesDecoder The decoder used in this work is Moses , a log-linear decoder similar to Pharaoh ( ) , modified to accommodate supertag phrase probabilities and supertag language modelsWithin the field of Machine Translation , by far the most dominant paradigm is Phrase-based Statistical Machine Translation (PBSMT) ( )For example , ( ) demonstrated that adding syntax actually harmed the quality of their SMT systemThe bidirectional word alignment is used to obtain phrase translation pairs using heuristics presented in ( ) and ( ) , and the Moses decoder was used for phrase extraction and decodingThe bidirectional word alignment is used to obtain lexical phrase translation pairs using heuristics presented in ( ) and ( )Coming right up to date , ( ) demonstrate that 'syntactified' target language phrases can improve translation quality for Chinese-EnglishWhile the research of ( ) has much in common with the approach proposed here (such as the syntactified target phrases) , there remain a number of significant differencesThe NIST MT03 test set is used for development , particularly for optimizing the interpolation weights using Minimum Error Rate training ( )Firstly , rather than induce millions of xRS rules from parallel data , we extract phrase pairs in the standard way ( ) and associate with each phrase-pair a set of target language syntactic structures based on supertag sequencesTable 1 presents the BLEU scores ( ) of both systems on the NIST 2005 MT Evaluation test setFor less commonly used languages , one might use open source research systems ( )Also relevant is previous work that applied machine learning approaches to MT evaluation , both with human references ( ) and without ( )METEOR uses the Porter stemmer and synonym-matching via WordNet to calculate recall and precision more accurately ( )As its loss function , support vector regression uses an e-insensitive error function , which allows for errors within a margin of a small positive value , e , to be considered as having zero error (cf.Bishop ( ) , pp.339-344)This can be seen as a form of confidence estimation on MT outputs ( )To remove the bias in the distributions of scores between different judges , we follow the normalization procedure described by Blatz et al. ( )We conducted experiments to determine the feasibility of the proposed approach and to address the following questions: (1) How informative are pseudo references in-and-of themselves? Does varying the number and/or the quality of the references have an impact on the metrics? (2) What are the contributions of the adequacy features versus the fluency features to the learning-based metric? (3) How do the quality and distribution of the training examples , together with the quality of the pseudo references , impact the metric training? (4) Do these factors impact the metric's ability in assessing sentences produced within a single MT system? How does that system's quality affect metric performance? The implementation of support vector regression used for these experiments is SVM-Light ( )To compare the relative quality of different metrics , we apply bootstrapping re-sampling on the data , and then use paired t-test to determine the statistical significance of the correlation differences ( )ROUGE utilizes 'skip n-grams' , which allow for matches of sequences of words that are not necessarily adjacent ( )BLEU is smoothed ( ) , and it considers only matching up to bigrams because this has higher correlations with human judgments than when higher-ordered n-grams are includedThe HWC metrics compare dependency and constituency trees for both reference and machine translations ( )In addition to adapting the idea of Head Word Chains ( ) , we also compared the input sentence's argument structures against the treebank for certain syntactic categoriesReference-based metrics such as BLEU ( ) have rephrased this subjective task as a somewhat more objective question: how closely does the translation resemble sentences that are known to be good translations for the same source? This approach requires the participation of human translators , who provide the \"gold standard\" reference sentencesThe relationship between word alignments and their impact on MT is also investigated in ( )Most current statistical models ( ) treat the aligned sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target wordsTo quickly (and approximately) evaluate this phenomenon , we trained the statistical IBM word-alignment model 4 ( ) , 1 using the GIZA++ software ( ) for the first two language pairs , and the Europarl corpus ( ) for the last oneThey can be seen as extensions of the simpler IBM models 1 and 2 ( )We use a standard log-linear phrase-based statistical machine translation system as a baseline: GIZA++ implementation of IBM word alignment model 4 ( ) , 8 the refinement and phrase-extraction heuristics described in ( ) , minimum-error-rate training Table 2: Chinese-English corpus statistics ( ) using Phramer ( ) , a 3-gram language model with Kneser-Ney smoothing trained with SRILM ( ) on the English side of the training data and Pharaoh ( ) with default settings to decodeWe also want to bootstrap on different word aligners; in particular , one possibility is to use the flexible HMM word-to-phrase model of Deng and Byrne ( ) in place of IBM model 4We evaluate the reliability of these candidates , using simple metrics based on co-occurence frequencies , similar to those used in associative approaches to word alignment ( )Second , an increase in AER does not necessarily imply an improvement in translation quality ( ) and vice-versa ( )This very simple measure is frequently used in associative approaches ( )%: there is want to need not I^iS: in front of �: as soon as ;#: look at Figure 2: Examples of entries from the manually developed dictionary The intrinsic quality of word alignment can be assessed using the Alignment Error Rate (AER) metric ( ) , that compares a system's alignment output to a set of gold-standard alignmentThe quality of the translation output is evaluated using BLEU ( )The experiments were carried out using the Chinese-English datasets provided within the IWSLT 2006 evaluation campaign ( ) , extracted from the Basic Travel Expression Corpus (BTEC) ( )For Chinese , the data provided were tokenized according to the output format of ASR systems , and human-corrected ( )Note that the need to consider segmentation and alignment at the same time is also mentioned in ( ) , and related issues are reported in ( )More importantly , however , this segmentation is often performed in a monolingual context , which makes the word alignment task more difficult since different languages may realize the same concept using varying numbers of words (see e.g. ( ))The log-linear model is also based on standard features: conditional probabilities and lexical smoothing ofphrases in both directions , and phrase penalty ( )To test the influence of the initial word segmentation on the process of word packing , we considered an additional segmentation configuration , based on an automatic segmenter combining rule-based and statistical techniques ( )These resources follow more or less the same format as the output of the word segmenter mentioned in Section 5.1.2 ( ) , so the experiments are carried out using this segmentationIt has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision ( )Recently , confusion network decoding for MT system combination has been proposed ( )Powell's method ( ) is used to tune the system and feature weights simultaneously so as to optimize various automatic evaluation metrics on a development setIn this work , modified Powell's method as proposed by ( ) is usedSix MT systems were combined: three (A ,C ,E) were phrase-based similar to ( ) , two (B ,D) were hierarchical similar to ( ) and one (F) was syntax-based similar to ( )Combination of speech recognition outputs is an example of this approach ( )Also , a more heuristic alignment method has been proposed in a different system combination approach ( )In speech recognition , confusion network decoding ( ) has become widely used in system combinationIn ( ) , different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ ( )Tuning is fully automatic , as opposed to ( ) where global system weights were set manuallySimilar combination of multiple confusion networks was presented in ( )The same Powell's method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in ( )The optimization of the system and feature weights may be carried out using -best lists as in ( )Currently , the most widely used automatic MT evaluation metric is the NIST BLEU-4 ( )This work was extended in ( ) by introducing system weights for word confidencesIn ( ) , simple score was assigned to the word coming from the th-best hypothesisIn ( ) , the total confidence of the nth best confusion network hypothesis  , including NULL words , given the th source sentence  was given by where is the number of nodes in the confusion network for the source sentence  , is the number of translation systems , is the th system weight ,  c wn is the accumulated confidence for word produced by system between nodes and  , and is a weight for the number of NULL links along the hypothesis The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in ( ) on the Arabic to English and Chinese to English NIST MT05 tasksCompared to the baseline from ( ) , the new method improves the BLEU scores significantlyIn ensemble learning , a collection of simple classifiers is used to yield better performance than any single classifier; for example boosting ( )A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) ( ) was used to align hy-potheses in ( )Minimum Bayes risk (MBR) was used to choose the skeleton in ( )This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities ( )It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output ( )Translation edit rate (TER) ( ) has been proposed as more intuitive evaluation metric since it is based on the rate of edits required to transform the hypothesis into the referenceHowever , this would require time consuming evaluations such as human mediated TER post-editing ( )The TnT tagger ( ) and the TreeTagger ( ) are used for tagging and lemmatizationMotivated by the theoretical work by Chafe ( ) and Jacobs ( ) , we view the VF as the place for elements which modify the situation described in the sentence , i.eFinally , the articles are parsed with the CDG dependency parser ( )The preferences summarized below have mo-tivated our choice of features: � constituents in the nominative case precede those in other cases , and dative constituents often precede those in the accusative case ( ); � the verb arguments' order depends on the verb's  subcategorization properties ( ); � constituents with a definite article precede those with an indefinite one ( ); � pronominalized constituents precede non-pronominalized ones ( ) ; � animate referents precede inanimate ones ( ); � short constituents precede longer ones ( ); � the preferred topic position is right after the verb ( ); � the initial position is usually occupied by scene-setting elements and topics ( ) The sentence-initial position , which in German is the VF , has been shown to be cognitively more prominent than other positions ( )Inspired by the findings of the Prague School ( ) and Systemic Functional Linguistics ( ) , they focus on the role that information structure plays in constituent orderingHarbusch et al. ( ) present a generation workbench , which has the goal of producing not the most appropriate order , but all grammatical onesWe suppose that this dificulty comes from the double function of the initial position which can either introduce the ad-dressation topic , or be the scene- or frame-setting position ( )We hypothesize that the reasons which bring a constituent to the VF are different from those which place it , say , to the beginning of the MF , for the order in the MF has been shown to be relatively rigid ( )Since our learner treats all values as nominal , we discretized the values of dep and len with a C4.5 classifier ( )Kruijff et al. ( ) describe an architecture which supports generating the appropriate word order for different languagesKruijff-Korbayova et al. ( ) address the task of word order generation in the same veinSimilar to Langkilde & Knight ( ) we utilize statistical methodsKendall's t , which has been used for evaluating sentence ordering tasks ( ) , is the second metric we useE.g. , in text-to-text generation ( ) , new sentences are fused from dependency structures of input sentencesRingger et al. ( ) aim at regenerating the order of constituents as well as the order within them for German and French technical manualsSimilar to Ringger et al. ( ) , we find the order with the highest probability conditioned on syntactic and semantic categoriesApart from acc and t , we also adopt the metrics used by Uchimoto et al. ( ) and Ringger et al. ( )According to the inv metric , our results are considerably worse than those reported by Ringger et al. ( )We retrained our system on a corpus of newspaper articles ( ) which is manually annotated but encodes no semantic knowledgeThe work of Uchimoto et al. ( ) is done on the free word order language JapaneseFor the fourth baseline (UCHIMOTO) , we utilized a maximum entropy learner (OpenNLP 8) and reim-plemented the algorithm of Uchimoto et al. ( )Uszkoreit ( ) addresses the problem from a mostly grammar-based perspective and suggests weighted constraints , such as [+nom] -< [+dat] , [+pro] -< [-pro] , [-focus] -< [+focus] , etcUnlike overgeneration approaches ( ) which select the best of all possible outputs ours is more efficient , because we do not need to generate every permutationIt also compares reasonably with other more recent evaluations ( ) which derive their input data from the penn Treebank by transforming each sentence tree into a format suitable for the realiser ( )For instance , ( ) reports that the implementation of such a processor for Surge was the most time consuming part of the evaluation with the resulting component containing 4000 lines of code and 900 rulesThe realiser presented here differs in mainly two ways from existing reversible realisers such as ( )'s CCG system or the HPSG ERG based realiser ( )The reason for this is that the grammar is compiled from a higher level description where tree fragments are first encapsulated into so-called classes and then explicitly combined (by inheritance , conjunction and disjunction) to produce the grammar elementary trees (cf. ( ))Thus for instance , both REALPRO ( ) and Surge ( ) assume that the input associates semantic literals with low level syntactic and lexical information mostly leaving the realiser to just handle inflection , word order , insertion of grammatical words and agreementTo associate semantic representations with natural language expressions , the FTAG is modified as proposed in ( )The proposal draws on ideas from ( ) and aims to determine whether for a given input (a set of TAG elementary trees whose semantics equate the input semantics) , syntactic requirements and resources cancel outIt could be used for instance , in combination with the parser and the semantic construction module described in ( ) , to support textual entailment recognition or answer detection in question answeringWe rely on these features to associate one and the same semantic to large sets of trees denoting semantically equivalent but syntactically distinct configurations (cf. ( ))The basic surface realisation algorithm used is a bottom up , tabular realisation algorithm ( ) optimised for TAGsSimilarly , KPML ( ) assumes access to ideational , interpersonal and textual information which roughly corresponds to semantic , mood/voice , theme/rheme and focus/ground informationIn order to ensure this determinism , NLG geared realisers generally rely on theories of grammar which systematically link form to function such as systemic functional grammar (SFG , ( )) and , to a lesser extent , Meaning Text Theory (MTT , ( ))First , the paraphrase figures might seem low wrt to e.g. , work by ( ) which mentions several thousand outputs for one given input and an average number of realisations per input varying between 85.7 and 102.2This does not seem to be the case in ( )'s approach where the count seems to include all sentences associated by the grammar with the input semanticsA Feature-based TAG (FTAG , ( )) consists of a set of (auxiliary or initial) elementary trees and of two tree composition operations: substitution and ad-junctionA first possibility would be to draw on ( )'s proposal and compute the enriched input based on the traversal of a systemic networkThus for instance , ( ) resorts to ad hoc \"mapping tables\" to associate substitution nodes with semantic indices and \"fr-nodes\" to constrain adjunction to the correct nodesWhile there have been previous systems that encode generation as planning ( ) , our approach is distinguished from these systems by its focus on the grammatically specified contributions of each individual word (and the TAG tree it anchors) to syntax , semantics , and local pragmatics ( )It also allows us to benefit from the past and ongoing advances in the performance of off-the-shelf planners ( )Unlike some approaches ( ) , we do not have to distinguish between generating NPs and expressions of other syntactic categoriesThe context set of an intended referent is the set of all individuals that the hearer might possibly confuse it with ( )It is based on the well-known STRIPS language ( )The grammar formalism we use here is that of lex-icalized tree-adjoining grammars (LTAG; Joshi and Schabes ( ))In order to use the planner as a surface realization algorithm for TAG along the lines of Koller and Striegnitz ( ) , we attach semantic content to each elementary tree and require that the sentence achieves a certain communicative goalHowever , this problem is NP-complete , by reduction of Hamiltonian Cycle - unsurprisingly , given that it encompasses realization , and the very similar realization problem in Koller and Striegnitz ( ) is NP-hardPDDL ( ) is the standard input language for modern planning systemsIn a scenario that involves multiple rabbits , multiple hats , and multiple individuals that are inside other individuals , but only one pair of a rabbit r inside a hat h , the expression \"X takes the rabbit from the hat\" is sufficient to refer uniquely to r and h ( )We share these advantages with systems such as SPUD ( )This makes our encoding more direct and transparent than those in work like Thomason and Hobbs ( ) and Stone et al. ( )We follow Stone et al. ( ) in formalizing the semantic content of a lexicalized elementary tree t as a finite set of atoms; but unlike in earlier approaches , we use the semantic roles in t as the arguments of these atomsThe three pragmatic predicates that we will use here are hearer-new , indicating that the hearer does not know about the existence of an individual and can't infer it ( ) , hearer-old for the opposite , and contextsetIn addition to the semantic content , we equip every elementary tree in the grammar with a semantic requirement and a pragmatic condition ( )Supertag This is a variant of the approach above , but using supertags ( ) instead of PoS tagsFor example , the metrics proposed in Bangalore et al. ( ) , such as Simple Accuracy and Generation Accuracy , measure changes with respect to a reference string based on the idea of string-edit distanceThe judges were then presented with the 50 sentences in random order , and asked to score the sentences according to their own scale , as in magnitude estimation ( ); these scores were then normalised in the range [0 ,1]Regarding the interpretation of the absolute value of (Pearson's) correlation coefficients , both here and in the rest of the paper , we adopt Cohen's scale ( ) for use in human judgements , given in Table 1; we use this as most of this work is to do with human judgements of fluencyThose chosen were the Connexor parser , 2 the Collins parser ( ) , and the Link Grammar parser ( )For example , in statistical MT the translation model and the language model are treated separately , characterised as faithfulness and fluency respectively (as in the treatment in Jurafsky and Martin ( ))A neat solution to poor sentence-level evaluation proposed by Kulesza and Shieber ( ) is to use a Support Vector Machine , using features such as word error rate , to estimate sentence-level translation qualityBleu ( ) is a canonical example: in matching n-grams in a candidate translation text with those in a reference text , the metric measures faithfulness by counting the matches , and fluency by implicitly using the reference n-grams as a language modelQuite a different idea was suggested in Wan et al. ( ) , of using the grammatical judgement of a parser to assess fluency , giving a measure independent of the language model used to generate the textIn terms of automatic evaluation , we are not aware of any technique that measures only fluency or similar characteristics , ignoring content , apart from that of Wan et al. ( )The consistency and magnitude of the first three parser metrics , however , lends support to the idea of Wan et al. ( ) to use something like these as indicators of generated sentence fluencySimilarly , the ultrasummarisa-tion model of Witbrock and Mittal ( ) consists of a content model , modelling the probability that a word in the source text will be in the summary , and a language modelIn this model we violate the Markov assumption of independence in much the same way as Witbrock and Mittal ( ) in their combination of content and language model probabilities , by backtracking at every state in order to discourage repeated words and avoid loopsZajic et al. ( ) use similar scales for summarisationCoreference resolution on text datasets is well-studied (e.g. , ( ))We employ a set of verbal features that is similar to the features used by state-of-the-art coreference resolution systems that operate on text (e.g. , ( ))Evaluation metric Coreference resolution is often performed in two phases: a binary classification phase , in which the likelihood of corefer-ence for each pair of noun phrases is assessed; and a partitioning phase , in which the clusters of mutually-coreferring NPs are formed , maximizing some global criterion ( )The verbal features that we have included are a representative sample from the literature (e.g. , ( ))also consider training separate classifiers and combining their posteriors , either through weighted addition or multiplication; this is sometimes called \"late fusion.\" Late fusion is also employed for gesture-speech combination in ( )All features are computed from hand and body pixel coordinates , which are obtained via computer vision; our vision system is similar to ( )The continuous-valued features were binned using a supervised technique ( )While people have little difficulty distinguishing between meaningful gestures and irrelevant hand motions (e.g. , self-touching , adjusting glasses) ( ) , NLP systems may be confused by such seemingly random movementsMarkable noun phrases - those that are permitted to participate in coreference relations - were annotated by the first author , in accordance with the MUC task definition ( )To measure the similarity between gesture trajectories , we use dynamic time warping ( ) , which gives a similarity metric for temporal data that is invariant to speedIn addition , verbal language is different when used in combination with meaningful non-verbal communication than when it is used unimodally ( )Kehler finds that fully-specified noun phrases are less likely to receive multimodal support ( )Last , we note that NPs with adjectival modifiers were assigned negative weights , supporting the finding of ( ) that fully-specified NPs are less likely to receive multimodal supportExperiments in both ( ) and ( ) find no conclusive winner among early fusion , additive late fusion , and multiplicative late fusionJS-div reports the Jensen-Shannon divergence , a continuous-valued feature used to measure the similarity in cluster assignment probabilities between the two gestures ( )The objective function (Equation 1) is optimized using a Java implementation of L-BFGS , a quasiNewton numerical optimization technique ( )However , non-verbal modalities are often noisy , and their interactions with speech are complex ( )Our non-verbal features attempt to capture similarity between the speaker's hand gestures; similar gestures are thought to suggest semantic similarity ( )Euclidean distance captures cases in which the speaker is performing a gestural \"hold\" in roughly the same location ( )Non-verbal meta features Research on gesture has shown that semantically meaningful hand motions usually take place away from \"rest position ,\" which is located at the speaker's lap or sides ( )Indeed , the psychology literature describes a finite-state model of gesture , proceeding from \"preparation ,\" to \"stroke ,\" \"hold ,\" and then \"retraction\" ( )Verbal meta features Meaningful gesture has been shown to be more frequent when the associated speech is ambiguous ( )The use of hidden variables in a conditionally-trained model follows ( )For example , Shriberg et al. ( ) explore the use of prosodic features for sentence and topic segmentationWhile more flexible than the interpolation techniques described in ( ) , training modality-specific classifiers separately is still suboptimal compared to training them jointly , because independent training of the modality-specific classifiers forces them to account for data that they cannot possibly explainToyama and Horvitz ( ) introduce a Bayesian network approach to modality combination for speaker identificationIntroduction With recent advances in spoken dialogue system technologies , researchers have turned their attention to more complex domains (e.g.tutoring ( ) , technical support ( ) , medication assistance ( ))Average (standard deviation) for objective metrics in the first problem Related work Discourse structure has been successfully used in non-interactive settings (e.g.understanding specific lexical and prosodic phenomena ( )  , natural language generation ( ) , essay scoring ( ) as well as in interactive settings (e.g.predictive/generative models of postural shifts ( ) , generation/interpretation of anaphoric expressions ( ) , performance modeling ( ))Other visual improvements for dialogue-based computer tutors have been explored in the past (e.g.talking heads ( ))This information is implicitly encoded in the intentional structure of a discourse as proposed in the Grosz & Sidner theory of discourse ( )3  The Navigation Map (NM) We use the Grosz & Sidner theory of discourse ( ) to inform our NM design2 ITSPOKE ITSPOKE ( ) is a state-of-the-art tutoring spoken dialogue system for conceptual physicsThus , interacting with such systems can be characterized by an increased user cognitive load associated with listening to often lengthy system turns and the need to integrate the current information to the discussion overall ( )However , implementing the NM in a new domain requires little expertise as previous work has shown that na�ve users can reliably annotate the information needed for the NM ( )While a somewhat similar graphical representation of the discourse structure has been explored in one previous study ( ) , to our knowledge we are the first to test its benefits (see Section 6)This theory has inspired several generic dialogue managers for spoken dialogue systems (e.g. ( ))One related study is that of ( )Results for Q1-6 Questions Q1-6 were inspired by previous work on spoken dialogue system evaluation (e.g. ( )) and measure user's overall perception of the systemThis situation is very similar to the training process of translation models in statistical machine translation ( ) , where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns   J^Pr(Wj |o k) = 1 , Vk j =1 This optimization problem can be solved by the EM algorithm ( )Studies have also shown that eye gaze has a potential to improve resolution of underspecified referring expressions in spoken dialog systems ( ) and to disambiguate speech input ( )Given the recent advances in eye tracking technology ( ) , integrating non-intrusive and high performance eye trackers with conversational interfaces becomes feasibleMotivated by psycholinguistic studies ( ) and recent investigations on computational models for language acquisition and grounding ( ) , we are particularly interested in two unique questions related to multimodal conversational systems: (1) In a multimodal conversation that involves more complex tasks (e.g. , both user initiated tasks and system initiated tasks) , is there a reliable temporal alignment between eye gaze and spoken references so that the coupled inputs can be used for automated vocabulary acquisition and interpretation? (2) If such an alignment exists , how can we model this alignment and automatically acquire and interpret the vocabularies? To address the first question , we conducted an empirical study to examine the temporal relationships between eye fixations and their corresponding spoken referencesAdditionally , before speaking a word , the eyes usually move to the objects to be mentioned ( )Previous psycholinguistics studies have shown that the direction of gaze carries information about the focus of the user's attention ( )In research on multimodal interactive systems , recent work indicates that the speech and gaze integration patterns can be modeled reliably for individual users and therefore be used to improve multimodal system performances ( )In addition , visual properties of the interface also affect user gaze behavior and thus influence the predication of attention ( ) based on eye gazeRecent work has shown that the effect of eye gaze in facilitating spoken language processing varies among different users ( )Recent studies have shown that multisensory information (e.g. , through vision and language processing) can be combined to effectively acquire words to their perceptually grounded objects in the environment ( )The perceived visual context influences spoken word recognition and mediates syntactic processing ( )Figure 1 Multimodal interface on tablet In this paper we explore the application of multimodal interface technologies (See Andr� ( ) for an overview) to the creation of more effective systems used to search and browse for entertainment content in the homeThese interfaces are cumbersome and do not scale well as the range of content available increases ( )An important advantage of speech is that it makes it easy to combine multiple constraints over multiple dimensions within a single query ( )A number of previous systems have investigated the addition of unimodal spoken search queries to a graphical electronic program guide ( ); Goto et al. , 2003; Wittenburg et al. , 2006)Others have gone beyond unimodal speech input and added multimodal commands combining speech with pointing ( )This develops and extends upon the multimodal architecture underlying the MATCH system ( )Speech recognition results , pointing gestures made on the display , and handwritten inputs , are all passed to a multimodal understanding server which uses finite-state multimodal language proc-essing techniques ( ) to interpret and integrate the speech and gestureHowever , as also reported in previous work ( ) , recognition accuracy remains a serious problemThe past few years have seen considerable improvement in the performance of unsupervised parsers ( ) and , for the first time , unsupervised parsers have been able to improve on the right-branching heuristic for parsing EnglishSome of these subsets were used for scoring in ( )Table 1 gives two baselines and the parsing results for WSJ10 , WSJ40 , Negra10 and Negra40 for recent unsupervised parsing algorithms: CCM and DMV+CCM ( ) , U-DOP ( ) and UML-DOP ( )There are several algorithms for doing so ( ) , which cluster words into classes based on the most frequent neighbors of each wordThis restriction is inspired by psycholin-guistic research which suggests that humans process language incrementally ( )When Klein and Manning induce the parts-of-speech , they do so from a much larger corpus containing the full WSJ treebank together with additional WSJ newswire ( )This can either be semi-supervised parsing , using both annotated and unannotated data ( ) or unsupervised parsing , training entirely on unan-notated textThis problem is known in psycholinguistics as the problem of reanalysis ( )For large datasets , we use an ensemble technique inspired by Bagging ( )In particular , we consider an algorithm proposed by Camerini et al. ( ) which has a worst-case complexity of O(km log(n)) , where k is the number of parses we want , n is the number of words in the input sentence , and m is the number of edges in the hypothesis graphThe k-best MST algorithm we introduce in this paper is the algorithm described in Camerini et al. ( )Algorithm 1 is a version of the MST algorithm as presented by Camerini et al. ( ); subtleties of the algorithm have been omittedWe have introduced the Camerini et al. ( ) k-best MST algorithm and have shown how to efficiently train MaxEnt models for dependency parsingMany of the model features have been inspired by the constituency-based features presented in Charniak and Johnson ( )Other DP solutions use constituency-based parsers to produce phrase-structure trees , from which dependency structures are extracted ( )An efficient algorithm for generating the k-best parse trees for a constituency-based parser was presented in Huang and Chiang ( ); a variation of that algorithm was used for generating projective dependency trees for parsing in Dreyer et al. ( ) and for training in McDonald et al. ( )The DP algorithms are generally variants of the CKY bottom-up chart parsing algorithm such as that proposed by Eisner ( )2 In order to explore a rich set of syntactic features in the MST framework , we can either approximate the optimal non-projective solution as in McDonald and Pereira ( ) , or we can use the constrained MST model to select a subset of the set of dependency parses to which we then apply less-constrained modelsUnlike the training procedure employed by McDonald et al. ( ) and McDonald and Pereira ( ) , we provide positive and negative examples in the training dataA second labeling stage can be applied to get labeled dependency structures as described in ( )The Maximum Spanning Tree algorithm 1 was recently introduced as a viable solution for non-projective dependency parsing ( )McDonald et al. ( ) introduced a model for dependency parsing based on the Edmonds/Chu-Liu algorithmMany of the features above were introduced in McDonald et al. ( ); specifically , the node-type , inside , and edge featuresWe have adopted the conditional Maximum Entropy (MaxEnt) modeling paradigm as outlined in Char-niak and Johnson ( ) and Riezler et al. ( )Work on statistical dependency parsing has utilized either dynamic-programming (DP) algorithms or variants of the Edmonds/Chu-Liu MST algorithm (see Tarjan ( ))This can be reduced to O(kn 2) in dense graphs 4 by choosing appropriate data structures ( )\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Following the notation in section 2.1 , the ij-th entry of the matrix W is defined as in ( ) Wj = (1 - e Wz ) where Cj is the total number of words in the j-th sentence pairFor the simple bag-of-word bilingual LSA as described in Section 2.2.1 , after SVD on the sparse matrix using the toolkit SVDPACK ( ) , all source and target words are projected into a low-dimensional (R = 88) LSA-spaceDiscriminative word alignment models , such as Ittycheriah and Roukos ( ); Moore ( ); Blunsom and Cohn ( ) , have received great amount of study recentlyFor instance , the 1 most relaxed IBM Model-1 , which assumes that any source word can be generated by any target word equally regardless of distance , can be improved by demanding a Markov process of alignments as in HMM-based models ( ) , or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models ( )It can be applied to complicated models such IBM Model-4 ( )The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing ( ) using all English sentences in the parallel training dataLSA has been successfully applied to information retrieval ( ) , statistical langauge modeling ( ) and etcAlternative constructions of the matrix are possible using raw counts or TF-IDF ( )It has been shown that human knowledge , in the form of a small amount of manually annotated parallel data to be used to seed or guide model training , can significantly improve word alignment F-measure and translation performance ( )Our decoder is a phrase-based multi-stack imple-mentation of the log-linear model similar to Pharaoh ( )Since Arabic is a morphologically rich language where affixes are attached to stem words to indicate gender , tense , case and etc , in order to reduce vocabulary size and address out-of-vocabulary words , we split Arabic words into affix and root according to a rule-based segmentation scheme ( ) with the help from the Buckwalter analyzer ( ) outputBasic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by ( ) and ( )In ( ) , bilingual semantic maps are constructed to guide word alignmentAs formulated in the competitive linking algorithm ( ) , the problem of word alignment can be regarded as a process of word linkage disambiguation , that is , choosing correct associations among all competing hypothesisThe example demos that due to reasonable constraints placed in word alignment training , the link to \"_tK\" is corrected and consequently we have accurate word translation for the Arabic singleton 7 Heuristics based on co-occurrence analysis , such as point-wise mutual information or Dice coefficients  , have been shown to be indicative for word alignments ( )These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method ( )By combining word alignments in two directions using heuristics ( ) , a single set of static word alignments is then formedWe simply modify the GIZA++ toolkit ( ) by always weighting lexicon probabilities with soft constraints during iterative model training , and obtain 0.7% TER reduction on both sets and 0.4% BLEU improvement on the test setWe measure translation performance by the BLEU score ( ) and Translation Error Rate (TER) ( ) with one reference for each hypothesisToutanova et al. ( ) augmented bilingual sentence pairs with part-of-speech tags as linguistic constraints for HMM-based word alignmentsWhile word alignments can help identifying semantic relations ( ) , we proceed in the reverse directionWe shall take HMM-based word alignment model ( ) as an example and follow the notation of ( )Our baseline word alignment model is the word-to-word Hidden Markov Model ( )Many state-of-the-art SMT systems do not use trees and base the ordering decisions on surface phrases ( )An important advantage of our model is that it is global , and does not decompose the task of ordering a target sentence into a series of local decisions , as in the recently proposed order models for Machine Transition ( )Alternatively , order is modelled in terms of movement of automatically induced hierarchical structure of sentences ( )These N-best lists are generated using approximate search and simpler models , as in the re-ranking approach of ( )tings , even for a bi-gram language model ( )9 The advantages of modeling how a target language syntax tree moves with respect to a source language syntax tree are that (i) we can capture the fact that constituents move as a whole and generally respect the phrasal cohesion constraints ( ) , and (ii) we can model broad syntactic reordering phenomena , such as subject-verb-object constructions translating into subject-object-verb ones , as is generally the case for English and JapanesePrevious work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees ( ) or dependency trees ( ) , which are obtained using a parser trained to determine linguistic constituencyOur results show that combining features derived from the source and target dependency trees , distortion surface order-based features (like the distortion used in Pharaoh ( )) and language model-like features results in a model which significantly outperforms models using only some of the information sourcesPharaoh DISP: Displacement as used in Pharaoh ( )These models are combined as feature functions in a (log)linear model for predicting a target sentence given a source sentence , in the framework proposed by ( )The target dependency trees are obtained through projection of the source dependency trees , using the word alignment (we use GIZA++ ( )) , ensuring better parallelism of the source and target structuresThe sentences were annotated with alignment (using GIZA++ ( )) and syntactic dependency structures of the source and target , obtained as described in Section 2Our model is discriminatively trained to select the best order (according to the BLEU measure) ( ) of an unordered target dependency tree from the space of possible ordersOur algorithm for obtaining target dependency trees by projection of the source trees via the word alignment is the one used in the MT system of ( )It follows the order model defined in ( )Our baseline SMT system is the system of Quirk et al. ( )The projection algorithm of ( ) defines heuristics for each of these problems1 Previous studies have shown that if both the source and target dependency trees represent linguistic constituency , the alignment between subtrees in the two languages is very complex ( )A host of discriminative methods have been introduced ( )2 We also investigated extraction-specific metrics: the frequency of interior nodes - a measure of how often the alignments violate the constituent structure of English parses - and a variant of the CPER metric of Ayan and Dorr ( )* From Ayan and Dorr ( ) , grow-diag-final heuristic5 Similarly , we compared our Chinese results to the GIZA++ results in Ayan and Dorr ( )Additionally , we evaluated our model with the transducer analog to the consistent phrase error rate (CPER) metric of Ayan and Dorr ( )Like the classic IBM models ( ) , our model will introduce a latent alignment vector a = {a 1  ,... ,a J } that specifies the position of an aligned target word for each source wordHowever , few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them ( )Syntactic methods are an increasingly promising approach to statistical machine translation , being both algorithmically appealing ( ) and empirically successful ( )Daume III and Marcu ( ) employs a syntax-aware distortion model for aligning summaries to documents , but condition upon the roots of the constituents that are jumped over during a transition , instead ofthose that are visited during a walk through the treeOur transductive learning algorithm , Algorithm 1 , is inspired by the Yarowsky algorithm ( )Under certain precise conditions , as described in ( ) , we can analyze Algorithm 1 as minimizing the entropy of the distribution over translations of UWe used the following scoring functions in our experiments: Length-normalized Score: Each translated sentence pair (t , s) is scored according to the model probability p (t | s) normalized by the length |t| of the target sentence: Score(t , s)  = p (t | s) 1*1 (3) Confidence Estimation: The confidence estimation which we implemented follows the approaches suggested in ( ): The confidence score of a target sentence t is calculated as a log-linear combination of phrase posterior probabilities , Levenshtein-based word posterior probabilities , and a target language model scoreOne language pair creates data for another language pair and can be naturally used in a ( )-style co-training algorithmThese lists are rescored with the following models: (a) the different models used in the decoder which are described above , (b) two different features based on IBM Model l ( ) , (c) posterior probabilities for words , phrases , n-grams , and sentence length ( ) , all calculated over the N-best list and using the sentence probabilities which the baseline system assigns to the translation hypothesesIn ( ) , a generative model for word alignment is trained using unsupervised learning on parallel textIn ( ) co-training is applied to MTAlong similar lines , ( ) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentencesBLEU score using the algorithm described in ( )The models (or features) which are employed by the decoder are: (a) one or several phrase table(s) , which model the translation direction p (s 1 1) , (b) one or several n-gram language model(s) trained with the SRILM toolkit ( ); in the experiments reported here , we used 4-gram models on the NIST data , and a trigram model on EuroParl , (c) a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase , and (d) a word penaltyfor a detailed description see ( )For details , see ( )It overlaps with the original phrase tables , but also contains many new phrase pairs ( )Self-training for SMT was proposed in ( )Recently , Cabezas and Resnik ( ) experimented with incorporating WSD translations into Pharaoh , a state-of-the-art phrase-based MT system ( )The relatively small improvement reported by Cabezas and Resnik ( ) without a statistical significance test appears to be inconclusiveNote that comparing with the MT systems used in ( ) and ( ) , the Hiero system we are using represents a much stronger baseline MT system upon which the WSD system must improveCarpuat and Wu ( ) integrated the translation predictions from a Chinese WSD system ( ) into a Chinese-English word-based statistical MT system using the ISI ReWrite decoder ( )Note that the experiments in ( ) did not use a state-of-the-art MT system , while the experiments in ( ) were not done using a full-fledged MT system and the evaluation was not on how well each source sentence was translated as a wholeWe obtain accuracy that compares favorably to the best participating system in the task ( )For our experiments , we use the SVM implementation of ( ) as it is able to work on multi-class problems to output the classification probability for each classCapitalizing on the strength of the phrase-based approach , Chiang ( ) introduced a hierarchical phrase-based statistical MT system , Hiero , which achieves significantly better translation performance than Pharaoh ( ) , which is a state-of-the-art phrase-based statistical MT systemIn this paper , we successfully integrate a state-of-the-art WSD system into the state-of-the-art hierarchical phrase-based MT system , Hiero ( )Hiero ( ) is a hierarchical phrase-based model for statistical machine translation , based on weighted synchronous context-free grammar (CFG) ( )Similar to ( ) , we trained the Hiero system on the FBIS corpus , used the NIST MT 2002 evaluation test set as our development set to tune the feature weights , and the NIST MT 2003 evaluation test set as our test dataFollowing ( ) , we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores ( ) based on case-insensitive n-gram matching , where n is up to 4A n-gram language model adds a dependence on (n�1) neighboring target-side words ( ) , making decoding much more difficult but still polynomial; in this paper , we add features that depend on the neighboring source-side words , which does not affect decoding complexity at all because the source string is fixedThe improvement of 0.57 is statistically significant at p  < 0.05 using the sign-test as described by Collins et al. ( ) , with 374 (+1) , 318 (�1) and 227 (0)To perform translation , state-of-the-art MT systems use a statistical phrase-based approach ( ) by treating phrases as the basic units of translationThe word alignments of both directions are then combined into a single set of alignments using the \"diag-and\" method of Koehn et al. ( )Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results ( )Our implemented WSD classifier uses the knowledge sources of local collocations , parts-of-speech (POS) , and surrounding words , following the successful approach of ( )First , we performed word alignment on the FBIS parallel corpus using GIZA++ ( ) in both directionsHiero uses a general log-linear model ( ) where the weight of a derivation  D for a particular source sentence and its translation is where  ^  i is a feature function and  X i is the weight for feature  ^  iUsing the MT 2002 test set , we ran the minimum-error rate training (MERT) ( ) with the decoder to tune the weights for each featurethe English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus , we trained a tri-gram language model using the SRI Language Modelling Toolkit ( )WSD approaches can be classified as (a) knowledge-based approaches , which make use of linguistic knowledge , manually coded or extracted from lexical resources ( ); (b) corpus-based approaches , which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models ( ); and (c) hybrid approaches , which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge ( )Although it has been argued that WSD does not yield better translation quality than a machine translation system alone , it has been recently shown that a WSD module that is developed following specific multilingual requirements can significantly improve the performance of a machine translation system ( )Finally , MC-WSD ( ) is a multi-class averaged perceptron classifier using syntactic and narrow context features , with one component trained on the data provided by Senseval and other trained on WordNet glossesIt is an interesting approach to learning which has been considered promising for several applications in natural language processing and has been explored for a few of them , namely POS-tagging , grammar acquisition and semantic parsing ( )For example , Dang and Palmer ( ) also use a rich set of features with a traditional learning algorithm (maximum entropy)Linguistic knowledge is available in electronic resources suitable for practical use , such as WordNet ( ) , dictionaries and parsersThere is not always a direct relation between the possible senses for a word in a (monolingual) lexicon and its translations to a particular language , so this represents a different task to WSD against a (monolingual) lexicon ( )CLaC1 ( ) uses a Naive Bayes algorithm with a dynamically adjusted context window around the target wordThe sense with the highest count of overlapping words in its dictionary definition and in the sentence containing the target verb (excluding stop words)   ( ) ,   represented by has_overlapping(sentence , translation) : has_overlapping(snt 1 , voltar)Verbs and possible senses in our corpus Both corpora were lemmatized and part-of-speech (POS) tagged using Minipar ( ) and Mxpost ( ) , respectivellyThese approaches have shown good results; particularly those using supervised learning ( )WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories ( )Syntalex-3 ( ) is based on an ensemble of bagged decision trees with narrow context part-of-speech features and bigramsThis is achieved using Inductive Logic Programming (ILP) ( ) , which has not yet been applied to WSDInductive Logic Programming ( ) employs techniques from Machine Learning and Logic Programming to build first-order theories from examples and background knowledge , which are also represented by first-order clauses2. A more specific clause (the bottom clause) is built using inverse entailment ( ) , generally consisting of the representation of all the knowledge about that exampleThis corpus was automatically annotated with the translation of the verb using a tagging system based on parallel corpus , statistical information and translation dictionaries ( ) , followed by a manual revisionAll the knowledge sources were made available to be used by the inference engine , since previous experiments showed that they are all relevant ( )We use the Aleph ILP system ( ) , which provides a complete inference engine and can be customized in various waysIn the hybrid approaches that have been explored so far , deep knowledge , like selectional preferences , is either pre-processed into a vector representation to accommodate machine learning algorithms , or used in previous steps to filter out possible senses e.g. ( )Roark and Bacchiani ( ) showed that weighted count-merging is a special case of maximum a posteriori (MAP) estimation , and successfully used it for probabilistic context-free grammar domain adaptation ( ) and language model adaptation ( )We have recently shown that this algorithm is effective in estimating the sense priors of a set of nouns ( )However , in ( ) , we showed that in a supervised setting where one has access to some annotated training data , the EM-based method in section 5 estimates the sense priors more effectively than the method described in ( )A similar work is the recent research by Chen et al. ( ) , where active learning was used successfully to reduce the annotation effort for WSD of 5 English verbs using coarse-grained evaluationThis is slightly higher than the 5.8 senses per verb in ( ) , where the experiments were conducted using coarse-grained evaluationFor WSD , Fujii et al. ( ) used selective sampling for a Japanese language WSD system , Chen et al. ( ) used active learning for 5 verbs using coarse-grained evaluation , and HDang ( ) employed active learning for another set of 5 verbsTo investigate this , Escudero et al. ( ) and Martinez and Agirre ( ) conducted experiments using the DSO corpus , which contains sentences from two different corpora , namely Brown Corpus (BC) and Wall Street Journal (WSJ)Escudero et al. ( ) pointed out that one of the reasons for the drop in accuracy is the difference in sense priors (i.e. , the proportions of the different senses of a word) between BC and WSJFollowing the setup of ( ) , we similarly made use of the DSO corpus to perform our experiments on domain adaptationAs mentioned in section 1 , research in ( ) noted an improvement in accuracy when they adjusted the BC and WSJ datasets such that the proportions of the different senses of each word were the same between BC and WSJEscudero et al. ( ) used the DSO corpus to highlight the importance of the issue of domain dependence of WSD systems , but did not propose methods such as active learning or count-merging to address the specific problem of how to perform domain adaptation for WSDThis is similar to the approach taken in ( ) where they focus on determining the predominant sense of words in corpora drawn from finance versus sports domainsResearch by McCarthy et al. ( ) and Koeling et al. ( ) pointed out that a change of predominant sense is often indicative of a change in domainThese knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( )To reduce the effort required to adapt a WSD system to a new domain , we employ an active learning strategy ( ) to select examples to annotate from the new domain of interestWith active learning ( ) , we use uncertainty sampling as shown r  � WSD system trained on D T b  � word sense prediction for d using r  p  � confidence of prediction b if p < p  min then Figure 1: Active learning in Figure 1The WordNet Domains resource ( ) assigns domain labels to synsets in WordNetAmong the few currently available manually sense-annotated corpora for WSD , the SEMCOR (SC) corpus ( ) is the most widely usedThe DSO corpus ( ) contains 192 ,800 annotated examples for 121 nouns and 70 verbs , drawn from BC and WSJIn this section , we describe an EM-based algorithm that was introduced by Saerens et al. ( ) , which can be used to estimate the sense priors , or a priori probabilities of the different senses in a new datasetMost of this section is based on ( )In applying active learning for domain adaptation , Zhang et al. ( ) presented work on sentence boundary detection using generalized Winnow , while Tur et al. ( ) performed language model adaptation of automatic speech recognition systemsIn most contexts , the similarity between chocolate , say , and a narcotic like heroin will mea-gerly reflect the simple ontological fact that both are kinds of substances; certainly , taxonomic measures of similarity as discussed in Budanitsky and Hirst ( ) will capture little more than this commonalityThe function (%sim arg 0 CAT) reflects the perceived similarity between the putative member arg 0 and a synset CAT in WordNet , using one of the standard formulations described in Budanitsky and Hirst ( )Whissell ( ) reduces the notion of affect to a single numeric dimension , to produce a dictionary of affect that associates a numeric value in the range 1.0 (most unpleasant) to 3.0 ( )We have described an approach that can be seen as a functional equivalent to the CPA (Corpus Pattern Analysis) approach of Pustejovsky et al. ( ) , in which our goal is not that of automated induction of word senses in context (as it is in CPA) but the automated induction of flexible , context-sensitive category structuresSince the line between literal and metaphoric uses of a category is often impossible to draw , the best one can do is to accept metaphor as a gradable phenomenon ( )The most revealing variations are syntagmatic in nature , which is to say , they look beyond individual word forms to larger patterns of contiguous usage ( )Dice\\'s coefficient ( ) is used to implement this measureAs noted by De Leenheer and de Moor ( ) , ontologies are lexical representations of concepts , so we can expect the effects of context on language use to closely reflect the effects of context on ontolog-Linguistic variation across contexts is often symp- ical structureWhile simile is a mechanism for highlighting inter-concept similarity , metaphor is at heart a mechanism of category inclusion ( )Glucksberg ( ) notes that the same category , used figuratively , can exhibit different qualities in different metaphorsIn this section , we describe how we use Markov chain Monte Carlo methods to perform inference in the statistical models described in the previous section; Andrieu et al. ( ) provide an excellent introduction to MCMC techniquesThese are short statements that restrict the space of languages in a concrete way (for instance \"object-verb ordering implies adjective-noun ordering\"); Croft ( ) , Hawkins ( ) and Song ( ) provide excellent introductions to linguistic typologyThis is a well-documented issue (see , eg. , ( )) stemming from the fact that any set of languages is not sampled uniformly from the space of all probable languagesThe closest work is represented by the books Possible and Probable Languages ( ) and Language Classification by Numbers ( ) , but the focus of these books is on automatically discovering phylogenetic trees for languages based on Indo-European cognate sets ( )Those that reference Hawkins (eg. , #11) are based on implications described by Hawkins ( ); those that reference Lehmann are references to the principles decided by Lehmann ( ) in Ch 4 & 8They have also been used computationally to aid in the learning of unsupervised part of speech taggers ( )For instance our #7 is implication #18 from Greenberg , reproduced by Song ( )We examined sentences using a phrase structure parser ( ) and an HPSG parser ( )Since the number of parameters in NLM is still large , several smoothing methods are used ( ) to produce more accurate probabilities , and to assign nonzero probabilities to any word stringWe would like to see more refined online learning methods with kernels ( ) that we could apply in these areasTherefore we make use of an online learning algorithm proposed by ( ) , which has a much smaller computational costBlei , 2003; Wang et al. , 2005) , our result may encourage the study ofthe combination offeatures forlanguage modelingWe used a Viterbi decoding ( ) for the partitionDiscriminative language models (DLMs) have been proposed to classify sentences directly as correct or incorrect ( ) , and these models can handle both non-local and overlapping informationFor fast kernel computation , the Polynomial Kernel Inverted method (PKI)) is proposed ( ) , which is an extension of Inverted Index in Information RetrievalThe class model was originally proposed by ( )However , by considering only those counts that actually change , the algorithm can be made to scale somewhere between linearly and quadratically to the number of classes ( )Recently , Whole Sentence Maximum Entropy Models ( ) (WSMEs) have been introducedIn our experiments , we did not examine the result of using other sampling methods , For example , it would be possible to sample sentences from a whole sentence maximum entropy model ( ) and this is a topic for future researchA contrastive estimation method ( ) is similar to ours with regard to constructing pseudo-negative examplesIf the kernel-trick ( ) is applied to online margin-based learning , a subset of the observed examples , called the active set , needs to be storedIt should be noted that models based on finite state transducers have been shown to be adequate for describing fusion as well( ) , and further work should evaluate these types of models in ASR of languages with higher indexes of fusionThe final approach applies a manually constructed rule-based morphological tagger( )For training the LMs , a subset of 43 million words from the Estonian Segakorpus was used( ) , preprocessed with a morphological analyzer( )In ( ) a WER of 44.5% was obtained with word-based trigrams and a WER of 37.2% with items similar to ones from \"grammar\" using the same speech corpus as in this workIt should be noted that every OOV causes roughly two errors in recognition , and vocabulary decomposition approaches such as the ones evaluated here give some benefits to word error rate (WER) even in recognizing languages such as English( )This is similar to what was introduced as \"flat hybrid model\"( ) , and it tries to model OOV-words as sequences of words and fragmentsThe results for \"hybrid\" are in in the range suggested by earlier work( )The morph approach was developed for the needs of Finnish speech recognition , which is a high synthesis , moderate fusion and very low orthographic irregularity language , whereas the hybrid approach in ( ) was developed for English , which has low synthesis , moderate fusion , and very high orthographic irregularityVarigrams( ) are used in this work , and to make LMs trained with each approach comparable , the varigrams have been grown to roughly sizes of 5 million countsing approach , growing varigram models( ) were used with no limits as to the order of n-grams , but limiting the number of counts to 4.8 and 5 million countsFor example , in English with language models (LM) of 60k words trained from the Gigaword Corpus V.2( ) , and testing on a very similar Voice of America -portion of TDT4 speech corpora( ) , this gives a OOV rate of 1.5%Models of this type have previously been shown to yield very good g2p conversion results ( )It has been argued that using morphological information is important for languages where morphology has an important influence on pronunciation , syllabiication and word stress such as German , Dutch , Swedish or , to a smaller extent , also English ( )Decision trees were one of the first data-based approaches to g2p and are still widely used ( )Best results were obtained when using a variant of Modified Kneser-Ney Smoothing 2 ( )( ) also used a joint n-gram modelWe compared four different state-of-the-art unsu-pervised systems for morphological decomposition (cf. ( ))In very recent work , ( ) developed an unsupervised algorithm (f-meas: 68%; an extension of RePortS) whose segmentations improve g2p when using a the decision tree (PER: 3.45%)The German corpus used in these experiments is CELEX ( )Among the unsupervised systems , best results 7 on the g2p task with morphological annotation were obtained with the RePortS system ( )The same algorithms have previously been shown to help a speech recognition task ( )The joint n-gram model performs significantly better than the decision tree (essentially based on ( )) , and achieves scores comparable to the Pronunciation by Analogy (PbA) algorithm ( )This is much faster than the times for Pronunciation by Analogy (PbA) ( ) on the same corpusExamples of such approaches using Hidden Markov Models are ( ) (who applied the HMM to the related task of phoneme-to-grapheme conversion) , ( ) and ( )For German , ( ) show that information about stress assignment and the position of a syllable within a word improve g2p conversionVowel length and quality has been argued to also depend on morphological structure ( )The two rule-based systems we evaluated , the ETI 4 morphological system and SMOR 5 ( ) , are both high-quality systems with large lexica that have been developed over several yearsWe used the syllabifier described in ( ) , which works similar to the joint n-gram model used for g2p conversionA possible reason for the observed dichotomy in the behavior of the vowel and consonant inventories with respect to redundancy can be as follows: while the organization of the vowel inventories is known to be governed by a single force - the maximal perceptual contrast ( )) , consonant inventories are shaped by a complex interplay of several forces ( )It has been postulated earlier by functional phonologists that such regularities are the consequences of certain general principles like maximal perceptual contrast ( ) , which is desirable between the phonemes of a language for proper perception of each individ-ual phoneme in a noisy environment , ease of articulation ( ) , which requires that the sound systems of all languages are formed of certain universal (and highly frequent) sounds , and ease of learnability( ) , which is necessary for a speaker to learn the sounds of a language with minimum effortSuch an observation is significant since whether or not these principles are similar/different for the two inventories had been a question giving rise to perennial debate among the past researchers ( )On the other hand , in spite of several attempts ( ) the organization of the consonant inventories lacks a satisfactory explanationVarious attempts have been made in the past to explain the aforementioned trends through linguistic insights ( ) mainly establishing their statistical significanceFor instance , in biological systems we find redundancy in the codons ( ) , in the genes ( ) and as well in the proteins ( )In fact , the organization of the vowel inventories (especially those with a smaller size) across languages has been satisfactorily explained in terms of the single principle of maximal perceptual contrast ( )This redundancy is present mainly to reduce the risk of the complete loss of information that might occur due to accidental errors ( )Many typological studies ( ) of segmental inventories have been carried out in past on the UCLA Phonological Segment Inventory Database (UPSID) ( )In order to explain these trends , feature economy was proposed as the organizing principle of the consonant inventories ( )Inspired by the aforementioned studies and the concepts of information theory ( ) we try to quantitatively capture the amount of redundancy found across the consonant Table 1: The table shows four plosivesFor this purpose , we present an information theoretic definition of redundancy , which is calculated based on the set of features 1 ( ) that are used to express the consonantsHowever , one of the earliest observations about the consonant inventories has been that consonants tend to occur in pairs that exhibit strong correlation in terms of their features ( )2 Previous Work Previous work � e.g. ( ) � has mostly assumed that one has a training lexicon of transliteration pairs , from which one can learn a model , often a source-channel or MaxEnt-based modelA linear classifier is trained using the Winnow algorithm from the SNoW toolkit ( )Using comparable corpora , the named-entities for persons and locations were extracted from the English text; in this paper , the English named-entities were extracted using the named-entity recognizer described in Li et al. ( ) , based on the SNoW machine learning toolkit ( )This is quite small compared to previous approaches such as Knight and Graehl ( ) or Gao et al. ( )Gildea and Jurafsky ( ) counted the number of features whose values are different , and used them as a substitution costHalle and Clements ( )\\'s distinctive features are used in order to model the substitution/ insertion/deletion costs for the string-alignment algorithm and linear classifierAll pronunciations are based on the WorldBet transliteration system ( ) , an ascii-only version of the IPAa. For all the training data , the pairs of pronunciations are aligned using standard string alignment algorithm based on Kruskal ( )For the set of features X and set of weights W , the linear classifier is defined as [1] ( ) X  = { X [ , X2 , ..In this paper , the phonetic transliteration is performed using the following steps: 1)  Generation of the pronunciation for English words and target words: a. Pronunciations for English words are obtained using the Festival text-to-speech system ( )Based on the pronunciation error data of learners of English as a second language as reported in ( ) , we propose the use of what we will term pseudofeaturesExamples of the top-3 candidates in the transliteration of English-Korean To evaluate the proposed transliteration methods quantitatively , the Mean Reciprocal Rank (MRR) , a measure commonly used in information retrieval when there is precisely one correct answer ( ) was measured , following Tao and Zhai ( )In our work , we adopt the method proposed in ( ) and apply it to the problem of transliterationThe substitution/insertion/deletion cost for the string alignment algorithm is based on the baseline cost from ( )The pseudo features in this study are same as in Tao et al. ( )MRRs of the phonetic transliteration The baseline was computed using the phonetic transliteration method proposed in Tao et al. ( )By treating a letter/character as a word and a group of letters/characters as a phrase or token unit in SMT , one can easily apply the traditional SMT models , such as the IBM generative model ( ) or the phrase-based translation model ( ) to transliterationIn G2P studies , Font Llitjos and Black ( ) showed how knowledge of language of origin may improve conversion accuracyPhonetic transliteration can be considered as an extension to the traditional grapheme-to-phoneme (G2P) conversion ( ) , which has been a much-researched topic in the field of speech processingMany of the loanwords exist in today\\'s Chinese through semantic transliteration , which has been well received ( ) by the people because of many advantagesUnfortunately semantic transliteration , which is considered as a good tradition in translation practice ( ) , has not been adequately addressed computationally in the literature5.S Semantic Transliteration The performance was measured using the Mean Reciprocal Rank (MRR) metric ( ) , a measure that is commonly used in information retrieval , assuming there is precisely one correct answerIn computational linguistic literature , much effort has been devoted to phonetic transliteration , such as English-Arabic , English-Chinese ( ) , English-Japanese ( ) and English-KoreanIn the extraction of transliterations , data-driven methods are adopted to extract actual transliteration pairs from a corpus , in an effort to construct a large , up-to-date transliteration lexicon ( )The Latin-scripted personal names are always assumed to homogeneously follow the English phonic rules in automatic transliteration ( )This model is conceptually similar to the joint source-channel model ( ) where the target token t i depends on not only its source token s i but also the history t i-1 and s i -1Some recent work ( ) has attempted to introduce preference into a probabilistic framework for selection of Chinese characters in phonetic transliterationIn transliteration modeling , transliteration rules are trained from a large , bilingual transliteration lexicon ( ) , with the objective of translating unknown words on the fly in an open , general domainAs discussed elsewhere ( ) , out of several thousand common Chinese characters , a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese , e.g.only 731 Chinese characters are adopted in the E-C corpusAs a Chinese transliteration can arouse to certain connotations , the choice of Chinese characters becomes a topic of interest ( )Table 4: Lexicon statistics For Arabic , as a full-size Arabic lexicon was not available to us , we used the Buckwalter morphological analyzer ( ) to derive a lexiconFor example , ( ) showed that factored language models , which consider morphological features and use an optimized backoff policy , yield lower perplexityA recent work ( ) experimented with English-to-Turkish translation with limited success , suggesting that inflection generation given morphological features may give positive resultsMore recently , ( ) achieved improvements in Czech-English MT , optimizing a Table 1: Morphological features used for Russian and Arabic set of possible source transformations , incorporating morphologyIdeally , the best word analysis should be provided as a result of contextual disambiguation (e.g. , ( )); we leave this for future workAnother work ( ) showed improvements by splitting compounds in GermanTranslating from a morphology-poor to a morphology-rich language is especially challenging since detailed morphological information needs to be decoded from a language that does not encode this information or does so only implicitly ( )Koehn ( ) includes a survey of statistical MT systems in both directions for the Europarl corpus , and points out the challenges of this taskFor example , it has been shown ( ) that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment , thus leading to improved overall translation qualityFor Arabic , we apply the following heuristic: use the most frequent analysis estimated from the gold standard labels in the Arabic Treebank ( ); if a word does not appear in the treebank , we choose the first analysis returned by the Buckwalter analyzerOur learning framework uses a Maximum Entropy Markov model ( )( ) demonstrated that a similar level of alignment quality can be achieved with smaller corpora applying morpho-syntactic source restructuring , using hierarchical lexicon models , in translating from German into EnglishThe sentence pairs were word-aligned using GIZA++ ( ) and submitted to a treelet-based MT system ( ) , which uses the word dependency structure of the source language and projects word dependency structure to the target language , creating the structure shown in Figure 1 above( ) experimented successfully with translating from inflectional languages into English making use of POS tags , word stems and suffixes in the source languageThe framework suggested here is most closely related to ( ) , which uses a probabilistic model to generate Japanese case markers for English-to-Japanese MTThe algorithm is similar to the one described in ( )It also differs from now traditional uses of comparable corpora for detecting translation equivalents ( ) or extracting terminology ( ) , which allows a one-to-one correspondence irrespective of the contextIn the spirit of ( ) , it is intended as a translator\\'s amenuensis \"under the tight control of a human translator ..It has been aligned on the sentence level by JAPA ( ) , and further on the word level by GIZA++ ( )Thus the present system is unlike SMT ( ) , where lexical selection is effected by a translation model based on aligned , parallel corpora , but the novel techniques it has developed are exploitable in the SMT paradigmSimilarity is measured as the cosine between collocation vectors , whose dimensionality is reduced by SVD using the implementation by Rapp ( )We have generalised the method used in our previous study ( ) for extracting equivalents for continuous multiword expressions (MWEs)Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality , especially the phrase-based models ( ) and syntax-based models ( )By adapting the k-best parsing Algorithm 2 of Huang and Chiang ( ) , it achieves significant speed-up over full-integration on Chiang\\'s Hiero system� We also devise a faster variant of cube pruning , called cube growing , which uses a lazy version of k-best parsing ( ) that tries to reduce k to the minimum needed at each node to obtain the desired number of hypotheses at the rootIn a nutshell , cube pruning works on the �LM forest , keeping at most k +LM items at each node , and uses the k-best parsing Algorithm 2 of Huang and Chiang ( ) to speed up the computationThis situation is very similar to k-best parsing and we can adapt the Algorithm 2 of Huang and Chiang ( ) here to explore this grid in a best-first orderThis new method , called cube growing , is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang ( ) , is a lazy version of Algorithm 2 (see Table 1)The different target sides then constitute a third dimension of the grid , forming a cube of possible combinations ( )The data set is same as in Section 5.1 , except that we also parsed the English-side using a variant of the Collins ( ) parser , and then extracted 24.7M tree-to-string rules using the algorithm of ( )These forest rescoring algorithms have potential applications to other computationally intensive tasks involving combinations of different models , for example , head-lexicalized parsing ( ); joint parsing and semantic role labeling ( ); or tagging and parsing with nonlocal featuresIn tree-to-string (also called syntax-directed) decoding ( ) , the source string is first parsed into a tree , which is then recursively converted into a target string according to transfer rules in a synchronous grammar ( )We generalize cube pruning and adapt it to two systems very different from Hiero: a phrase-based system similar to Pharaoh ( ) and a tree-to-string system ( )We test our methods on two large-scale English-to-Chinese translation systems: a phrase-based system and our tree-to-string system ( )Our data preparation follows Huang et al. ( ): the training data is a parallel corpus of28.3M words on the English side , and a trigram language model is trained on the Chinese sideFor cube growing , we use a non-duplicate k-best method ( ) to get 100-best unique translations according to � LM to estimate the lower-bound heuristicsSince our tree-to-string rules may have many variables , we first binarize each hyperedge in the forest on the target projection ( )Thus we envision forest rescoring as being of general applicability for reducing complicated search spaces , as an alternative to simulated annealing methods ( )Part of the complexity arises from the expressive power of the translation model: for example , a phrase- or word-based model with full reordering has exponential complexity ( )We will use the following example from Chinese to English for both systems described in this section: yU  Shalong juxing le huitan with Sharon hold   [past] meeting \\'held a meeting with Sharon\\' A typical phrase-based decoder generates partial target-language outputs in left-to-right order in the form of hypotheses ( )We implemented Cubit , a Python clone of the Pharaoh decoder ( ) , 3 and adapted cube pruning to it as followsWe set the decoder phrase-table limit to 100 as suggested in ( ) and the distortion limit to 4An SCFG ( ) is a context-free rewriting system for generating string pairsTo integrate with a bigram language model , we can use the dynamic-programming algorithms of Och and Ney ( ) and Wu ( ) for phrase-based and SCFG-based systems , respectively , which we may think of as doing a iner-grained version of the deductions aboveThe language model also , if fully integrated into the decoder , introduces an expensive overhead for maintaining target-language boundary words for dynamic programming ( )Similarly , the decoding problem with SCFGs can also be cast as a deductive (parsing) system ( )However , the hope is that by choosing the right value ofi , these estimates will be accurate enough to affect the search quality only slightly , which is analogous to \"almost admissible\" heuristics in A* search ( )A few exceptions are the hierarchical (possibly syntax-based) trans-duction models ( ) and the string transduction models ( )The SFST approach described here is similar to the one described in ( ) which has subsequently been adopted by ( )In preliminary experiments , we have associated the target lexical items with supertag information ( )We separate the most popular classification techniques into two broad categories: also called Maxent as it finds the distribution with maximum entropy that properly estimates the average of each feature over the training data ( )Most of the previous work on statistical machine translation , as exemplified in ( ) , employs word-alignment algorithm (such as GIZA++ ( )) that provides local associations between source and target wordsThe BOW approach is different from the parsing based approaches ( ) where the translation model tightly couples the syntactic and lexical items of the two languagesThe excellent results recently obtained with the SEARN algorithm ( ) also suggest that binary classifiers , when properly trained and combined , seem to be capable ofmatching more complex structured output approachesA new L1-regularized Maxent algorithms was proposed for density estimation ( ) and we adapted it to classificationFrom the bilanguage corpus B , we train an n-gram language model using standard tools ( )The use of supertags in phrase-based SMT system has been shown to improve results ( )here: all state hypotheses of a whole sentence are kept in memory) , it is necessary to either use heuristic forward pruning or constrain permutations to be within a local window of adjustable size (also see ( ))Although Conditional Random Fields (CRF) ( ) train an exponential model at the sequence level , in translation tasks such as ours the computational requirements of training such models are prohibitively expensiveWe found this algorithm to converge faster than the current state-of-the-art in Maxent training , which is L2-regularized L-BFGS ( ) 1Discriminative training has been used mainly for translation model combination ( ) and with the exception of ( ) , has not been used to directly train parameters of a translation modelFor the work reported in this paper , we have used the GIZA++ tool ( ) which implements a string-alignment algorithmEach output label t is projected into a bit string with components b j (t) where probability of each component is estimated independently: In practice , despite the approximation , the 1-vs-other scheme has been shown to perform as well as the multiclass scheme ( )For the Hansard corpus we used the same training and test split as in ( ): 1.4 million training sentence pairs and 5432 test sentencesIn search of a balance between structural flexibility and computational complexity , several authors have proposed constraints to identify classes of non-projec-tive dependency structures that are computationally well-behaved ( )This result generalizes previous work on the relation between ltag and dependency representations ( )? The encoding of dependency structures as order-annotated trees allows us to reformulate two constraints on non-projectivity originally defined on fully specified dependency structures ( ) in terms of syntactic properties of the order annotations that they induce: Gap-degree The gap-degree of a dependency structure is the maximum over the number of discontinuities in any yield of that structureThis enables us to generalize a previous result on the class of dependency structures generated by lexicalized tags ( ) to the class of generated dependency languages , LTALLately , they have also been used in many computational tasks , such as relation extraction ( ) , parsing ( ) , and machine translation ( )Unfortunately , most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar ( ) , parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted ( )We also show that adding the well-nestedness condition corresponds to the restriction of lcfrs to Coupled Context-Free Grammars ( ) , and that regular sets of well-nested structures with a gap-degree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoining Grammar (ltag)This restriction is central to the formalism of Coupled-Context-Free Grammar (ccfg) ( )REGD w�(k) = LCCFL(k + 1) As a special case , Coupled-Context-Free Grammars with fan-out 2 are equivalent to Tree Adjoining Grammars (tags) ( )This gives rise to a notion of regular dependency languages , and allows us to establish a formal relation between the structural constraints and mildly context-sensitive grammar formalisms ( ): We show that regular dependency languages correspond to the sets of derivations of lexicalized Linear Context-Free Rewriting Systems (lcfrs) ( ) , and that the gap-degree measure is the structural correspondent of the concept of \\'fan-out\\' in this formalism ( )Such a comparison may be empirically more adequate than one based on traditional notions of generative capacity ( )Both constraints have been shown to be in very good fit with data from dependency treebanks ( )A dependency structure is projective , if each of its yields forms an interval with respect to the precedence order ( )Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another ( ) , but becomes intractable when this assumption is abandoned ( )The number of components in the order-annotation , and hence , the gap-degree of the resulting dependency language , corresponds to the fan-out of the function: the highest number of components among the arguments of the function ( )Linear Context-Free Rewriting Systems Gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (lcfrs) ( ) , a class of formal systems that generalizes several mildly context-sensitive grammar formalismsThe Unfold-Fold transformation is a calculus for transforming functional and logic programs into equivalent but (hopefully) faster programs ( )Standard methods for converting weighted CFGs to equivalent PCFGs can be used if required ( )Second , Eisner-Satta O(n 3) PBDG parsing algorithms are extremely fast ( )It is straight-forward to extend the split-head CFG to encode the additional state information required by the head automata of Eisner and Satta ( ); this corresponds to splitting the non-terminals L u and uRThe O(n 3) split-head grammar is closely related to the O(n 3) PBDG parsing algorithm given by Eisner and Satta ( )Goodman ( ) observed that the Viterbi parse is in general not the optimal parse for evaluation metrics such as f-score that are based on the number of correct constituents in a parseFor example , incremental CFG parsing algorithms can be used with the CFGs produced by this transform , as can the Inside-Outside estimation algorithm ( ) and more exotic methods such as estimating adjoined hidden states ( )The closest related work we are aware of is McAllester ( ) , which also describes a reduction of PBDGs to efficiently-parsable CFGs and directly inspired this workThis paper investigates the relationship between Context-Free Grammar (CFG) parsing and the Eis-ner/Satta PBDG parsing algorithms , including their extension to second-order PBDG parsing ( )First , because they capture bilexical head-to-head dependencies they are capable of producing extremely high-quality parses: state-of-the-art dis-criminatively trained PBDG parsers rival the accuracy of the very best statistical parsers available today ( )The steps involved in CKY parsing with this grammar correspond closely to those of the McDonald ( ) second-order PBDG parsing algorithmThese weights are estimated by an online procedure as in McDonald ( ) , and are not intended to define a probability distributionWe provided one grammar which captures horizontal second-order dependencies ( ) , and another which captures vertical second-order head-to-head-to-head dependenciesSince CFGs can be expressed as Horn-clause logic programs ( ) and the Unfold-Fold transformation is provably correct for such programs ( ) , it follows that its application to CFGs is provably correct as wellSpecifically , we show how to use an off-line preprocessing step , the Unfold-Fold transformation , to transform a PBDG into an equivalent CFG that can be parsed in O(n 3) time using a version of the CKY algorithm with suitable indexing ( ) , and extend this transformation so that it captures second-order PBDG dependencies as wellBy a slight generalization of a result by Aoto ( ) , this typing r h N\\' : a must be negatively non-duplicated in the sense that each atomic type has at most one negative occurrence in itBy Aoto and Ono\\'s ( ) generalization of the Coherence Theorem ( ) , it follows that every /-term P such that r\\' h P : a for some r\\' c r must be Sr-equal to N\\' (and consequently to N)The reduction to Datalog makes it possible to apply to parsing and generation sophisticated evaluation techniques for Datalog queries; in particular , an application of generalized supplementary magic-sets rewriting ( ) automatically yields Earley-style algorithms for both parsing and generation(In the case of an IO macro grammar , the result is an IO context-free tree grammar ( ).) String copying becomes tree copying , and the resulting grammar can be represented by an almost linear CFLG and hence by a Datalog programWith regard to parsing and recognition of input strings , polynomial-time algorithms and the LOGCFL upper bound on the computational complexity are already known for the grammar formalisms covered by our results ( ); nevertheless , we believe that our reduction to Datalog offers valuable insightsIn this paper , we show that a similar reduction to Datalog is possible for more powerful grammar formalisms with \"context-free\" derivations , such as (multi-component) tree-adjoining grammars ( ) , IO macro grammars ( ) , and (parallel) multiple context-free grammars ( )By the main result of Gottlob et al. ( ) , the related search problem of finding one derivation tree for the input /-term is in functional LOGCFL , i.e. , the class of functions that can be computed by a logspace-bounded Turing machine with a LOGCFL oracleOur method essentially relies on the encoding of different formalisms in terms of abstract categorial grammars ( )What we have called a context-free / -term grammar is nothing but an alternative notation for an abstract categorial grammar ( ) whose abstract vocabulary is second-order , with the restriction to linear /-terms removedA string-generating grammar coupled with Montague semantics may be represented by a synchronous CFLG , a pair of CFLGs with matching rule sets ( )linear ACGs are known to be expressive enough to encode well-known mildly context-sensitive grammar formalisms in a straightforward way , including TAGs and multiple context-free grammars ( )For example , the linear CFLG in Figure 8 is an encoding of the TAG in Figure 3 , where a(S) = o�>o and a(A) = (o �> o) �> o � > o ( )We can eliminate e-rules from an almost linear CFLG by the same method that Kanazawa and Yoshinaka ( ) used for linear grammars , noting that for any r and a , there are only finitely many almost linear /-terms M such that r h M : a.If a grammar has no e-rule , any derivation tree for the input /-term N that has a /-term P at its root node corresponds to a Datalog derivation tree whose number of leaves is equal to the number of occurrences of constants in P , which cannot exceed the number of occurrences of constants in NFor such P and D , it is known that {(D , q) | D eD , P U D derives q } is in the complexity class LOGCFL ( )3 In the linear case , Salvati ( ) has shown the recognition/parsing complexity to be PTIME , and exhibited an algorithm similar to Earley parsing for TAGsThe result of the generalized supplementary magic-sets rewriting of Beeri and Ramakrish-nan ( ) applied to the Datalog program representing a CFG essentially coincides with the deduction system ( ) or uninstantiated parsing system ( ) for Earley parsingBy naive (or seminaive) bottom-up evaluation ( ) , the answer to such a query can be computed in polynomial time in the size of the database for any Datalog programWe illustrate this approach with the program in Figure 4 , following the presentation of Ullman ( )But there are also other factors involved - for example , the tendency to put \"given\" discourse elements before \"new\" ones , which has been shown to play a role independent of length ( )First , how close is dependency length in English to that of this optimal DLA? Secondly , how similar is the optimal DLA to English in terms of the actual rules that arise? Finding linear arrangements of graphs that minimize total edge length is a classic problem , NP-complete for general graphs but with an O(n 16) algorithm for trees ( )Statistical parsers make use of features that capture dependency length (e.g.an adjacency feature in Collins ( ) , more explicit length features in McDonald et al. ( ) and Eisner and Smith ( )) and thus learn to favor parses with shorter dependenciesWe take sentences from the Wall Street Journal section of the Penn Treebank , extract the dependency trees using the head-word rules of Collins ( ) , consider them to be unordered dependency trees , and linearize them to minimize dependency lengthExactly this pattern has been observed by Dryer ( ) in natural languagesFrazier ( ) suggests that this might serve the function of keeping heads and dependents close togetherThis has been offered as an explanation for numerous psycholinguistic phenomena , such as the greater processing difficulty of object relative clauses versus subject relative clauses ( )Hawkins ( ) has shown that this principle is reflected in grammatical rules across many languagesOne might suppose that such syntactic choices in English are guided at least partly by dependency length minimization , and indeed there is evidence for this; for example , people tend to put the shorter of two PPs closer to the verb ( )The problem of finding the optimum weighted DLA for a set of input trees can be shown to be NP-complete by reducing from the problem of finding a graph\\'s minimum Feedback Arc Set , one of the 21 classic problems of Karp ( )This setting is reminiscent of the problem of optimizing feature weights for reranking of candidate machine translation outputs , and we employ an optimization technique similar to that used by Och ( ) for machine translationIn particular , our approach would be applicable to corpora with frame-specific role labels , e.g.FrameNet ( )Our work suggests that feature generalization based on verb-similarity may compliment approaches to generalization based on role-similarity ( )For this task we utilized the August 2005 release of the Charniak parser with the default speed/accuracy settings ( ) , which required roughly 360 hours of processor time on a 2.5 GHz PowerPC G5To automatically identify all verb inflections , we utilized the English DELA electronic dictionary ( ) , which contained all but 21 of the PropBank verbs (for which we provided the inflections ourselves) , with old-English verb inflections removedParse tree paths were used for semantic role labeling by Gildea and Jurafsky ( ) as descriptive features of the syntactic relationship between predicates and their arguments in the parse tree of a sentenceIn future work , it would be particularly interesting to compare empirically-derived verb clusters to verb classes derived from theoretical considerations ( ) , and to the automated verb classification techniques that use these classes ( )Our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information ( )This observation further supports the distributional hypothesis of word similarity and corresponding technologies for identifying synonyms by similarity of lexical-syntactic context ( )In our work , we utilized the GigaWord corpus of English newswire text ( ) , consisting of nearly 12 gigabytes of textual dataAnnotations similar to these have been used to create automated semantic role labeling systems ( ) for use in natural language processing applications that require only shallow semantic parsingThe overall performance of our semantic role labeling approach is not competitive with leading contemporary systems , which typically employ support vector machine learning algorithms with syntactic features ( ) or syntactic tree kernels ( )A recent release of the PropBank ( ) corpus of semantic role annotations of Tree-bank parses contained 112 ,917 labeled instances of 4 ,250 rolesets corresponding to 3 ,257 verbs , as illustrated by this example for the verb buyAn important area for future research will be to explore the correlation between our distance metric for syntactic similarity and various quantitative measures of semantic similarity ( )To prepare this corpus for analysis , we extracted the body text from each of the 4.1 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries ( )Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky ( ) , who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet ( )SVM ( ) is selected as our classifier and the one vsIn the context of it , more and more kernels for restricted syntaxes or specific domains ( ) are proposed and explored in the NLP domainIn this paper , we apply Alternating Structure Optimization (ASO) ( ) to the semantic role labeling task on NomBankASO has been shown to be effective on the following natural language processing tasks: text categorization , named entity recognition , part-of-speech tagging , and word sense disambiguation ( )For a more complete description , see ( )In this work , we use a modification of Huber\\'s robust loss function , similar to that used in ( ): L(p , y) �4py if py < �1 (1 � py) 2 if � 1 py< 1 (2) 0 if py > 1 We fix the regularization parameter A to 10 -4 , similar to that used in ( )This relationship is modeled by + 6 Tvi (3) The parameters [{w l , vi} , 6] may then be found by joint empirical risk minimization over all the m problems , i.e. , their values should minimize the combined empirical risk: l=i  V   i=l J (4) An important observation in ( ) is that the binary classification problems used to derive 6 are not necessarily those problems we are aiming to solveAssuming there are k target problems and m auxiliary problems , it is shown in ( ) that by performing one round of minimization , an approximate solution of 6 can be obtained from (4) by the following algorithm: 1. For each of the m auxiliary problems , learn u l as described by (1)This is a simplified version of the definition in ( ) , made possible because the same A is used for all auxiliary problemsASO has been demonstrated to be an effective semi-supervised learning algorithm ( )A variety of auxiliary problems are tested in ( ) in the semi-supervised settings , i.e. , their auxiliary problems are generated from unlabeled dataMore recently , for the word sense disambiguation (WSD) task , ( ) experimented with both supervised and semi-supervised auxiliary problems , although the auxiliary problems she used are different from oursIn recent years , the availability of large human-labeled corpora such as PropBank ( ) and FrameNet ( ) has made possible a statistical approach of identifying and classifying the arguments of verbs in natural language textsThis is known as multi-task learning in the machine learning literature ( )A large number of SRL systems have been evaluated and compared on the standard data set in the CoNLL shared tasks ( ) , and many systems have performed reasonably well In addition to the target outputs , ( ) discusses configurations where both used inputs and unused inputs (due to excessive noise) are utilized as additional outputsFirst , we train the various classifiers on sections 2 to 21 using gold argument labels and automatic parse trees produced by Charniak\\'s re-ranking parser ( ) , and test them on section 23 with automatic parse treesNoun predicates also appear in FrameNet semantic role labeling ( ) , and many FrameNet SRL systems are evaluated in Senseval-3 ( )So far we are aware of only one English NomBank-based SRL system ( ) , which uses the maximum entropy classifier , although similar efforts are reported on the Chinese NomBank by ( ) and on FrameNet by ( ) using a small set of hand-selected nominalizationsSecond , we achieve accuracy higher than that reported in ( ) and advance the state of the art in SRL researchEighteen baseline features and six additional features are proposed in ( ) for NomBank argument identificationUnlike in ( ) , we do not prune arguments dominated by other arguments or those that overlap with the predicate in the training dataThe J&N column presents the result reported in ( ) using both baseline and additional featuresA diverse set of 28 features is used in ( ) for argument classificationTo find a smaller set of effective features , we start with all the features considered in ( ) , in ( ) , and various combinations of them , for a total of 52 featuresThe J&N column presents the result reported in ( )This is the same configuration as reported in ( )Table 3: Fl scores of various classifiers on NomBank SRL Our maximum entropy classifier consistently outperforms ( ) , which also uses a maximum entropy classifierOur results outperform those reported in ( )With the recent release of NomBank ( ) , it becomes possible to apply machine learning techniques to the taskAccordingly , we do not maximize the probability of the entire labeled parse tree as in ( )Some approaches have used WordNet for the generalization step ( ) , others EM-based clustering ( )The argument positions for which we compute selec-tional preferences will be semantic roles in the FrameNet ( ) paradigm , and the predicates we consider will be semantic classes of words rather than individual words (which means that different preferences will be learned for different senses of a predicate word)We use FrameNet ( ) , a semantic lexicon for English that groups words in semantic classes called frames and lists semantic roles for each frameBrockmann and Lapata ( ) perform a comparison of WordNet-based modelsThe sim function can equally well be in-c)stantiated with a WordNet-based metric (for an overview see Budanitsky and Hirst ( )) , but we restrict our experiments to corpus-based metrics (a) in the interest of greatest possible sim cosine(w , w\\')   =    ,-      p   �  , =    sim Dice(w ,w\\')   =  ip/  Miiip)  a V  \\'    7 ^/E r p  f(w ,r p ) 2YE r p  f (w\\' ,r p )  2   Dic ^ V  \\'    7  \\\\  R(w) \\\\  +  \\\\  R(w\\' ) \\\\ sim  Lj n(w ,w\\') = ^�  p�  �n� - tt  � i �v  sim i accarH(w ,w\\') =   p  ) \\\\i  ,�  a slm nindie(w ,w /)   =    r  p sim Hindie(w , w\\' ,r p) where sim Hindle(w , w\\' , r p)  =   ^   abs(max(1 (w ,r p ) ,1 (w\\' ,r p)))   if I(w ,r p) <  0 and I (w\\' ,rp) <  0 Table 1: Similarity measures used resource-independence and (b) in order to be able to shape the similarity metric by the choice of generalization corpusIn SRL , the two most pressing issues today are (1) the development of strong semantic features to complement the current mostly syntactically-based systems , and (2) the problem of the domain dependence ( )The preference that r p has for a given synset co , the selectional association between the two , is then defined as the contribution of c 0 to r p\\'s selectional preference strength: A(rp ,C 0) = P (C 0|r p)log  ^gf* S (rp Further WordNet-based approaches to selec-tional preference induction include Clark and Weir ( ) , and Abe and Li ( )To determine headwords of the semantic roles , the corpus was parsed using the Collins ( ) parser5x2cv ( )They have been used for example for syntactic disambiguation ( ) , word sense disambiguation (WSD) ( ) and semantic role labeling (SRL) ( )While EM-based models have been shown to work better in SRL tasks ( ) , this has been attributed to the difference in coverageWe will be using the similarity metrics shown in Table 1: Cosine , the Dice and Jaccard coefficients , and Hindle\\'s ( ) and Lin\\'s ( ) mutual information-based metricsSelectional restrictions and selectional preferences that predicates impose on their arguments have long been used in semantic theories , (see e.g. ( ))It was parsed using Minipar ( ) , which is considerably faster than the Collins parser but failed to parse about a third of all sentencesIn this paper we propose a new , simple model for selectional preference induction that uses corpus-based semantic similarity metrics , such as Cosine or Lin\\'s ( ) mutual information-based metric , for the generalization stepThe corpus-based induction of selectional preferences was first proposed by Resnik ( )The induction of selectional preferences from corpus data was pioneered by Resnik ( )Rooth et al. ( ) generalize over seen headwords using EM-based clustering rather than WordNetExperimental design Like Rooth et al. ( ) we evaluate selectional preference induction approaches in a pseudo-disambiguation taskThe intuition that \"hard to learn\" examples are suspect corpus errors is not new , and appears also in Abney et al. ( ) , who consider the \"heaviest\" samples in the final distribution of the AdaBoost algorithm to be the hardest to classify and thus likely corpus errorsThe HEB  Err version of the corpus is obtained by projecting the chunk boundaries on the sequence of PoS and morphology tags obtained by the automatic PoS tagger of Adler & Elhadad ( )We tested this hypothesis by training the Error-Driven Pruning (EDP) method of ( ) with an extended set of featuresIn ( ) , we established that the task is not trivially transferable to Hebrew , but reported that SVM based chunking ( ) performs wellIn ( ) we argued that it is not applicable to Hebrew , mainly because of the prevalence of the Hebrew\\'s construct state (smixut)For the Hebrew experiments , we use the corpora of ( )These are the same settings as in ( )Refining the SimpleNP Definition: The hard cases analysis identified examples that challenge the SimpleNP definition proposed in Goldberg et al. ( )Kudo and Matsumoto ( ) used SVM as a classification engine and achieved an F-Score of 93.79 on the shared task NPsFurther details can be found in Kudo and Matsumoto ( )Following Ramshaw and Marcus ( ) , the current dominant approach is formulating chunking as a classification task , in which each word is classified as the (B)eginning , (I)nside or (O)outside of a chunkNP chunks in the shared task data are BaseNPs , which are non-recursive NPs , a definition first proposed by Ramshaw and Marcus ( )For the English experiments , we use the now-standard training and test sets that were introduced in ( ) 2This method is similar to the corpus error detection method presented by Nakagawa and Matsumoto ( )It is a well studied problem in English , and was the focus of CoNLL2000\\'s Shared Task ( )We applied this definition to the Hebrew Tree Bank ( ) , and constructed a moderate size corpus (about 5 ,000 sentences) for Hebrew SimpleNP chunkingSVM ( ) is a supervised binary classifierHowever , each of these assumes that the relations themselves are known in advance (implicitly or explicitly) so that the method can be provided with seed patterns ( ) , pattern-based rules ( ) , relation keywords ( ) , or word pairs exemplifying relation instances ( )Most related work deals with discovery of hypernymy ( ) , synonymy ( ) and meronymy ( )In addition to these basic types , several studies deal with the discovery and labeling of more specific relation sub-types , including inter-verb relations ( ) and noun-compound relationships ( )It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing ( ) and named entity tagging ( ) , while others take advantage of handcrafted databases such as WordNet ( ) and Wikipedia ( )In several studies ( ) it has been shown that relatively unsupervised and language-independent methods could be used to generate many thousands of sets of words whose semantics is similar in some senseWe do this as follows , essentially implementing a simplified version of the method of Davidov and Rappoport ( )Note that our method differs from that of Davidov and Rappoport ( ) in that here we provide an initial seed pair , representing our target concept , while there the goal is grouping of as many words as possible into concept classesIt was shown in ( ) that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is , they share some notable aspect of their semantics)Studying relationships between tagged named entities , ( ) proposed unsupervised clustering methods that assign given (or semi-automatically extracted) sets of pairs into several clusters , where each cluster corresponds to one of a known relationship typeA lot of this research is based on the initial insight ( ) that certain lexical patterns (\\'X is a country\\') can be exploited to automatically generate hyponyms of a specified wordIn some recent work ( ) , it has been shown that related pairs can be generated without pre-specifying the nature of the relation soughtFinally , ( ) provided a pattern distance measure which allows a fully unsupervised measurement of relational similarity between two pairs of words; however , relationship types were not discovered explicitlyThe bracketing guidelines ( ) also mention the considerable difficulty of identifying the correct scope for nominal modifiersWe use Bikel\\'s implementation ( ) of Collins\\' parser ( ) in order to carry out these experiments , using the non-deficient Collins settingsWe draw our counts from a corpus of n-gram counts calculated over 1 trillion words from the web ( )We use the Briscoe and Carroll ( ) version of DepBank , a 560 sentence subset used to evaluate the rasp parserWe map the brackets to dependencies by finding the head of the np , using the Collins ( ) head finding rules , and then creating a dependency between each other child\\'s head and this headWe dis-cretised the non-binary features using an implementation of Fayyad and Irani\\'s ( ) algorithm , and classify using MegaM 2For instance , CCGbank ( ) was created by semi-automatically converting the Treebank phrase structure to Combinatory Categorial Grammar (ccg) ( ) derivationsAn additional grammar rule is needed just to get a parse , but it is still not correct (Hockenmaier , 2003 , pWe check the correctness of the corpus by measuring inter-annotator agreement , by reannotating the first section , and by comparing against the sub -NP structure in DepBank ( )We used the PARC700 Dependency Bank ( ) which consists of 700 Section 23 sentences annotated with labelled dependenciesOur annotation guidelines 1 are based on those developed for annotating full sub -np structure in the biomedical domain ( )Lapata and Keller ( ) derive estimates from web counts , and only compare at a lexical level , achieving 78.7% accuracyFinally , we test the utility of the extended Treebank for training statistical models on two tasks: NP bracketing ( ) and full parsing ( )Lauer ( ) has demonstrated superior performance of the dependency model using a test set of 244 (216 unique) noun compounds drawn from Grolier\\'s encyclopediaWe implement a similar system to Table 4: Comparison of NP bracketing corpora Table 5: Lexical overlap Lauer ( ) , described in Section 3 , and report on results from our own data and Lauer\\'s original setThe np bracketing task has often been posed in terms of choosing between the left or right branching structure of three word noun compounds: (a)  (world (oil prices)) - Right-branching (b)  ((crude oil) prices) - Left-branching Most approaches to the problem use unsupervised methods , based on competing association strength between two of the words in the compound (Marcus , 1980 , pThe Penn Treebank ( ) is perhaps the most influential resource in Natural Language Processing (NLP)According to Marcus et al. ( ) , asking annota-tors to markup base -np structure significantly reduced annotation speed , and for this reason base- nps were left flatFor the original bracketing of the Treebank , anno-tators performed at 375-475 words per hour after a Table 1: Agreement between annotators few weeks , and increased to about 1000 words per hour after gaining more experience ( )Nakov and Hearst ( ) also use web counts , but incorporate additional counts from several variations on simple bigram queries , including queries for the pairs of words concatenated or joined by a hyphenWith our new data set , we began running experiments similar to those carried out in the literature ( )Many approaches to identifying base noun phrases have been explored as part of chunking ( ) , but determining sub -np structure is rarely addressedThe bracketing tool often suggests a bracketing using rules based mostly on named entity tags , which are drawn from the bbn corpus ( )The most common form of parser evaluation is to apply the parseval metrics to phrase-structure parsers based on the penn Treebank , and the highest reported scores are now over 90% ( )In this paper we evaluate a ccg parser ( ) on the Briscoe and Carroll version of DepBank ( )Briscoe and Carroll ( ) reannotated this resource using their grs scheme , and used it to evaluate the rasp parserParsers have been developed for a variety of grammar formalisms , for example hpsg ( ) , lfg ( ) , tag ( ) , ccg ( ) , and variants of phrase-structure grammar ( ) , including the phrase-structure grammar implicit in the Penn Treebank ( )And third , we provide the first evaluation of a wide-coverage ccg parser outside of CCGbank , obtaining impressive results on DepBank and outperforming the rasp parser ( ) by over 5% overall and on the majority of dependency typesFor the gold standard we chose the version of Dep-Bank reannotated by Briscoe and Carroll ( ) , consisting of 700 sentences from Section 23 of the Penn TreebankThe results in Table 4 were obtained by parsing the sentences from CCGbank corresponding to those in the 560-sentence test set used by Briscoe et al. ( )the macro-averaged scores are the mean of the individual scores for each relation ( )Can the ccg parser be compared with parsers other than rasp? Briscoe and Carroll ( ) give a rough comparison of rasp with the Parc lfg parser on the different versions of DepBank , obtaining similar results overall , but they acknowledge that the results are not strictly comparable because of the different annotation schemes usedBriscoe et al. ( ) split the 700 sentences in DepBank into a test and development set , but the latter only consists of 140 sentences which was not enough to reliably create the transformationAll the results were obtained using the RASP evaluation scripts , with the results for the rasp parser taken from Briscoe et al. ( )Preiss ( ) compares the parsers of Collins ( ) and Charniak ( ) , the gr finder of Buchholz et al. ( ) , and the rasp parser , using the Carroll et al. ( ) gold-standardIt has been argued that the parseval metrics are too forgiving and that phrase structure is not the ideal representation for a gold standard ( )Carroll et al. ( ) describe such a suite , consisting of sentences taken from the Susanne corpus , annotated with Grammatical Relations (grs) which specify the syntactic relation between a head and dependentWe chose not to use the corpus based on the Susanne corpus ( ) because the grs are less like the ccg dependencies; the corpus is not based on the Penn Treebank , making comparison more difficult because of tokenisation differences , for example; and the latest results for rasp are on DepBankparser evaluation has improved on the original parseval measures ( ) , but the challenge remains to develop a representation and evaluation suite which can be easily applied to a wide variety of parsers and formalismsClark and Curran ( ) describes the ccg parser used for the evaluationPrevious evaluations of ccg parsers have used the predicate-argument dependencies from CCGbank as a test set ( ) , with impressive results of over 84% F-score on labelled dependenciesKaplan et al. ( ) compare the Collins ( ) parser with the Parc lfg parser by mapping lfg F-structures and Penn Treebank parses into DepBank dependencies , claiming that the lfg parser is considerably more accurate with only a slight reduction in speedThe ccg parser results are based on automatically assigned pos tags , using the Curran and Clark ( ) taggerAn example of this is from CCGbank ( ) , where all modifiers in noun-noun compound constructions modify the final noun (because the penn Treebank , from which CCGbank is derived , does not contain the necessary information to obtain the correct bracketing)The grammar used by the parser is extracted from CCGbank , a ccg version of the Penn Treebank ( )Such conversions have been performed for other parsers , including parsers producing phrase structure output ( )Kaplan et al. ( ) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies , but they also note that \"This conversion was relatively straightforward for LFG structures ..In the case of Kaplan et al. ( ) , the testing procedure would include running their conversion process on Section 23 of the Penn Treebank and evaluating the output against DepBankA similar resource � the Parc Dependency Bank (DepBank) ( ) � has been created using sentences from the Penn TreebankThe b&c scheme is similar to the original DepBank scheme ( ) , but overall contains less grammatical detail; Briscoe and Carroll ( ) describes the differencesDifferent parsers produce different output , for ex-ample phrase structure trees ( ) , dependency trees ( ) , grammatical relations ( ) , and formalism-specific dependencies ( )The grammar consists of 425 lexical categories � expressing subcategorisation information � plus a small number of combinatory rules which combine the categories ( )A more interesting statement would be that it makes learning easier , along the lines of the result of ( ) � note , however , that their results are for the \"semi-supervised\" domain adaptation problem and so do not apply directlyA part-of-speech tagging problem on PubMed abstracts introduced by Blitzer et al. ( )The first model , which we shall refer to as the Prior model , was first introduced by Chelba and Acero ( )This is a recapitalization task introduced by Chelba and Acero ( ) and also used by Daume III and Marcu ( )For the CNN-Recap task , we use identical feature to those used by both Chelba and Acero ( ) and Daume III and Marcu ( ): the current , previous and next word , and 1-3 letter prefixes and suffixesMany of these are presented and evaluated by Daume III and Marcu ( )Daume III and Marcu ( ) provide empirical evidence on four datasets that the Prior model outperforms the baseline approachesMore recently , Daume III and Marcu ( ) presented an algorithm for domain adaptation for maximum entropy classifiersWe additionally ran the MegaM model ( ) on these data (though not in the multi-conditional case; for this , we considered the single source as the union of all sources)In all cases , we use the S earn algorithm for solving the sequence labeling problem ( ) with an underlying averaged perceptron classifier; implementation due to ( )Second , it is arguable that a measure like F 1 is inappropriate for chunking tasks ( )Following ( ) , we call the first the source domain , and the second the target domainRecently there have been some studies addressing domain adaptation from different perspectives ( )The POS data set and the CTS data set have previously been used for testing other adaptation methods ( ) , though the setup there is different from oursBlitzer et al. ( ) propose a domain adaptation method that uses the unlabeled target instances to infer a good feature representation , which can be regarded as weighting the featuresChelba and Acero ( ) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target dataThe setup is very similar to Daume III and Marcu ( )els the different distributions in the source and the target domains is by Daume III and Marcu ( )Florian et al. ( ) first train a NE tagger on the source domain , and then use the tagger\\'s predictions as features for training and testing on the target domainThis way of setting 7 corresponds to the entropy minimization semi-supervised learning method ( )For generative syntactic parsing , Roark and Bac-chiani ( ) have used the source domain data to construct a Dirichlet prior for MAP estimation of the PCFG for the target domainber of hidden components is not fixed , but emerges We begin by presenting three finite tree models , each naturally from the training data ( )The closely related infinite hidden Markov model is an HMM in which the transitions are modeled using an HDP , enabling unsupervised learning of sequence models when the number of hidden states is unknown ( )The infinite hidden Markov model (iHMM) or HDP-HMM ( ) is a model of sequence data with transitions modeled by an HDPThis is useful , because coarse-grained syntactic categories , such as those used in the Penn Treebank (PTB) , make insufficient distinctions to be the basis of accurate syntactic parsing ( )Hence , state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves ( ) , manually split the tagset into a finer-grained one ( ) , or learn finer grained tag distinctions using a heuristic learning procedure ( )But the introduction of nonparametric priors such as the Dirichletprocess ( ) enabled development of infinite mixture models , in which the num-Teh et al. ( ) proposed the hierarchical Dirichlet process (HDP) as a way of applying the Dirichlet process (DP) to more complex model forms , so as to allow multiple , group-specific , infinite mixture models to share their mixture components8 Additionally , we compute the mutual information of the learned clusters with the gold tags , and we compute the cluster F-score ( )First , we use the standard approach of greedily assigning each of the learned classes to the POS tag with which it has the greatest overlap , and then computing tagging accuracy ( )For comparison , Haghighi and Klein ( ) report an unsupervised baseline of 41.3% , and a best result of 80.5% from using hand-labeled prototypes and distributional similarityEarlier , Johnson et al. ( ) presented adaptor grammars , which is a very similar model to the HDP-PCFGWe use the generative dependency parser distributed with the Stanford factored parser ( ) for the comparison , since it performs simultaneous tagging and parsing during testingThe HDP-PCFG ( ) , developed at the same time as this work , aims to learn state splits for a binary-branching PCFGIn contrast , Liang et al. ( ) define a global DP over sequences , with the base measure defined over the global state probabilities ,  0; locally , each state has an HDP , with this global DP as the base measureFor both experiments , we used dependency trees extracted from the Penn Treebank ( ) using the head rules and dependency extractor from Yamada and Matsumoto ( )To generate  n we first generate an infinite sequence of variables  n\\' = (n k each of which is distributed according to the Beta distribution: Then  n = (n k)� = = 1 is defined as: (1 Following Pitman ( ) we refer to this process as n � GEM (a 0)Teh , 2006 , p.c.) , to sample each m jk: sampleM (j , k) 1  if n jk = 0 2  then m jk = 0 3  else m jk = 1 4  for i ^ 2 to n jk 5  doifrand ()<^^T 6  then m jk = m jk + 1 7  return m jk Sampling /? In many cases , improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology ( )This follows a conceptually similar approach by ( ) that uses a large named-entity dictionary , where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classifierTherefore , an increasing attention has been recently given to semi-supervised learning , where large amounts of unlabeled data are used to improve the models learned from a small training set ( )This was used , for example , by ( ) in information extraction , and by ( ) in POS taggingThis decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs , in which case the linear sum corresponds to log likelihood assigned to the input/output pair by the model (for details see ( ) for the classification case and ( ) for the structured case)For example , ( ) proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labelsThe second problem we consider is extracting fields from advertisements ( )( ) and ( ) also report results for semi-supervised learning for these domains( ) extends the dictionary-based approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarityWe implement some global constraints and include unary constraints which were largely imported from the list of seed words used in ( )( ) also worked on one of our data sets1 The first task is to identify fields from citations ( ) Another way to look the algorithm is from the self-training perspective ( )However , in the general case , semi-supervised approaches give mixed results , and sometimes even degrade the model performance ( )( ) has suggested to balance the contribution of labeled and unlabeled data to the parameters( )This confirms results reported for the supervised learning case in ( )On the other hand , in the supervised setting , it has been shown that incorporating domain and problem specific structured information can result in substantial improvements ( )However ( ) showed that reasoning with more expressive , non-sequential constraints can improve the performance for the supervised protocolWe note that in the presence of constraints , the inference procedure (for finding the output y that maximizes the cost function) is usually done with search techniques (rather than Viterbi decoding , see ( ) for a discussion) , we chose beamsearch decodingWhile ( ) showed the significance of using hard constraints , our experiments show that using soft constraints is a superior optionConceptually , although not technically , the most related work to ours is ( ) that , in a somewhat ad-hoc manner uses soft constraints to guide an unsupervised model that was crafted for mention trackingCrucially , the kind of lexical descriptions that we employ are those that are commonly devised within lexicon-driven approaches to linguistic syntax , e.g.Lexicalized Tree-Adjoining Grammar ( ) and Combinary Categorial Grammar ( )There are currently two supertagging approaches available: LTAG-based ( ) and CCG-based ( )One important way of portraying such lexical descriptions is via the supertags devised in the LTAG and CCG frameworks ( )The term \"supertagging\" ( ) refers to tagging the words of a sentence , each with a supertagThe LTAG-based supertagger of ( ) is a standard HMM tagger and consists of a (second-order) Markov language model over supertags and a lexical model conditioning the probability of every word on its own supertag (just like standard HMM-based POS taggers)For the LTAG supertags experiments , we used the LTAG English supertagger 5 ( ) to tag the English part of the parallel data and the supertag language model dataAkin to POS tagging , the process of supertagging an input utterance proceeds with statistics that are based on the probability of a word-supertag pair given their Markovian or local context ( )Besides the difference in probabilities and statistical estimates , these two supertaggers differ in the way the supertags are extracted from the Penn Treebank , cf. ( )Only quite recently have ( ) and ( ) shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT systemAmong the first to demonstrate improvement when adding recursive structure was ( ) , who allows for hierarchical phrase probabilities that handle a range of reordering phenomena in the correct fashionThe CCG supertagger ( ) is based on log-linear probabilities that condition a supertag on features representing its contextFor the CCG supertag experiments , we used the CCG supertagger of ( ) and the Edinburgh CCG tools 6 to tag the English part of the parallel corpus as well as the CCG supertag language model dataBoth the LTAG ( ) and the CCG supertag sets ( ) were acquired from the WSJ section of the Penn-II Treebank using hand-built extraction rulesDecoder The decoder used in this work is Moses , a log-linear decoder similar to Pharaoh ( ) , modified to accommodate supertag phrase probabilities and supertag language modelsWithin the field of Machine Translation , by far the most dominant paradigm is Phrase-based Statistical Machine Translation (PBSMT) ( )For example , ( ) demonstrated that adding syntax actually harmed the quality of their SMT systemThe bidirectional word alignment is used to obtain phrase translation pairs using heuristics presented in ( ) and ( ) , and the Moses decoder was used for phrase extraction and decodingThe bidirectional word alignment is used to obtain lexical phrase translation pairs using heuristics presented in ( ) and ( )Coming right up to date , ( ) demonstrate that \\'syntactified\\' target language phrases can improve translation quality for Chinese-EnglishWhile the research of ( ) has much in common with the approach proposed here (such as the syntactified target phrases) , there remain a number of significant differencesThe NIST MT03 test set is used for development , particularly for optimizing the interpolation weights using Minimum Error Rate training ( )Firstly , rather than induce millions of xRS rules from parallel data , we extract phrase pairs in the standard way ( ) and associate with each phrase-pair a set of target language syntactic structures based on supertag sequencesTable 1 presents the BLEU scores ( ) of both systems on the NIST 2005 MT Evaluation test setFor less commonly used languages , one might use open source research systems ( )Also relevant is previous work that applied machine learning approaches to MT evaluation , both with human references ( ) and without ( )METEOR uses the Porter stemmer and synonym-matching via WordNet to calculate recall and precision more accurately ( )As its loss function , support vector regression uses an e-insensitive error function , which allows for errors within a margin of a small positive value , e , to be considered as having zero error (cf.Bishop ( ) , pp.339-344)This can be seen as a form of confidence estimation on MT outputs ( )To remove the bias in the distributions of scores between different judges , we follow the normalization procedure described by Blatz et al. ( )We conducted experiments to determine the feasibility of the proposed approach and to address the following questions: (1) How informative are pseudo references in-and-of themselves? Does varying the number and/or the quality of the references have an impact on the metrics? (2) What are the contributions of the adequacy features versus the fluency features to the learning-based metric? (3) How do the quality and distribution of the training examples , together with the quality of the pseudo references , impact the metric training? (4) Do these factors impact the metric\\'s ability in assessing sentences produced within a single MT system? How does that system\\'s quality affect metric performance? The implementation of support vector regression used for these experiments is SVM-Light ( )To compare the relative quality of different metrics , we apply bootstrapping re-sampling on the data , and then use paired t-test to determine the statistical significance of the correlation differences ( )ROUGE utilizes \\'skip n-grams\\' , which allow for matches of sequences of words that are not necessarily adjacent ( )BLEU is smoothed ( ) , and it considers only matching up to bigrams because this has higher correlations with human judgments than when higher-ordered n-grams are includedThe HWC metrics compare dependency and constituency trees for both reference and machine translations ( )In addition to adapting the idea of Head Word Chains ( ) , we also compared the input sentence\\'s argument structures against the treebank for certain syntactic categoriesReference-based metrics such as BLEU ( ) have rephrased this subjective task as a somewhat more objective question: how closely does the translation resemble sentences that are known to be good translations for the same source? This approach requires the participation of human translators , who provide the \"gold standard\" reference sentencesThe relationship between word alignments and their impact on MT is also investigated in ( )Most current statistical models ( ) treat the aligned sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target wordsTo quickly (and approximately) evaluate this phenomenon , we trained the statistical IBM word-alignment model 4 ( ) , 1 using the GIZA++ software ( ) for the first two language pairs , and the Europarl corpus ( ) for the last oneThey can be seen as extensions of the simpler IBM models 1 and 2 ( )We use a standard log-linear phrase-based statistical machine translation system as a baseline: GIZA++ implementation of IBM word alignment model 4 ( ) , 8 the refinement and phrase-extraction heuristics described in ( ) , minimum-error-rate training Table 2: Chinese-English corpus statistics ( ) using Phramer ( ) , a 3-gram language model with Kneser-Ney smoothing trained with SRILM ( ) on the English side of the training data and Pharaoh ( ) with default settings to decodeWe also want to bootstrap on different word aligners; in particular , one possibility is to use the flexible HMM word-to-phrase model of Deng and Byrne ( ) in place of IBM model 4We evaluate the reliability of these candidates , using simple metrics based on co-occurence frequencies , similar to those used in associative approaches to word alignment ( )Second , an increase in AER does not necessarily imply an improvement in translation quality ( ) and vice-versa ( )This very simple measure is frequently used in associative approaches ( )%: there is want to need not I^iS: in front of �: as soon as ;#: look at Figure 2: Examples of entries from the manually developed dictionary The intrinsic quality of word alignment can be assessed using the Alignment Error Rate (AER) metric ( ) , that compares a system\\'s alignment output to a set of gold-standard alignmentThe quality of the translation output is evaluated using BLEU ( )The experiments were carried out using the Chinese-English datasets provided within the IWSLT 2006 evaluation campaign ( ) , extracted from the Basic Travel Expression Corpus (BTEC) ( )For Chinese , the data provided were tokenized according to the output format of ASR systems , and human-corrected ( )Note that the need to consider segmentation and alignment at the same time is also mentioned in ( ) , and related issues are reported in ( )More importantly , however , this segmentation is often performed in a monolingual context , which makes the word alignment task more difficult since different languages may realize the same concept using varying numbers of words (see e.g. ( ))The log-linear model is also based on standard features: conditional probabilities and lexical smoothing ofphrases in both directions , and phrase penalty ( )To test the influence of the initial word segmentation on the process of word packing , we considered an additional segmentation configuration , based on an automatic segmenter combining rule-based and statistical techniques ( )These resources follow more or less the same format as the output of the word segmenter mentioned in Section 5.1.2 ( ) , so the experiments are carried out using this segmentationIt has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision ( )Recently , confusion network decoding for MT system combination has been proposed ( )Powell\\'s method ( ) is used to tune the system and feature weights simultaneously so as to optimize various automatic evaluation metrics on a development setIn this work , modified Powell\\'s method as proposed by ( ) is usedSix MT systems were combined: three (A ,C ,E) were phrase-based similar to ( ) , two (B ,D) were hierarchical similar to ( ) and one (F) was syntax-based similar to ( )Combination of speech recognition outputs is an example of this approach ( )Also , a more heuristic alignment method has been proposed in a different system combination approach ( )In speech recognition , confusion network decoding ( ) has become widely used in system combinationIn ( ) , different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ ( )Tuning is fully automatic , as opposed to ( ) where global system weights were set manuallySimilar combination of multiple confusion networks was presented in ( )The same Powell\\'s method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in ( )The optimization of the system and feature weights may be carried out using -best lists as in ( )Currently , the most widely used automatic MT evaluation metric is the NIST BLEU-4 ( )This work was extended in ( ) by introducing system weights for word confidencesIn ( ) , simple score was assigned to the word coming from the th-best hypothesisIn ( ) , the total confidence of the nth best confusion network hypothesis  , including NULL words , given the th source sentence  was given by where is the number of nodes in the confusion network for the source sentence  , is the number of translation systems , is the th system weight ,  c wn is the accumulated confidence for word produced by system between nodes and  , and is a weight for the number of NULL links along the hypothesis The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in ( ) on the Arabic to English and Chinese to English NIST MT05 tasksCompared to the baseline from ( ) , the new method improves the BLEU scores significantlyIn ensemble learning , a collection of simple classifiers is used to yield better performance than any single classifier; for example boosting ( )A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) ( ) was used to align hy-potheses in ( )Minimum Bayes risk (MBR) was used to choose the skeleton in ( )This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities ( )It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output ( )Translation edit rate (TER) ( ) has been proposed as more intuitive evaluation metric since it is based on the rate of edits required to transform the hypothesis into the referenceHowever , this would require time consuming evaluations such as human mediated TER post-editing ( )The TnT tagger ( ) and the TreeTagger ( ) are used for tagging and lemmatizationMotivated by the theoretical work by Chafe ( ) and Jacobs ( ) , we view the VF as the place for elements which modify the situation described in the sentence , i.eFinally , the articles are parsed with the CDG dependency parser ( )The preferences summarized below have mo-tivated our choice of features: � constituents in the nominative case precede those in other cases , and dative constituents often precede those in the accusative case ( ); � the verb arguments\\' order depends on the verb\\'s  subcategorization properties ( ); � constituents with a definite article precede those with an indefinite one ( ); � pronominalized constituents precede non-pronominalized ones ( ) ; � animate referents precede inanimate ones ( ); � short constituents precede longer ones ( ); � the preferred topic position is right after the verb ( ); � the initial position is usually occupied by scene-setting elements and topics ( ) The sentence-initial position , which in German is the VF , has been shown to be cognitively more prominent than other positions ( )Inspired by the findings of the Prague School ( ) and Systemic Functional Linguistics ( ) , they focus on the role that information structure plays in constituent orderingHarbusch et al. ( ) present a generation workbench , which has the goal of producing not the most appropriate order , but all grammatical onesWe suppose that this dificulty comes from the double function of the initial position which can either introduce the ad-dressation topic , or be the scene- or frame-setting position ( )We hypothesize that the reasons which bring a constituent to the VF are different from those which place it , say , to the beginning of the MF , for the order in the MF has been shown to be relatively rigid ( )Since our learner treats all values as nominal , we discretized the values of dep and len with a C4.5 classifier ( )Kruijff et al. ( ) describe an architecture which supports generating the appropriate word order for different languagesKruijff-Korbayova et al. ( ) address the task of word order generation in the same veinSimilar to Langkilde & Knight ( ) we utilize statistical methodsKendall\\'s t , which has been used for evaluating sentence ordering tasks ( ) , is the second metric we useE.g. , in text-to-text generation ( ) , new sentences are fused from dependency structures of input sentencesRingger et al. ( ) aim at regenerating the order of constituents as well as the order within them for German and French technical manualsSimilar to Ringger et al. ( ) , we find the order with the highest probability conditioned on syntactic and semantic categoriesApart from acc and t , we also adopt the metrics used by Uchimoto et al. ( ) and Ringger et al. ( )According to the inv metric , our results are considerably worse than those reported by Ringger et al. ( )We retrained our system on a corpus of newspaper articles ( ) which is manually annotated but encodes no semantic knowledgeThe work of Uchimoto et al. ( ) is done on the free word order language JapaneseFor the fourth baseline (UCHIMOTO) , we utilized a maximum entropy learner (OpenNLP 8) and reim-plemented the algorithm of Uchimoto et al. ( )Uszkoreit ( ) addresses the problem from a mostly grammar-based perspective and suggests weighted constraints , such as [+nom] -< [+dat] , [+pro] -< [-pro] , [-focus] -< [+focus] , etcUnlike overgeneration approaches ( ) which select the best of all possible outputs ours is more efficient , because we do not need to generate every permutationIt also compares reasonably with other more recent evaluations ( ) which derive their input data from the penn Treebank by transforming each sentence tree into a format suitable for the realiser ( )For instance , ( ) reports that the implementation of such a processor for Surge was the most time consuming part of the evaluation with the resulting component containing 4000 lines of code and 900 rulesThe realiser presented here differs in mainly two ways from existing reversible realisers such as ( )\\'s CCG system or the HPSG ERG based realiser ( )The reason for this is that the grammar is compiled from a higher level description where tree fragments are first encapsulated into so-called classes and then explicitly combined (by inheritance , conjunction and disjunction) to produce the grammar elementary trees (cf. ( ))Thus for instance , both REALPRO ( ) and Surge ( ) assume that the input associates semantic literals with low level syntactic and lexical information mostly leaving the realiser to just handle inflection , word order , insertion of grammatical words and agreementTo associate semantic representations with natural language expressions , the FTAG is modified as proposed in ( )The proposal draws on ideas from ( ) and aims to determine whether for a given input (a set of TAG elementary trees whose semantics equate the input semantics) , syntactic requirements and resources cancel outIt could be used for instance , in combination with the parser and the semantic construction module described in ( ) , to support textual entailment recognition or answer detection in question answeringWe rely on these features to associate one and the same semantic to large sets of trees denoting semantically equivalent but syntactically distinct configurations (cf. ( ))The basic surface realisation algorithm used is a bottom up , tabular realisation algorithm ( ) optimised for TAGsSimilarly , KPML ( ) assumes access to ideational , interpersonal and textual information which roughly corresponds to semantic , mood/voice , theme/rheme and focus/ground informationIn order to ensure this determinism , NLG geared realisers generally rely on theories of grammar which systematically link form to function such as systemic functional grammar (SFG , ( )) and , to a lesser extent , Meaning Text Theory (MTT , ( ))First , the paraphrase figures might seem low wrt to e.g. , work by ( ) which mentions several thousand outputs for one given input and an average number of realisations per input varying between 85.7 and 102.2This does not seem to be the case in ( )\\'s approach where the count seems to include all sentences associated by the grammar with the input semanticsA Feature-based TAG (FTAG , ( )) consists of a set of (auxiliary or initial) elementary trees and of two tree composition operations: substitution and ad-junctionA first possibility would be to draw on ( )\\'s proposal and compute the enriched input based on the traversal of a systemic networkThus for instance , ( ) resorts to ad hoc \"mapping tables\" to associate substitution nodes with semantic indices and \"fr-nodes\" to constrain adjunction to the correct nodesWhile there have been previous systems that encode generation as planning ( ) , our approach is distinguished from these systems by its focus on the grammatically specified contributions of each individual word (and the TAG tree it anchors) to syntax , semantics , and local pragmatics ( )It also allows us to benefit from the past and ongoing advances in the performance of off-the-shelf planners ( )Unlike some approaches ( ) , we do not have to distinguish between generating NPs and expressions of other syntactic categoriesThe context set of an intended referent is the set of all individuals that the hearer might possibly confuse it with ( )It is based on the well-known STRIPS language ( )The grammar formalism we use here is that of lex-icalized tree-adjoining grammars (LTAG; Joshi and Schabes ( ))In order to use the planner as a surface realization algorithm for TAG along the lines of Koller and Striegnitz ( ) , we attach semantic content to each elementary tree and require that the sentence achieves a certain communicative goalHowever , this problem is NP-complete , by reduction of Hamiltonian Cycle - unsurprisingly , given that it encompasses realization , and the very similar realization problem in Koller and Striegnitz ( ) is NP-hardPDDL ( ) is the standard input language for modern planning systemsIn a scenario that involves multiple rabbits , multiple hats , and multiple individuals that are inside other individuals , but only one pair of a rabbit r inside a hat h , the expression \"X takes the rabbit from the hat\" is sufficient to refer uniquely to r and h ( )We share these advantages with systems such as SPUD ( )This makes our encoding more direct and transparent than those in work like Thomason and Hobbs ( ) and Stone et al. ( )We follow Stone et al. ( ) in formalizing the semantic content of a lexicalized elementary tree t as a finite set of atoms; but unlike in earlier approaches , we use the semantic roles in t as the arguments of these atomsThe three pragmatic predicates that we will use here are hearer-new , indicating that the hearer does not know about the existence of an individual and can\\'t infer it ( ) , hearer-old for the opposite , and contextsetIn addition to the semantic content , we equip every elementary tree in the grammar with a semantic requirement and a pragmatic condition ( )Supertag This is a variant of the approach above , but using supertags ( ) instead of PoS tagsFor example , the metrics proposed in Bangalore et al. ( ) , such as Simple Accuracy and Generation Accuracy , measure changes with respect to a reference string based on the idea of string-edit distanceThe judges were then presented with the 50 sentences in random order , and asked to score the sentences according to their own scale , as in magnitude estimation ( ); these scores were then normalised in the range [0 ,1]Regarding the interpretation of the absolute value of (Pearson\\'s) correlation coefficients , both here and in the rest of the paper , we adopt Cohen\\'s scale ( ) for use in human judgements , given in Table 1; we use this as most of this work is to do with human judgements of fluencyThose chosen were the Connexor parser , 2 the Collins parser ( ) , and the Link Grammar parser ( )For example , in statistical MT the translation model and the language model are treated separately , characterised as faithfulness and fluency respectively (as in the treatment in Jurafsky and Martin ( ))A neat solution to poor sentence-level evaluation proposed by Kulesza and Shieber ( ) is to use a Support Vector Machine , using features such as word error rate , to estimate sentence-level translation qualityBleu ( ) is a canonical example: in matching n-grams in a candidate translation text with those in a reference text , the metric measures faithfulness by counting the matches , and fluency by implicitly using the reference n-grams as a language modelQuite a different idea was suggested in Wan et al. ( ) , of using the grammatical judgement of a parser to assess fluency , giving a measure independent of the language model used to generate the textIn terms of automatic evaluation , we are not aware of any technique that measures only fluency or similar characteristics , ignoring content , apart from that of Wan et al. ( )The consistency and magnitude of the first three parser metrics , however , lends support to the idea of Wan et al. ( ) to use something like these as indicators of generated sentence fluencySimilarly , the ultrasummarisa-tion model of Witbrock and Mittal ( ) consists of a content model , modelling the probability that a word in the source text will be in the summary , and a language modelIn this model we violate the Markov assumption of independence in much the same way as Witbrock and Mittal ( ) in their combination of content and language model probabilities , by backtracking at every state in order to discourage repeated words and avoid loopsZajic et al. ( ) use similar scales for summarisationCoreference resolution on text datasets is well-studied (e.g. , ( ))We employ a set of verbal features that is similar to the features used by state-of-the-art coreference resolution systems that operate on text (e.g. , ( ))Evaluation metric Coreference resolution is often performed in two phases: a binary classification phase , in which the likelihood of corefer-ence for each pair of noun phrases is assessed; and a partitioning phase , in which the clusters of mutually-coreferring NPs are formed , maximizing some global criterion ( )The verbal features that we have included are a representative sample from the literature (e.g. , ( ))also consider training separate classifiers and combining their posteriors , either through weighted addition or multiplication; this is sometimes called \"late fusion.\" Late fusion is also employed for gesture-speech combination in ( )All features are computed from hand and body pixel coordinates , which are obtained via computer vision; our vision system is similar to ( )The continuous-valued features were binned using a supervised technique ( )While people have little difficulty distinguishing between meaningful gestures and irrelevant hand motions (e.g. , self-touching , adjusting glasses) ( ) , NLP systems may be confused by such seemingly random movementsMarkable noun phrases - those that are permitted to participate in coreference relations - were annotated by the first author , in accordance with the MUC task definition ( )To measure the similarity between gesture trajectories , we use dynamic time warping ( ) , which gives a similarity metric for temporal data that is invariant to speedIn addition , verbal language is different when used in combination with meaningful non-verbal communication than when it is used unimodally ( )Kehler finds that fully-specified noun phrases are less likely to receive multimodal support ( )Last , we note that NPs with adjectival modifiers were assigned negative weights , supporting the finding of ( ) that fully-specified NPs are less likely to receive multimodal supportExperiments in both ( ) and ( ) find no conclusive winner among early fusion , additive late fusion , and multiplicative late fusionJS-div reports the Jensen-Shannon divergence , a continuous-valued feature used to measure the similarity in cluster assignment probabilities between the two gestures ( )The objective function (Equation 1) is optimized using a Java implementation of L-BFGS , a quasiNewton numerical optimization technique ( )However , non-verbal modalities are often noisy , and their interactions with speech are complex ( )Our non-verbal features attempt to capture similarity between the speaker\\'s hand gestures; similar gestures are thought to suggest semantic similarity ( )Euclidean distance captures cases in which the speaker is performing a gestural \"hold\" in roughly the same location ( )Non-verbal meta features Research on gesture has shown that semantically meaningful hand motions usually take place away from \"rest position ,\" which is located at the speaker\\'s lap or sides ( )Indeed , the psychology literature describes a finite-state model of gesture , proceeding from \"preparation ,\" to \"stroke ,\" \"hold ,\" and then \"retraction\" ( )Verbal meta features Meaningful gesture has been shown to be more frequent when the associated speech is ambiguous ( )The use of hidden variables in a conditionally-trained model follows ( )For example , Shriberg et al. ( ) explore the use of prosodic features for sentence and topic segmentationWhile more flexible than the interpolation techniques described in ( ) , training modality-specific classifiers separately is still suboptimal compared to training them jointly , because independent training of the modality-specific classifiers forces them to account for data that they cannot possibly explainToyama and Horvitz ( ) introduce a Bayesian network approach to modality combination for speaker identificationIntroduction With recent advances in spoken dialogue system technologies , researchers have turned their attention to more complex domains (e.g.tutoring ( ) , technical support ( ) , medication assistance ( ))Average (standard deviation) for objective metrics in the first problem Related work Discourse structure has been successfully used in non-interactive settings (e.g.understanding specific lexical and prosodic phenomena ( )  , natural language generation ( ) , essay scoring ( ) as well as in interactive settings (e.g.predictive/generative models of postural shifts ( ) , generation/interpretation of anaphoric expressions ( ) , performance modeling ( ))Other visual improvements for dialogue-based computer tutors have been explored in the past (e.g.talking heads ( ))This information is implicitly encoded in the intentional structure of a discourse as proposed in the Grosz & Sidner theory of discourse ( )3  The Navigation Map (NM) We use the Grosz & Sidner theory of discourse ( ) to inform our NM design2 ITSPOKE ITSPOKE ( ) is a state-of-the-art tutoring spoken dialogue system for conceptual physicsThus , interacting with such systems can be characterized by an increased user cognitive load associated with listening to often lengthy system turns and the need to integrate the current information to the discussion overall ( )However , implementing the NM in a new domain requires little expertise as previous work has shown that na�ve users can reliably annotate the information needed for the NM ( )While a somewhat similar graphical representation of the discourse structure has been explored in one previous study ( ) , to our knowledge we are the first to test its benefits (see Section 6)This theory has inspired several generic dialogue managers for spoken dialogue systems (e.g. ( ))One related study is that of ( )Results for Q1-6 Questions Q1-6 were inspired by previous work on spoken dialogue system evaluation (e.g. ( )) and measure user\\'s overall perception of the systemThis situation is very similar to the training process of translation models in statistical machine translation ( ) , where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns   J^Pr(Wj |o k) = 1 , Vk j =1 This optimization problem can be solved by the EM algorithm ( )Studies have also shown that eye gaze has a potential to improve resolution of underspecified referring expressions in spoken dialog systems ( ) and to disambiguate speech input ( )Given the recent advances in eye tracking technology ( ) , integrating non-intrusive and high performance eye trackers with conversational interfaces becomes feasibleMotivated by psycholinguistic studies ( ) and recent investigations on computational models for language acquisition and grounding ( ) , we are particularly interested in two unique questions related to multimodal conversational systems: (1) In a multimodal conversation that involves more complex tasks (e.g. , both user initiated tasks and system initiated tasks) , is there a reliable temporal alignment between eye gaze and spoken references so that the coupled inputs can be used for automated vocabulary acquisition and interpretation? (2) If such an alignment exists , how can we model this alignment and automatically acquire and interpret the vocabularies? To address the first question , we conducted an empirical study to examine the temporal relationships between eye fixations and their corresponding spoken referencesAdditionally , before speaking a word , the eyes usually move to the objects to be mentioned ( )Previous psycholinguistics studies have shown that the direction of gaze carries information about the focus of the user\\'s attention ( )In research on multimodal interactive systems , recent work indicates that the speech and gaze integration patterns can be modeled reliably for individual users and therefore be used to improve multimodal system performances ( )In addition , visual properties of the interface also affect user gaze behavior and thus influence the predication of attention ( ) based on eye gazeRecent work has shown that the effect of eye gaze in facilitating spoken language processing varies among different users ( )Recent studies have shown that multisensory information (e.g. , through vision and language processing) can be combined to effectively acquire words to their perceptually grounded objects in the environment ( )The perceived visual context influences spoken word recognition and mediates syntactic processing ( )Figure 1 Multimodal interface on tablet In this paper we explore the application of multimodal interface technologies (See Andr� ( ) for an overview) to the creation of more effective systems used to search and browse for entertainment content in the homeThese interfaces are cumbersome and do not scale well as the range of content available increases ( )An important advantage of speech is that it makes it easy to combine multiple constraints over multiple dimensions within a single query ( )A number of previous systems have investigated the addition of unimodal spoken search queries to a graphical electronic program guide ( ); Goto et al. , 2003; Wittenburg et al. , 2006)Others have gone beyond unimodal speech input and added multimodal commands combining speech with pointing ( )This develops and extends upon the multimodal architecture underlying the MATCH system ( )Speech recognition results , pointing gestures made on the display , and handwritten inputs , are all passed to a multimodal understanding server which uses finite-state multimodal language proc-essing techniques ( ) to interpret and integrate the speech and gestureHowever , as also reported in previous work ( ) , recognition accuracy remains a serious problemThe past few years have seen considerable improvement in the performance of unsupervised parsers ( ) and , for the first time , unsupervised parsers have been able to improve on the right-branching heuristic for parsing EnglishSome of these subsets were used for scoring in ( )Table 1 gives two baselines and the parsing results for WSJ10 , WSJ40 , Negra10 and Negra40 for recent unsupervised parsing algorithms: CCM and DMV+CCM ( ) , U-DOP ( ) and UML-DOP ( )There are several algorithms for doing so ( ) , which cluster words into classes based on the most frequent neighbors of each wordThis restriction is inspired by psycholin-guistic research which suggests that humans process language incrementally ( )When Klein and Manning induce the parts-of-speech , they do so from a much larger corpus containing the full WSJ treebank together with additional WSJ newswire ( )This can either be semi-supervised parsing , using both annotated and unannotated data ( ) or unsupervised parsing , training entirely on unan-notated textThis problem is known in psycholinguistics as the problem of reanalysis ( )For large datasets , we use an ensemble technique inspired by Bagging ( )In particular , we consider an algorithm proposed by Camerini et al. ( ) which has a worst-case complexity of O(km log(n)) , where k is the number of parses we want , n is the number of words in the input sentence , and m is the number of edges in the hypothesis graphThe k-best MST algorithm we introduce in this paper is the algorithm described in Camerini et al. ( )Algorithm 1 is a version of the MST algorithm as presented by Camerini et al. ( ); subtleties of the algorithm have been omittedWe have introduced the Camerini et al. ( ) k-best MST algorithm and have shown how to efficiently train MaxEnt models for dependency parsingMany of the model features have been inspired by the constituency-based features presented in Charniak and Johnson ( )Other DP solutions use constituency-based parsers to produce phrase-structure trees , from which dependency structures are extracted ( )An efficient algorithm for generating the k-best parse trees for a constituency-based parser was presented in Huang and Chiang ( ); a variation of that algorithm was used for generating projective dependency trees for parsing in Dreyer et al. ( ) and for training in McDonald et al. ( )The DP algorithms are generally variants of the CKY bottom-up chart parsing algorithm such as that proposed by Eisner ( )2 In order to explore a rich set of syntactic features in the MST framework , we can either approximate the optimal non-projective solution as in McDonald and Pereira ( ) , or we can use the constrained MST model to select a subset of the set of dependency parses to which we then apply less-constrained modelsUnlike the training procedure employed by McDonald et al. ( ) and McDonald and Pereira ( ) , we provide positive and negative examples in the training dataA second labeling stage can be applied to get labeled dependency structures as described in ( )The Maximum Spanning Tree algorithm 1 was recently introduced as a viable solution for non-projective dependency parsing ( )McDonald et al. ( ) introduced a model for dependency parsing based on the Edmonds/Chu-Liu algorithmMany of the features above were introduced in McDonald et al. ( ); specifically , the node-type , inside , and edge featuresWe have adopted the conditional Maximum Entropy (MaxEnt) modeling paradigm as outlined in Char-niak and Johnson ( ) and Riezler et al. ( )Work on statistical dependency parsing has utilized either dynamic-programming (DP) algorithms or variants of the Edmonds/Chu-Liu MST algorithm (see Tarjan ( ))This can be reduced to O(kn 2) in dense graphs 4 by choosing appropriate data structures ( )'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NasPy3bX9i8J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "a5fc48bc-521f-44fa-c0b3-fe2feeb8fd1c"
      },
      "source": [
        "import string\n",
        "\n",
        "\n",
        "translation = str.maketrans(\"\",\"\", string.punctuation)\n",
        "\n",
        "for x in dd[\"c2\"]:\n",
        "  words = dd.split()\n",
        "  new = words.translate(translation)\n",
        "  new"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-182-c0667454df2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"c2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG23upEd_fN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dd[\"new_column\"] = dd['c2'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1fThg1RGtS9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6c97943-f916-4ff9-94ae-38c5ba8d379d"
      },
      "source": [
        "dd\n"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c2</th>\n",
              "      <th>c4</th>\n",
              "      <th>c5</th>\n",
              "      <th>c6</th>\n",
              "      <th>new_column</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Following the notation in section 2.1 , the ij...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>1</td>\n",
              "      <td>Following the notation in section 21  the ijth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>For the simple bag-of-word bilingual LSA as de...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>For the simple bagofword bilingual LSA as desc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Discriminative word alignment models , such as...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Discriminative word alignment models  such as ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>For instance , the 1 most relaxed IBM Model-1 ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>For instance  the 1 most relaxed IBM Model1  w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It can be applied to complicated models such I...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>It can be applied to complicated models such I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The language model is a statistical trigram mo...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>The language model is a statistical trigram mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LSA has been successfully applied to informati...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>2</td>\n",
              "      <td>LSA has been successfully applied to informati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Alternative constructions of the matrix are po...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Alternative constructions of the matrix are po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>It has been shown that human knowledge , in th...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>2</td>\n",
              "      <td>It has been shown that human knowledge  in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Our decoder is a phrase-based multi-stack impl...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>1</td>\n",
              "      <td>Our decoder is a phrasebased multistack implem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Since Arabic is a morphologically rich languag...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>Since Arabic is a morphologically rich languag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Basic models in two translation directions are...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>2</td>\n",
              "      <td>Basic models in two translation directions are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>In ( ) , bilingual semantic maps are construct...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>In    bilingual semantic maps are constructed ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>As formulated in the competitive linking algor...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>As formulated in the competitive linking algor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>The example demos that due to reasonable const...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>The example demos that due to reasonable const...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>These feature weights are tuned on the dev set...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>These feature weights are tuned on the dev set...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>By combining word alignments in two directions...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>By combining word alignments in two directions...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>We simply modify the GIZA++ toolkit ( ) by alw...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>We simply modify the GIZA toolkit   by always ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>We measure translation performance by the BLEU...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>We measure translation performance by the BLEU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Toutanova et al. ( ) augmented bilingual sente...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Toutanova et al   augmented bilingual sentence...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>While word alignments can help identifying sem...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>3</td>\n",
              "      <td>While word alignments can help identifying sem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>We shall take HMM-based word alignment model (...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>We shall take HMMbased word alignment model   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Our baseline word alignment model is the word-...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>Our baseline word alignment model is the wordt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Many state-of-the-art SMT systems do not use t...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Many stateoftheart SMT systems do not use tree...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>An important advantage of our model is that it...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>3</td>\n",
              "      <td>An important advantage of our model is that it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Alternatively , order is modelled in terms of ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>3</td>\n",
              "      <td>Alternatively  order is modelled in terms of m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>These N-best lists are generated using approxi...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>These Nbest lists are generated using approxim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>tings , even for a bi-gram language model ( )</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tings  even for a bigram language model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>9 The advantages of modeling how a target lang...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>2</td>\n",
              "      <td>9 The advantages of modeling how a target lang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Previous work has shown that it is useful to m...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>2</td>\n",
              "      <td>Previous work has shown that it is useful to m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>Others have gone beyond unimodal speech input ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Others have gone beyond unimodal speech input ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>This develops and extends upon the multimodal ...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>This develops and extends upon the multimodal ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>Speech recognition results , pointing gestures...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>Speech recognition results  pointing gestures ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>However , as also reported in previous work ( ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>However  as also reported in previous work    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>768</th>\n",
              "      <td>The past few years have seen considerable impr...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>The past few years have seen considerable impr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>769</th>\n",
              "      <td>Some of these subsets were used for scoring in...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Some of these subsets were used for scoring in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>770</th>\n",
              "      <td>Table 1 gives two baselines and the parsing re...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>Table 1 gives two baselines and the parsing re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>771</th>\n",
              "      <td>There are several algorithms for doing so ( ) ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>There are several algorithms for doing so    w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>772</th>\n",
              "      <td>This restriction is inspired by psycholin-guis...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>1</td>\n",
              "      <td>This restriction is inspired by psycholinguist...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>773</th>\n",
              "      <td>When Klein and Manning induce the parts-of-spe...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>When Klein and Manning induce the partsofspeec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>774</th>\n",
              "      <td>This can either be semi-supervised parsing , u...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>This can either be semisupervised parsing  usi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>775</th>\n",
              "      <td>This problem is known in psycholinguistics as ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>This problem is known in psycholinguistics as ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>776</th>\n",
              "      <td>For large datasets , we use an ensemble techni...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>1</td>\n",
              "      <td>For large datasets  we use an ensemble techniq...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>777</th>\n",
              "      <td>In particular , we consider an algorithm propo...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>In particular  we consider an algorithm propos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>778</th>\n",
              "      <td>The k-best MST algorithm we introduce in this ...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>The kbest MST algorithm we introduce in this p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>779</th>\n",
              "      <td>Algorithm 1 is a version of the MST algorithm ...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>1</td>\n",
              "      <td>Algorithm 1 is a version of the MST algorithm ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780</th>\n",
              "      <td>We have introduced the Camerini et al. ( ) k-b...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>2</td>\n",
              "      <td>We have introduced the Camerini et al   kbest ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>Many of the model features have been inspired ...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>1</td>\n",
              "      <td>Many of the model features have been inspired ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782</th>\n",
              "      <td>Other DP solutions use constituency-based pars...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Other DP solutions use constituencybased parse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>783</th>\n",
              "      <td>An efficient algorithm for generating the k-be...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>An efficient algorithm for generating the kbes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784</th>\n",
              "      <td>The DP algorithms are generally variants of th...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>The DP algorithms are generally variants of th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785</th>\n",
              "      <td>2 In order to explore a rich set of syntactic ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>2 In order to explore a rich set of syntactic ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>786</th>\n",
              "      <td>Unlike the training procedure employed by McDo...</td>\n",
              "      <td>Compare</td>\n",
              "      <td>Compare</td>\n",
              "      <td>1</td>\n",
              "      <td>Unlike the training procedure employed by McDo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>787</th>\n",
              "      <td>A second labeling stage can be applied to get ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>A second labeling stage can be applied to get ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>788</th>\n",
              "      <td>The Maximum Spanning Tree algorithm 1 was rece...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>The Maximum Spanning Tree algorithm 1 was rece...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>789</th>\n",
              "      <td>McDonald et al. ( ) introduced a model for dep...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>McDonald et al   introduced a model for depend...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>790</th>\n",
              "      <td>Many of the features above were introduced in ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Many of the features above were introduced in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>791</th>\n",
              "      <td>We have adopted the conditional Maximum Entrop...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>We have adopted the conditional Maximum Entrop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792</th>\n",
              "      <td>Work on statistical dependency parsing has uti...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Work on statistical dependency parsing has uti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>793</th>\n",
              "      <td>This can be reduced to O(kn 2) in dense graphs...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>This can be reduced to Okn 2 in dense graphs 4...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>794 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    c2  ...                                         new_column\n",
              "0    Following the notation in section 2.1 , the ij...  ...  Following the notation in section 21  the ijth...\n",
              "1    For the simple bag-of-word bilingual LSA as de...  ...  For the simple bagofword bilingual LSA as desc...\n",
              "2    Discriminative word alignment models , such as...  ...  Discriminative word alignment models  such as ...\n",
              "3    For instance , the 1 most relaxed IBM Model-1 ...  ...  For instance  the 1 most relaxed IBM Model1  w...\n",
              "4    It can be applied to complicated models such I...  ...  It can be applied to complicated models such I...\n",
              "5    The language model is a statistical trigram mo...  ...  The language model is a statistical trigram mo...\n",
              "6    LSA has been successfully applied to informati...  ...  LSA has been successfully applied to informati...\n",
              "7    Alternative constructions of the matrix are po...  ...  Alternative constructions of the matrix are po...\n",
              "8    It has been shown that human knowledge , in th...  ...  It has been shown that human knowledge  in the...\n",
              "9    Our decoder is a phrase-based multi-stack impl...  ...  Our decoder is a phrasebased multistack implem...\n",
              "10   Since Arabic is a morphologically rich languag...  ...  Since Arabic is a morphologically rich languag...\n",
              "11   Basic models in two translation directions are...  ...  Basic models in two translation directions are...\n",
              "12   In ( ) , bilingual semantic maps are construct...  ...  In    bilingual semantic maps are constructed ...\n",
              "13   As formulated in the competitive linking algor...  ...  As formulated in the competitive linking algor...\n",
              "14   The example demos that due to reasonable const...  ...  The example demos that due to reasonable const...\n",
              "15   These feature weights are tuned on the dev set...  ...  These feature weights are tuned on the dev set...\n",
              "16   By combining word alignments in two directions...  ...  By combining word alignments in two directions...\n",
              "17   We simply modify the GIZA++ toolkit ( ) by alw...  ...  We simply modify the GIZA toolkit   by always ...\n",
              "18   We measure translation performance by the BLEU...  ...  We measure translation performance by the BLEU...\n",
              "19   Toutanova et al. ( ) augmented bilingual sente...  ...  Toutanova et al   augmented bilingual sentence...\n",
              "20   While word alignments can help identifying sem...  ...  While word alignments can help identifying sem...\n",
              "21   We shall take HMM-based word alignment model (...  ...  We shall take HMMbased word alignment model   ...\n",
              "22   Our baseline word alignment model is the word-...  ...  Our baseline word alignment model is the wordt...\n",
              "23   Many state-of-the-art SMT systems do not use t...  ...  Many stateoftheart SMT systems do not use tree...\n",
              "24   An important advantage of our model is that it...  ...  An important advantage of our model is that it...\n",
              "25   Alternatively , order is modelled in terms of ...  ...  Alternatively  order is modelled in terms of m...\n",
              "26   These N-best lists are generated using approxi...  ...  These Nbest lists are generated using approxim...\n",
              "27       tings , even for a bi-gram language model ( )  ...          tings  even for a bigram language model  \n",
              "28   9 The advantages of modeling how a target lang...  ...  9 The advantages of modeling how a target lang...\n",
              "29   Previous work has shown that it is useful to m...  ...  Previous work has shown that it is useful to m...\n",
              "..                                                 ...  ...                                                ...\n",
              "764  Others have gone beyond unimodal speech input ...  ...  Others have gone beyond unimodal speech input ...\n",
              "765  This develops and extends upon the multimodal ...  ...  This develops and extends upon the multimodal ...\n",
              "766  Speech recognition results , pointing gestures...  ...  Speech recognition results  pointing gestures ...\n",
              "767  However , as also reported in previous work ( ...  ...  However  as also reported in previous work    ...\n",
              "768  The past few years have seen considerable impr...  ...  The past few years have seen considerable impr...\n",
              "769  Some of these subsets were used for scoring in...  ...   Some of these subsets were used for scoring in  \n",
              "770  Table 1 gives two baselines and the parsing re...  ...  Table 1 gives two baselines and the parsing re...\n",
              "771  There are several algorithms for doing so ( ) ...  ...  There are several algorithms for doing so    w...\n",
              "772  This restriction is inspired by psycholin-guis...  ...  This restriction is inspired by psycholinguist...\n",
              "773  When Klein and Manning induce the parts-of-spe...  ...  When Klein and Manning induce the partsofspeec...\n",
              "774  This can either be semi-supervised parsing , u...  ...  This can either be semisupervised parsing  usi...\n",
              "775  This problem is known in psycholinguistics as ...  ...  This problem is known in psycholinguistics as ...\n",
              "776  For large datasets , we use an ensemble techni...  ...  For large datasets  we use an ensemble techniq...\n",
              "777  In particular , we consider an algorithm propo...  ...  In particular  we consider an algorithm propos...\n",
              "778  The k-best MST algorithm we introduce in this ...  ...  The kbest MST algorithm we introduce in this p...\n",
              "779  Algorithm 1 is a version of the MST algorithm ...  ...  Algorithm 1 is a version of the MST algorithm ...\n",
              "780  We have introduced the Camerini et al. ( ) k-b...  ...  We have introduced the Camerini et al   kbest ...\n",
              "781  Many of the model features have been inspired ...  ...  Many of the model features have been inspired ...\n",
              "782  Other DP solutions use constituency-based pars...  ...  Other DP solutions use constituencybased parse...\n",
              "783  An efficient algorithm for generating the k-be...  ...  An efficient algorithm for generating the kbes...\n",
              "784  The DP algorithms are generally variants of th...  ...  The DP algorithms are generally variants of th...\n",
              "785  2 In order to explore a rich set of syntactic ...  ...  2 In order to explore a rich set of syntactic ...\n",
              "786  Unlike the training procedure employed by McDo...  ...  Unlike the training procedure employed by McDo...\n",
              "787  A second labeling stage can be applied to get ...  ...  A second labeling stage can be applied to get ...\n",
              "788  The Maximum Spanning Tree algorithm 1 was rece...  ...  The Maximum Spanning Tree algorithm 1 was rece...\n",
              "789  McDonald et al. ( ) introduced a model for dep...  ...  McDonald et al   introduced a model for depend...\n",
              "790  Many of the features above were introduced in ...  ...  Many of the features above were introduced in ...\n",
              "791  We have adopted the conditional Maximum Entrop...  ...  We have adopted the conditional Maximum Entrop...\n",
              "792  Work on statistical dependency parsing has uti...  ...  Work on statistical dependency parsing has uti...\n",
              "793  This can be reduced to O(kn 2) in dense graphs...  ...  This can be reduced to Okn 2 in dense graphs 4...\n",
              "\n",
              "[794 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCizHhY7Gu-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dd[\"c2\"]=dd[\"new_column\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJg2B0xeHOjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60b82fcd-1087-40a4-a276-74ca41c60b80"
      },
      "source": [
        "dd\n"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c2</th>\n",
              "      <th>c4</th>\n",
              "      <th>c5</th>\n",
              "      <th>c6</th>\n",
              "      <th>new_column</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Following the notation in section 21  the ijth...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>1</td>\n",
              "      <td>Following the notation in section 21  the ijth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>For the simple bagofword bilingual LSA as desc...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>For the simple bagofword bilingual LSA as desc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Discriminative word alignment models  such as ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Discriminative word alignment models  such as ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>For instance  the 1 most relaxed IBM Model1  w...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>For instance  the 1 most relaxed IBM Model1  w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It can be applied to complicated models such I...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>It can be applied to complicated models such I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The language model is a statistical trigram mo...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>The language model is a statistical trigram mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LSA has been successfully applied to informati...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>2</td>\n",
              "      <td>LSA has been successfully applied to informati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Alternative constructions of the matrix are po...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Alternative constructions of the matrix are po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>It has been shown that human knowledge  in the...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>2</td>\n",
              "      <td>It has been shown that human knowledge  in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Our decoder is a phrasebased multistack implem...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>1</td>\n",
              "      <td>Our decoder is a phrasebased multistack implem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Since Arabic is a morphologically rich languag...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>Since Arabic is a morphologically rich languag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Basic models in two translation directions are...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>2</td>\n",
              "      <td>Basic models in two translation directions are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>In    bilingual semantic maps are constructed ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>In    bilingual semantic maps are constructed ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>As formulated in the competitive linking algor...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>As formulated in the competitive linking algor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>The example demos that due to reasonable const...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>The example demos that due to reasonable const...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>These feature weights are tuned on the dev set...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>These feature weights are tuned on the dev set...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>By combining word alignments in two directions...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>By combining word alignments in two directions...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>We simply modify the GIZA toolkit   by always ...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>We simply modify the GIZA toolkit   by always ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>We measure translation performance by the BLEU...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>We measure translation performance by the BLEU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Toutanova et al   augmented bilingual sentence...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Toutanova et al   augmented bilingual sentence...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>While word alignments can help identifying sem...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>3</td>\n",
              "      <td>While word alignments can help identifying sem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>We shall take HMMbased word alignment model   ...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>We shall take HMMbased word alignment model   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Our baseline word alignment model is the wordt...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>Our baseline word alignment model is the wordt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Many stateoftheart SMT systems do not use tree...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Many stateoftheart SMT systems do not use tree...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>An important advantage of our model is that it...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>3</td>\n",
              "      <td>An important advantage of our model is that it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Alternatively  order is modelled in terms of m...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>3</td>\n",
              "      <td>Alternatively  order is modelled in terms of m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>These Nbest lists are generated using approxim...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>These Nbest lists are generated using approxim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>tings  even for a bigram language model</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tings  even for a bigram language model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>9 The advantages of modeling how a target lang...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>2</td>\n",
              "      <td>9 The advantages of modeling how a target lang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Previous work has shown that it is useful to m...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>2</td>\n",
              "      <td>Previous work has shown that it is useful to m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>Others have gone beyond unimodal speech input ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Others have gone beyond unimodal speech input ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>This develops and extends upon the multimodal ...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>This develops and extends upon the multimodal ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>Speech recognition results  pointing gestures ...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>Speech recognition results  pointing gestures ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>However  as also reported in previous work    ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>However  as also reported in previous work    ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>768</th>\n",
              "      <td>The past few years have seen considerable impr...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>The past few years have seen considerable impr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>769</th>\n",
              "      <td>Some of these subsets were used for scoring in</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Some of these subsets were used for scoring in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>770</th>\n",
              "      <td>Table 1 gives two baselines and the parsing re...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>Table 1 gives two baselines and the parsing re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>771</th>\n",
              "      <td>There are several algorithms for doing so    w...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>There are several algorithms for doing so    w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>772</th>\n",
              "      <td>This restriction is inspired by psycholinguist...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>1</td>\n",
              "      <td>This restriction is inspired by psycholinguist...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>773</th>\n",
              "      <td>When Klein and Manning induce the partsofspeec...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>When Klein and Manning induce the partsofspeec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>774</th>\n",
              "      <td>This can either be semisupervised parsing  usi...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>This can either be semisupervised parsing  usi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>775</th>\n",
              "      <td>This problem is known in psycholinguistics as ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>This problem is known in psycholinguistics as ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>776</th>\n",
              "      <td>For large datasets  we use an ensemble techniq...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>1</td>\n",
              "      <td>For large datasets  we use an ensemble techniq...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>777</th>\n",
              "      <td>In particular  we consider an algorithm propos...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>In particular  we consider an algorithm propos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>778</th>\n",
              "      <td>The kbest MST algorithm we introduce in this p...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>The kbest MST algorithm we introduce in this p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>779</th>\n",
              "      <td>Algorithm 1 is a version of the MST algorithm ...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>1</td>\n",
              "      <td>Algorithm 1 is a version of the MST algorithm ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780</th>\n",
              "      <td>We have introduced the Camerini et al   kbest ...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>2</td>\n",
              "      <td>We have introduced the Camerini et al   kbest ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>Many of the model features have been inspired ...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Idea</td>\n",
              "      <td>1</td>\n",
              "      <td>Many of the model features have been inspired ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782</th>\n",
              "      <td>Other DP solutions use constituencybased parse...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Other DP solutions use constituencybased parse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>783</th>\n",
              "      <td>An efficient algorithm for generating the kbes...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>An efficient algorithm for generating the kbes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784</th>\n",
              "      <td>The DP algorithms are generally variants of th...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>The DP algorithms are generally variants of th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785</th>\n",
              "      <td>2 In order to explore a rich set of syntactic ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>2 In order to explore a rich set of syntactic ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>786</th>\n",
              "      <td>Unlike the training procedure employed by McDo...</td>\n",
              "      <td>Compare</td>\n",
              "      <td>Compare</td>\n",
              "      <td>1</td>\n",
              "      <td>Unlike the training procedure employed by McDo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>787</th>\n",
              "      <td>A second labeling stage can be applied to get ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>A second labeling stage can be applied to get ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>788</th>\n",
              "      <td>The Maximum Spanning Tree algorithm 1 was rece...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>The Maximum Spanning Tree algorithm 1 was rece...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>789</th>\n",
              "      <td>McDonald et al   introduced a model for depend...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>GRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>McDonald et al   introduced a model for depend...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>790</th>\n",
              "      <td>Many of the features above were introduced in ...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Many of the features above were introduced in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>791</th>\n",
              "      <td>We have adopted the conditional Maximum Entrop...</td>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Basis</td>\n",
              "      <td>1</td>\n",
              "      <td>We have adopted the conditional Maximum Entrop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792</th>\n",
              "      <td>Work on statistical dependency parsing has uti...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>Work on statistical dependency parsing has uti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>793</th>\n",
              "      <td>This can be reduced to Okn 2 in dense graphs 4...</td>\n",
              "      <td>BackGround</td>\n",
              "      <td>SRelated</td>\n",
              "      <td>1</td>\n",
              "      <td>This can be reduced to Okn 2 in dense graphs 4...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>794 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    c2  ...                                         new_column\n",
              "0    Following the notation in section 21  the ijth...  ...  Following the notation in section 21  the ijth...\n",
              "1    For the simple bagofword bilingual LSA as desc...  ...  For the simple bagofword bilingual LSA as desc...\n",
              "2    Discriminative word alignment models  such as ...  ...  Discriminative word alignment models  such as ...\n",
              "3    For instance  the 1 most relaxed IBM Model1  w...  ...  For instance  the 1 most relaxed IBM Model1  w...\n",
              "4    It can be applied to complicated models such I...  ...  It can be applied to complicated models such I...\n",
              "5    The language model is a statistical trigram mo...  ...  The language model is a statistical trigram mo...\n",
              "6    LSA has been successfully applied to informati...  ...  LSA has been successfully applied to informati...\n",
              "7    Alternative constructions of the matrix are po...  ...  Alternative constructions of the matrix are po...\n",
              "8    It has been shown that human knowledge  in the...  ...  It has been shown that human knowledge  in the...\n",
              "9    Our decoder is a phrasebased multistack implem...  ...  Our decoder is a phrasebased multistack implem...\n",
              "10   Since Arabic is a morphologically rich languag...  ...  Since Arabic is a morphologically rich languag...\n",
              "11   Basic models in two translation directions are...  ...  Basic models in two translation directions are...\n",
              "12   In    bilingual semantic maps are constructed ...  ...  In    bilingual semantic maps are constructed ...\n",
              "13   As formulated in the competitive linking algor...  ...  As formulated in the competitive linking algor...\n",
              "14   The example demos that due to reasonable const...  ...  The example demos that due to reasonable const...\n",
              "15   These feature weights are tuned on the dev set...  ...  These feature weights are tuned on the dev set...\n",
              "16   By combining word alignments in two directions...  ...  By combining word alignments in two directions...\n",
              "17   We simply modify the GIZA toolkit   by always ...  ...  We simply modify the GIZA toolkit   by always ...\n",
              "18   We measure translation performance by the BLEU...  ...  We measure translation performance by the BLEU...\n",
              "19   Toutanova et al   augmented bilingual sentence...  ...  Toutanova et al   augmented bilingual sentence...\n",
              "20   While word alignments can help identifying sem...  ...  While word alignments can help identifying sem...\n",
              "21   We shall take HMMbased word alignment model   ...  ...  We shall take HMMbased word alignment model   ...\n",
              "22   Our baseline word alignment model is the wordt...  ...  Our baseline word alignment model is the wordt...\n",
              "23   Many stateoftheart SMT systems do not use tree...  ...  Many stateoftheart SMT systems do not use tree...\n",
              "24   An important advantage of our model is that it...  ...  An important advantage of our model is that it...\n",
              "25   Alternatively  order is modelled in terms of m...  ...  Alternatively  order is modelled in terms of m...\n",
              "26   These Nbest lists are generated using approxim...  ...  These Nbest lists are generated using approxim...\n",
              "27           tings  even for a bigram language model    ...          tings  even for a bigram language model  \n",
              "28   9 The advantages of modeling how a target lang...  ...  9 The advantages of modeling how a target lang...\n",
              "29   Previous work has shown that it is useful to m...  ...  Previous work has shown that it is useful to m...\n",
              "..                                                 ...  ...                                                ...\n",
              "764  Others have gone beyond unimodal speech input ...  ...  Others have gone beyond unimodal speech input ...\n",
              "765  This develops and extends upon the multimodal ...  ...  This develops and extends upon the multimodal ...\n",
              "766  Speech recognition results  pointing gestures ...  ...  Speech recognition results  pointing gestures ...\n",
              "767  However  as also reported in previous work    ...  ...  However  as also reported in previous work    ...\n",
              "768  The past few years have seen considerable impr...  ...  The past few years have seen considerable impr...\n",
              "769   Some of these subsets were used for scoring in    ...   Some of these subsets were used for scoring in  \n",
              "770  Table 1 gives two baselines and the parsing re...  ...  Table 1 gives two baselines and the parsing re...\n",
              "771  There are several algorithms for doing so    w...  ...  There are several algorithms for doing so    w...\n",
              "772  This restriction is inspired by psycholinguist...  ...  This restriction is inspired by psycholinguist...\n",
              "773  When Klein and Manning induce the partsofspeec...  ...  When Klein and Manning induce the partsofspeec...\n",
              "774  This can either be semisupervised parsing  usi...  ...  This can either be semisupervised parsing  usi...\n",
              "775  This problem is known in psycholinguistics as ...  ...  This problem is known in psycholinguistics as ...\n",
              "776  For large datasets  we use an ensemble techniq...  ...  For large datasets  we use an ensemble techniq...\n",
              "777  In particular  we consider an algorithm propos...  ...  In particular  we consider an algorithm propos...\n",
              "778  The kbest MST algorithm we introduce in this p...  ...  The kbest MST algorithm we introduce in this p...\n",
              "779  Algorithm 1 is a version of the MST algorithm ...  ...  Algorithm 1 is a version of the MST algorithm ...\n",
              "780  We have introduced the Camerini et al   kbest ...  ...  We have introduced the Camerini et al   kbest ...\n",
              "781  Many of the model features have been inspired ...  ...  Many of the model features have been inspired ...\n",
              "782  Other DP solutions use constituencybased parse...  ...  Other DP solutions use constituencybased parse...\n",
              "783  An efficient algorithm for generating the kbes...  ...  An efficient algorithm for generating the kbes...\n",
              "784  The DP algorithms are generally variants of th...  ...  The DP algorithms are generally variants of th...\n",
              "785  2 In order to explore a rich set of syntactic ...  ...  2 In order to explore a rich set of syntactic ...\n",
              "786  Unlike the training procedure employed by McDo...  ...  Unlike the training procedure employed by McDo...\n",
              "787  A second labeling stage can be applied to get ...  ...  A second labeling stage can be applied to get ...\n",
              "788  The Maximum Spanning Tree algorithm 1 was rece...  ...  The Maximum Spanning Tree algorithm 1 was rece...\n",
              "789  McDonald et al   introduced a model for depend...  ...  McDonald et al   introduced a model for depend...\n",
              "790  Many of the features above were introduced in ...  ...  Many of the features above were introduced in ...\n",
              "791  We have adopted the conditional Maximum Entrop...  ...  We have adopted the conditional Maximum Entrop...\n",
              "792  Work on statistical dependency parsing has uti...  ...  Work on statistical dependency parsing has uti...\n",
              "793  This can be reduced to Okn 2 in dense graphs 4...  ...  This can be reduced to Okn 2 in dense graphs 4...\n",
              "\n",
              "[794 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3qnxew8HPox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ee = dd.drop([\"new_column\"], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMe8Da1LHXSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ee.to_csv(\"final_file.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHAPoUfjHmtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}